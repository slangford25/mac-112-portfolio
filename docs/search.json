[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COMP/STAT112 Notebook",
    "section": "",
    "text": "Introduction\nThis is my online notebook for COMP/STAT112 course taken at Macalester College. Please, use the side bar on the left for navigation.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "activities/README.html",
    "href": "activities/README.html",
    "title": "Activities",
    "section": "",
    "text": "All activities live here.",
    "crumbs": [
      "Activities"
    ]
  },
  {
    "objectID": "activities/activity-01.html",
    "href": "activities/activity-01.html",
    "title": "1  Activity 01- Data Viz",
    "section": "",
    "text": "# Load the package\nlibrary(tidyverse)\n\n# Import data\nhikes &lt;- read.csv(\"https://mac-stat.github.io/data/high_peaks.csv\")\n\n\nhead(hikes)\n\n             peak elevation difficulty ascent length time    rating\n1     Mt. Marcy        5344          5   3166   14.8 10.0  moderate\n2 Algonquin Peak       5114          5   2936    9.6  9.0  moderate\n3   Mt. Haystack       4960          7   3570   17.8 12.0 difficult\n4   Mt. Skylight       4926          7   4265   17.9 15.0 difficult\n5 Whiteface Mtn.       4867          4   2535   10.4  8.5      easy\n6       Dix Mtn.       4857          5   2800   13.2 10.0  moderate\n\n\nWhat features would we like a visualization of the categorical difficulty rating variable to capture? - A bar chart would be good for a categorical variable such as difficulty.\nWhat about a visualization of the quantitative elevation variable? - A histogram would be good for elevation variable.\n\n# Use the ggplot function\nggplot(hikes, aes(x = rating))\n\n\n\n\n\n\n\n# Adding geom_bar() indicates what kind of graph to create\nggplot(hikes, aes(x = rating)) +\n  geom_bar()\n\n\n\n\n\n\n\n# Changes the axis labels\nggplot(hikes, aes(x = rating)) +\n  geom_bar() +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n# Changes the color of the bar chart\nggplot(hikes, aes(x = rating)) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n# Adds an orange outline\nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\") +\n  labs(x = \"Rating\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n# Changes the theme of the graph, making the background white\nggplot(hikes, aes(x = rating)) +\n  geom_bar(color = \"orange\", fill = \"blue\")  +\n  labs(x = \"Rating\", y = \"Number of hikes\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nPart a Reflect on the ggplot() code.\nWhat’s the purpose of the +? When do we use it? - the + indicates that another function is being added to the ggplot function.\nWe added the bars using geom_bar()? Why “geom”? - It indicates that the graph is geometric\nWhat does labs() stand for? - labs() stands for labels\nWhat’s the difference between color and fill? - color changes the color of the border, fill changes the color of the entire bar.\nPart b In general, bar charts allow us to examine the following properties of a categorical variable: observed categories: What categories did we observe? - difficulty\nvariability between categories: Are observations evenly spread out among the categories, or are some categories more common than others? - moderate is by far the most common category\nWe must then translate this information into the context of our analysis, here hikes in the Adirondacks. Summarize below what you learned from the bar chart, in context.\nPart c Is there anything you don’t like about this barplot? For example: check out the x-axis again. - the x-axis is in alphabetical order not order of difficulty.\nExercise 6\n\nggplot(hikes, aes(x = elevation)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nThis is NOT an effective visualization because it creates a new bar for every unique elevation, making the bars small and difficult to read as very few peaks have the exact same elevation.\nExercise 8\n\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nExercise 9\n\n# Adds a histogram plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n# Adds white border to bars in plot\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\") \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n# Makes the bars blue\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", fill = \"blue\") \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n# Adds axis labels\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\") +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n# Changes the range of each bar to 1000\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 1000) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n# Changes the range of each bar to 2\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 5) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n# Changes the range of each bar to 200\nggplot(hikes, aes(x = elevation)) +\n  geom_histogram(color = \"white\", binwidth = 200) +\n  labs(x = \"Elevation (feet)\", y = \"Number of hikes\")\n\n\n\n\n\n\n\n\nExercise 11\n\nggplot(hikes, aes(x = elevation)) +\n  geom_density(color = \"blue\", fill = \"orange\")\n\n\n\n\n\n\n\n\nggplot template:\n\n#ggplot(___, aes(x = ___)) + \n  #geom___(color = \"___\", fill = \"___\") + \n  #labs(x = \"___\", y = \"___\")\n\nPRACTICE\n\n# Data on students in this class\nsurvey &lt;- read.csv(\"https://ajohns24.github.io/data/112/about_us_2024.csv\")\n\n# World Cup data\nworld_cup &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv\")\n\n\nggplot(data=survey, aes(x = minutes_to_campus)) + \n  geom_histogram(color = \"purple\", fill = \"navy\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nggplot(data=world_cup, aes(x=host)) +\n  geom_bar(fill=\"green\")",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Activity 01- Data Viz</span>"
    ]
  },
  {
    "objectID": "activities/activity-02.html",
    "href": "activities/activity-02.html",
    "title": "2  Activity 02- Bivariate Viz",
    "section": "",
    "text": "# Load data\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\n#packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Check it out\nhead(elections)\n\n  state_name state_abbr historical    county_name county_fips total_votes_20\n1    Alabama         AL        red Autauga County        1001          27770\n2    Alabama         AL        red Baldwin County        1003         109679\n3    Alabama         AL        red Barbour County        1005          10518\n4    Alabama         AL        red    Bibb County        1007           9595\n5    Alabama         AL        red  Blount County        1009          27588\n6    Alabama         AL        red Bullock County        1011           4613\n  repub_pct_20 dem_pct_20 winner_20 total_votes_16 repub_pct_16 dem_pct_16\n1        71.44      27.02     repub          24661        73.44      23.96\n2        76.17      22.41     repub          94090        77.35      19.57\n3        53.45      45.79     repub          10390        52.27      46.66\n4        78.43      20.70     repub           8748        76.97      21.42\n5        89.57       9.57     repub          25384        89.85       8.47\n6        24.84      74.70       dem           4701        24.23      75.09\n  winner_16 total_votes_12 repub_pct_12 dem_pct_12 winner_12 total_population\n1     repub          23909        72.63      26.58     repub            54907\n2     repub          84988        77.39      21.57     repub           187114\n3     repub          11459        48.34      51.25       dem            27321\n4     repub           8391        73.07      26.22     repub            22754\n5     repub          23980        86.49      12.35     repub            57623\n6       dem           5318        23.51      76.31       dem            10746\n  percent_white percent_black percent_asian percent_hispanic per_capita_income\n1            76            18             1                2             24571\n2            83             9             1                4             26766\n3            46            46             0                5             16829\n4            75            22             0                2             17427\n5            88             1             0                8             20730\n6            22            71             0                6             18628\n  median_rent median_age\n1         668       37.5\n2         693       41.5\n3         382       38.3\n4         351       39.4\n5         403       39.6\n6         276       39.6\n\n\nExercise 0: Review\n\nggplot(elections, aes(x=winner_20)) +\n  geom_bar()\n\n\n\n\n\n\n\nggplot(elections, aes(repub_pct_20)) +\n  geom_histogram(fill=\"blue\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nExercise 1 Quantitative vs Quantitative Intuition Check\n\nggplot(elections, aes(x=repub_pct_16, y=repub_pct_20)) +\n  geom_point()\n\n\n\n\n\n\n\n\nExercise 2: 2 Quantitiative Variables\n\n# Set up the plotting frame\n# How does this differ than the frame for our histogram of repub_pct_20 alone?\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16))\n\n\n\n\n\n\n\n# Add a layer of points for each county\n# Take note of the geom!\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point()\n\n\n\n\n\n\n\n# Change the shape of the points\n# What happens if you change the shape to another number?\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(shape = 4)\n\n\n\n\n\n\n\n# YOU TRY: Modify the code to make the points \"orange\"\n# NOTE: Try to anticipate if \"color\" or \"fill\" will be useful here. Then try both.\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point(color = \"orange\")\n\n\n\n\n\n\n\n\nExercise 3: Reflect Summarize the relationship between the Republican support in 2020 and 2016. Be sure to comment on:\nthe strength of the relationship (weak/moderate/strong) the direction of the relationship (positive/negative) outliers (in what state do counties deviate from the national trend? Any ideas why this might be the case?)\nRepublican support in 2020 and 2016 have a strong positive correlation. There are\nExercise 4:\n\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n# Construct a new plot that contains the model smooth but does not include the individual point glyphs.\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n#By default, geom_smooth() adds a smooth, localized model line. To examine the “best” linear model, we can specify method = \"lm\". It’s pretty similar in this example!\nggplot(elections, aes(y = repub_pct_20, x = repub_pct_16)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nExercise 5: Your Turn To examine how the 2020 results are related to some county demographics, construct scatterplots of repub_pct_20 vs median_rent, and repub_pct_20 vs median_age. Summarize the relationship between these two variables and comment on which is the better predictor of repub_pct_20, median_rent or median_age.\n\n# Scatterplot of repub_pct_20 vs median_rent\nggplot(elections, aes(x=median_rent, y=repub_pct_20)) +\n  geom_point()\n\n\n\n\n\n\n\n# Scatterplot of repub_pct_20 vs median_age\nggplot(elections, aes(x=median_age, y=repub_pct_20)) +\n  geom_point()\n\n\n\n\n\n\n\n\nExercise 6: A Sad Scatterplot Next, let’s explore the relationship between a county’s 2020 Republican support repub_pct_20 and the historical political trends in its state. In this case repub_pct_20 is quantitative, but historical is categorical. Explain why a scatterplot might not be an effective visualization for exploring this relationship. (What questions does / doesn’t it help answer?)\n\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_point()\n\n\n\n\n\n\n\n\nExercise 7: Quantitative vs Categorical – Violins & Boxes Though the above scatterplot did group the counties by historical category, it’s nearly impossible to pick out meaningful patterns in 2020 Republican support in each category. Let’s try adding 2 different geom layers to the frame:\n\n# Side-by-side violin plots\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_violin()\n\n\n\n\n\n\n\n# Side-by-side boxplots (defined below)\nggplot(elections, aes(y = repub_pct_20, x = historical)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nExercise 8\n\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density()\n\n\n\n\n\n\n\n\nExercise 9: Quantitative vs Categorical – Density Plots\n\n# Name two \"bad\" things about this plot: colors != historical colors, plots stacked on top of each other so hard to read\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density()\n\n\n\n\n\n\n\n# What does scale_fill_manual do? Changes the colors manually\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n\n\n\n\n\n# What does alpha = 0.5 do? Makes the fill of the plots 50% transparent\n# Play around with different values of alpha, between 0 and 1\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n\n\n\n\n\n\n# What does facet_wrap do?! Makes 3 different plots\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_density() +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\")) +\n  facet_wrap(~ historical)\n\n\n\n\n\n\n\n# Let's try a similar grouping strategy with a histogram instead of density plot. uggo, hard to read\n# Why is this terrible?\nggplot(elections, aes(x = repub_pct_20, fill = historical)) +\n  geom_histogram(color = \"white\") +\n  scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nExercise 10 We’ve now learned 3 (of many) ways to visualize the relationship between a quantitative and categorical variable: side-by-side violins, boxplots, and density plots.\nWhich do you like best? boxplots\nWhat is one pro of density plots relative to boxplots? can get more details (in terms of the numbers you get)\nWhat is one con of density plots relative to boxplots? specifics get lost\nExercise 11: Categorical vs Categorical – Intuition Check Finally, let’s simply explore who won each county in 2020 (winner_20) and how this breaks down by historical voting trends in the state. That is, let’s explore the relationship between 2 categorical variables! Following the same themes as above, we can utilize grouping features such as fill/color or facets to distinguish between different categories of winner_20 and historical.\n\n# Plot 1: adjust this to recreate the top plot\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n# Plot 2: adjust this to recreate the bottom plot\nggplot(elections, aes(x = winner_20)) +\n  geom_bar() +\n  facet_wrap(~ historical) \n\n\n\n\n\n\n\n\nExercise 12: Categorical vs Categorical Construct the following 4 bar plot visualizations.\n\n# A stacked bar plot\n# How are the \"historical\" and \"winner_20\" variables mapped to the plot, i.e. what roles do they play?\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar()\n\n\n\n\n\n\n\n# A faceted bar plot\nggplot(elections, aes(x = winner_20)) +\n  geom_bar() +\n  facet_wrap(~ historical)\n\n\n\n\n\n\n\n# A side-by-side bar plot\n# Note the new argument to geom_bar\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n# A proportional bar plot\n# Note the new argument to geom_bar\nggplot(elections, aes(x = historical, fill = winner_20)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nPart a Name one pro and one con of using the “proportional bar plot” instead of one of the other three options.\nPart b What’s your favorite bar plot from part and why?",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Activity 02- Bivariate Viz</span>"
    ]
  },
  {
    "objectID": "activities/activity-03.html",
    "href": "activities/activity-03.html",
    "title": "3  Multivariate Viz",
    "section": "",
    "text": "3.1 Review\nLet’s review some univariate and bivariate plotting concepts using some daily weather data from Australia. This is a subset of the data from the weatherAUS data in the rattle package.\nlibrary(tidyverse)\n\n# Import data\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))  \n\n# Check out the first 6 rows\nhead(weather)\n\n        date   location mintemp maxtemp rainfall evaporation sunshine\n1 2020-01-01 Wollongong    17.1    23.1        0          NA       NA\n2 2020-01-02 Wollongong    17.7    24.2        0          NA       NA\n3 2020-01-03 Wollongong    19.7    26.8        0          NA       NA\n4 2020-01-04 Wollongong    20.4    35.5        0          NA       NA\n5 2020-01-05 Wollongong    19.8    21.4        0          NA       NA\n6 2020-01-06 Wollongong    18.3    22.9        0          NA       NA\n  windgustdir windgustspeed winddir9am winddir3pm windspeed9am windspeed3pm\n1         SSW            39        SSW        SSE           20           15\n2         SSW            37          S        ENE           13           15\n3          NE            41        NNW        NNE            7           17\n4         SSW            78         NE        NNE           15           17\n5         SSW            57        SSW          S           31           35\n6          NE            35        ESE         NE           17           20\n  humidity9am humidity3pm pressure9am pressure3pm cloud9am cloud3pm temp9am\n1          69          64      1014.9      1014.0        8        1    19.1\n2          72          54      1020.1      1017.7        7        1    19.8\n3          72          71      1017.5      1013.0        6       NA    23.4\n4          77          69      1008.8      1003.9       NA       NA    24.5\n5          70          75      1018.9      1019.9       NA        7    20.7\n6          71          71      1021.2      1018.2       NA       NA    20.9\n  temp3pm raintoday risk_mm raintomorrow\n1    22.9        No     0.0           No\n2    23.6        No     0.0           No\n3    25.7        No     0.0           No\n4    26.7        No     0.0           No\n5    20.0        No     0.0           No\n6    22.6        No     0.8           No\n\n# What are the units of observation?\n\n\n# How many data points do we have? \n#24\n\n# What type of variables do we have?\n# categorical and numeric",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Viz</span>"
    ]
  },
  {
    "objectID": "activities/activity-03.html#review",
    "href": "activities/activity-03.html#review",
    "title": "3  Multivariate Viz",
    "section": "",
    "text": "Example 1\nConstruct a plot that allows us to examine how temp3pm varies.\n\n\nExample 2\nConstruct 3 plots that address the following research question:\nHow do afternoon temperatures (temp3pm) differ by location?\n\n# Plot 1 (no facets & starting from a density plot of temp3pm)\nggplot(weather, aes(x = temp3pm)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\n# Plot 2 (no facets or densities)\n\n\n# Plot 3 (facets)\n\n\nReflection\n\nTemperatures tend to be highest, and most variable, in Uluru. There, they range from ~10 to ~45 with a typical temp around ~30 degrees.\nTemperatures tend to be lowest in Hobart. There, they range from ~5 to ~45 with a typical temp around ~15 degrees.\nWollongong temps are in between and are the least variable from day to day.\n\nSUBTLETIES: Defining fill or color by a variable\nHow we define the fill or color depends upon whether we’re defining it by a named color or by some variable in our dataset. For example:\n\ngeom___(fill = \"blue\")\nnamed colors are defined outside the aesthetics and put in quotes\ngeom___(aes(fill = variable)) or ggplot(___, aes(fill = variable))\ncolors/fills defined by a variable are defined inside the aesthetics\n\n\n\n\nExample 3\nLet’s consider Wollongong alone:\n\n# Don't worry about the syntax (we'll learn it soon)\nwoll &lt;- weather |&gt;\n  filter(location == \"Wollongong\") |&gt; \n  mutate(date = as.Date(date))  \n\n\n# How often does it raintoday?\n# Fill your geometric layer with the color blue.\nggplot(woll, aes(x = raintoday))\n\n\n\n\n\n\n\n\n\n# If it does raintoday, what does this tell us about raintomorrow?\n# Use your intuition first\nggplot(woll, aes(x = raintoday))\n\n\n\n\n\n\n\n\n\n# Now compare different approaches\n\n# Default: stacked bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\n# Side-by-side bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n# Proportional bars\n# position = \"fill\" refers to filling the frame, nothing to do with the color-related fill\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n\nReflection\nThere’s often not one “best plot”, but a combination of plots that provide a complete picture:\n\nThe stacked and side-by-side bars reflect that on most days, it does not rain.\nThe proportional / filled bars lose that information, but make it easier to compare proportions: it’s more likely to rain tomorrow if it also rains today.\n\n\n\n\nExample 4\nConstruct a plot that illustrates how 3pm temperatures (temp3pm) vary by date in Wollongong. Represent each day on the plot and use a curve/line to help highlight the trends.\n\n# THINK: What variable goes on the y-axis?\n# For the curve, try adding span = 0.5 to tweak the curvature\n\n\n# Instead of a curve that captures the general TREND,\n# draw a line that illustrates the movement of RAW temperatures from day to day\n# NOTE: We haven't learned this geom yet! Guess.\nggplot(woll, aes(y = temp3pm, x = date))\n\n\n\n\n\n\n\n\nNOTE: A line plot isn’t always appropriate! It can be useful in situations like this, when our data are chronological.\n\nReflection\nThere’s a seasonal / cyclic behavior in temperatures – they’re highest in January (around 23 degrees) and lowest in July (around 16 degrees). There are also some outliers – some abnormally hot and cold days.",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Viz</span>"
    ]
  },
  {
    "objectID": "activities/activity-03.html#new-stuff",
    "href": "activities/activity-03.html#new-stuff",
    "title": "3  Multivariate Viz",
    "section": "3.2 New Stuff",
    "text": "3.2 New Stuff\nNext, let’s consider the entire weather data for all 3 locations. The addition of location adds a 3rd variable into our research questions:\n\nHow does the relationship between raintoday and raintomorrow vary by location?\nHow does the behavior of temp3pm over date vary by location?\nAnd so on.\n\nThus far, we’ve focused on the following components of a plot:\n\nsetting up a frame\nadding layers / geometric elements\nsplitting the plot into facets for different groups / categories\nchange the theme, e.g. axis labels, color, fill\n\nWe’ll have to think about all of this, along with scales. Scales change the color, fill, size, shape, or other properties according to the levels of a new variable. This is different than just assigning scale by, for example, color = \"blue\".\nWork on the examples below in your groups. Check in with your intuition! We’ll then discuss as a group as relevant.\n\nExample 5\n\n# Plot temp3pm vs temp9am\n# Change the code in order to indicate the location to which each data point corresponds\nggplot(weather, aes(y = temp3pm, x = temp9am)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n# Change the code in order to indicate the location to which each data point corresponds\n# AND identify the days on which it rained / didn't raintoday\nggplot(weather, aes(y = temp3pm, x = temp9am)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n# How many ways can you think to make that plot of temp3pm vs temp9am with info about location and rain?\n# Play around!\n\n\n\nExample 6\n\n# Change the code in order to construct a line plot of temp3pm vs date for each separate location (no points!)\nggplot(weather, aes(y = temp3pm, x = date)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\n\nExample 7\n\n# Plot the relationship of raintomorrow & raintoday\n# Change the code in order to indicate this relationship by location\nggplot(weather, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Not Get Overwhelmed?\n\n\n\nThere’s no end to the number and type of visualizations you could make. And it’s important to not just throw spaghetti at the wall until something sticks. FlowingData shows that one dataset can be visualized many ways, and makes good recommendations for data viz workflow, which we modify and build upon here:\n\nIdentify simple research questions.\nWhat do you want to understand about the variables or the relationships among them?\nStart with the basics and work incrementally.\n\nIdentify what variables you want to include in your plot and what structure these have (eg: categorical, quantitative, dates)\nStart simply. Build a plot of just 1 of these variables, or the relationship between 2 of these variables.\nSet up a plotting frame and add just one geometric layer at a time.\nStart tweaking: add whatever new variables you want to examine,\n\nAsk your plot questions.\n\nWhat questions does your plot answer? What questions are left unanswered by your plot?\nWhat new questions does your plot spark / inspire?\nDo you have the viz tools to answer these questions, or might you learn more?\n\nFocus.\nReporting a large number of visualizations can overwhelm the audience and obscure your conclusions. Instead, pick out a focused yet comprehensive set of visualizations.",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Viz</span>"
    ]
  },
  {
    "objectID": "activities/activity-03.html#exercises-required",
    "href": "activities/activity-03.html#exercises-required",
    "title": "3  Multivariate Viz",
    "section": "3.3 Exercises (required)",
    "text": "3.3 Exercises (required)\n\nThe story\nThough far from a perfect assessment of academic preparedness, SAT scores have historically been used as one measurement of a state’s education system. The education dataset contains various education variables for each state:\n\n# Import and check out data\neducation &lt;- read.csv(\"https://mac-stat.github.io/data/sat.csv\")\nhead(education)\n\n       State expend ratio salary frac verbal math  sat  fracCat\n1    Alabama  4.405  17.2 31.144    8    491  538 1029   (0,15]\n2     Alaska  8.963  17.6 47.951   47    445  489  934 (45,100]\n3    Arizona  4.778  19.3 32.175   27    448  496  944  (15,45]\n4   Arkansas  4.459  17.1 28.934    6    482  523 1005   (0,15]\n5 California  4.992  24.0 41.078   45    417  485  902  (15,45]\n6   Colorado  5.443  18.4 34.571   29    462  518  980  (15,45]\n\n\nA codebook is provided by Danny Kaplan who also made these data accessible:\n\n\n\nExercise 1: SAT scores\n\nPart a\nConstruct a plot of how the average sat scores vary from state to state. (Just use 1 variable – sat not State!)\n\nggplot(education, aes(x = sat)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\nPart b\nSummarize your observations from the plot. Comment on the basics: range, typical outcomes, shape. (Any theories about what might explain this non-normal shape?) The plot is right skewed with several blank spots in the center. the blank spaces may be due to how the SAT is scored, meaning its easier to get some scores rather than others.\n\n\n\nExercise 2: SAT Scores vs Per Pupil Spending & SAT Scores vs Salaries\nThe first question we’d like to answer is: Can the variability in sat scores from state to state be partially explained by how much a state spends on education, specifically its per pupil spending (expend) and typical teacher salary?\n\nPart a\n\n# Construct a plot of sat vs expend\n# Include a \"best fit linear regression model\" (HINT: method = \"lm\")\nggplot(education, aes(x = expend, y = sat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n# Construct a plot of sat vs salary\n# Include a \"best fit linear regression model\" (HINT: method = \"lm\")\nggplot(education, aes(x = salary, y = sat)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\nPart b\nWhat are the relationship trends between SAT scores and spending? Is there anything that surprises you? there is a weak negative relationship between SAT scores and spending meaning that expenditures do not help increase SAT scores and can actually harm them.\n\n\n\nExercise 3: SAT Scores vs Per Pupil Spending and Teacher Salaries\nConstruct one visualization of the relationship of sat with salary and expend. HINT: Start with just 2 variables and tweak that code to add the third variable. Try out a few things!\n\nggplot(education, aes(x= salary, y = sat)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nExercise 4: Another way to Incorporate Scale\nIt can be tough to distinguish color scales and size scales for quantitative variables. Another option is to discretize a quantitative variable, or basically cut it up into categories.\nConstruct the plot below. Check out the code and think about what’s happening here. What happens if you change “2” to “3”?\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 2))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\nDescribe the trivariate relationship between sat, salary, and expend.\n\n\nExercise 5: Finally an Explanation\nIt’s strange that SAT scores seem to decrease with spending. But we’re leaving out an important variable from our analysis: the fraction of a state’s students that actually take the SAT. The fracCat variable indicates this fraction: low (under 15% take the SAT), medium (15-45% take the SAT), and high (at least 45% take the SAT).\n\nPart a\nBuild a univariate viz of fracCat to better understand how many states fall into each category.\n\n\nPart b\nBuild 2 bivariate visualizations that demonstrate the relationship between sat and fracCat. What story does your graphic tell and why does this make contextual sense?\n\n\nPart c\nMake a trivariate visualization that demonstrates the relationship of sat with expend AND fracCat. Highlight the differences in fracCat groups through color AND unique trend lines. What story does your graphic tell?\nDoes it still seem that SAT scores decrease as spending increases?\n\n\nPart d\nPutting all of this together, explain this example of Simpson’s Paradox. That is, why did it appear that SAT scores decrease as spending increases even though the opposite is true?",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Viz</span>"
    ]
  },
  {
    "objectID": "activities/activity-03.html#exercises-optional",
    "href": "activities/activity-03.html#exercises-optional",
    "title": "3  Multivariate Viz",
    "section": "3.4 Exercises (optional)",
    "text": "3.4 Exercises (optional)\n\nExercise 6: Heat Maps\nAs usual, we’ve only just scratched the surface! There are lots of other data viz techniques for exploring multivariate relationships. Let’s start with a heat map.\n\nPart a\nRun the chunks below. Check out the code, but don’t worry about every little detail! NOTES:\n\nThis is not part of the ggplot() grammar, making it a bit complicated.\nIf you’re curious about what a line in the plot does, comment it out (#) and check out what happens!\nIn the plot, for each state (row), each variable (column) is scaled to indicate whether the state has a relative high value (yellow), a relatively low value (purple), or something in between (blues/greens).\nYou can also play with the color scheme. Type ?cm.colors in the console to learn about various options.\nWe’ll improve the plot later, so don’t spend too much time trying to learn something from this plot.\n\n\n# Remove the \"State\" column and use it to label the rows\n# Then scale the variables\nplot_data &lt;- education |&gt; \n  column_to_rownames(\"State\") |&gt; \n  data.matrix() |&gt; \n  scale()\n\n# Load the gplots package needed for heatmaps\nlibrary(gplots)\n\n# Construct heatmap 1\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = NA, \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n# Construct heatmap 2\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = TRUE,             ### WE CHANGED THIS FROM NA TO TRUE\n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n# Construct heatmap 3\nheatmap.2(plot_data,\n  dendrogram = \"row\",       ### WE CHANGED THIS FROM \"none\" TO \"row\"\n  Rowv = TRUE,            \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\nPart b\nIn the final two plots, the states (rows) are rearranged by similarity with respect to these education metrics. The final plot includes a dendrogram which further indicates clusters of similar states. In short, states that have a shorter path to connection are more similar than others.\nPutting this all together, what insight do you gain about the education trends across U.S. states? Which states are similar? In what ways are they similar? Are there any outliers with respect to 1 or more of the education metrics?\n\n\n\nExercise 7: Star plots\nLike heat maps, star plots indicate the relative scale of each variable for each state. Thus, we can use star maps to identify similar groups of states, and unusual states!\n\nPart a\nConstruct and check out the star plot below. Note that each state has a “pie”, with each segment corresponding to a different variable. The larger a segment, the larger that variable’s value is in that state. For example:\n\nCheck out Minnesota. How does Minnesota’s education metrics compare to those in other states? What metrics are relatively high? Relatively low?\nWhat states appear to be similar? Do these observations agree with those that you gained from the heat map?\n\n\nstars(plot_data,\n  flip.labels = FALSE,\n  key.loc = c(10, 1.5),\n  cex = 1, \n  draw.segments = TRUE\n)\n\n\n\nPart b\nFinally, let’s plot the state stars by geographic location! What new insight do you gain here?!\n\nstars(plot_data,\n  flip.labels = FALSE,\n  locations = data.matrix(as.data.frame(state.center)),  # added external data to arrange by geo location\n  key.loc = c(-110, 28),\n  cex = 1, \n  draw.segments = TRUE\n)",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Viz</span>"
    ]
  },
  {
    "objectID": "activities/activity-03.html#solutions",
    "href": "activities/activity-03.html#solutions",
    "title": "3  Multivariate Viz",
    "section": "3.5 Solutions",
    "text": "3.5 Solutions\n\n\nClick for Solutions\n\n\nlibrary(tidyverse)\n\n# Import data\nweather &lt;- read.csv(\"https://mac-stat.github.io/data/weather_3_locations.csv\") |&gt; \n  mutate(date = as.Date(date))  \n\n# Check out the first 6 rows\n# What are the units of observation?\nhead(weather)\n\n        date   location mintemp maxtemp rainfall evaporation sunshine\n1 2020-01-01 Wollongong    17.1    23.1        0          NA       NA\n2 2020-01-02 Wollongong    17.7    24.2        0          NA       NA\n3 2020-01-03 Wollongong    19.7    26.8        0          NA       NA\n4 2020-01-04 Wollongong    20.4    35.5        0          NA       NA\n5 2020-01-05 Wollongong    19.8    21.4        0          NA       NA\n6 2020-01-06 Wollongong    18.3    22.9        0          NA       NA\n  windgustdir windgustspeed winddir9am winddir3pm windspeed9am windspeed3pm\n1         SSW            39        SSW        SSE           20           15\n2         SSW            37          S        ENE           13           15\n3          NE            41        NNW        NNE            7           17\n4         SSW            78         NE        NNE           15           17\n5         SSW            57        SSW          S           31           35\n6          NE            35        ESE         NE           17           20\n  humidity9am humidity3pm pressure9am pressure3pm cloud9am cloud3pm temp9am\n1          69          64      1014.9      1014.0        8        1    19.1\n2          72          54      1020.1      1017.7        7        1    19.8\n3          72          71      1017.5      1013.0        6       NA    23.4\n4          77          69      1008.8      1003.9       NA       NA    24.5\n5          70          75      1018.9      1019.9       NA        7    20.7\n6          71          71      1021.2      1018.2       NA       NA    20.9\n  temp3pm raintoday risk_mm raintomorrow\n1    22.9        No     0.0           No\n2    23.6        No     0.0           No\n3    25.7        No     0.0           No\n4    26.7        No     0.0           No\n5    20.0        No     0.0           No\n6    22.6        No     0.8           No\n\n# How many data points do we have? \nnrow(weather)\n\n[1] 2367\n\n# What type of variables do we have?\nstr(weather)\n\n'data.frame':   2367 obs. of  24 variables:\n $ date         : Date, format: \"2020-01-01\" \"2020-01-02\" ...\n $ location     : chr  \"Wollongong\" \"Wollongong\" \"Wollongong\" \"Wollongong\" ...\n $ mintemp      : num  17.1 17.7 19.7 20.4 19.8 18.3 19.9 20.1 19.8 20.5 ...\n $ maxtemp      : num  23.1 24.2 26.8 35.5 21.4 22.9 25.6 23.2 23.1 25.4 ...\n $ rainfall     : num  0 0 0 0 0 0 0.8 1.6 0 0 ...\n $ evaporation  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ sunshine     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ windgustdir  : chr  \"SSW\" \"SSW\" \"NE\" \"SSW\" ...\n $ windgustspeed: int  39 37 41 78 57 35 44 41 39 56 ...\n $ winddir9am   : chr  \"SSW\" \"S\" \"NNW\" \"NE\" ...\n $ winddir3pm   : chr  \"SSE\" \"ENE\" \"NNE\" \"NNE\" ...\n $ windspeed9am : int  20 13 7 15 31 17 30 31 24 19 ...\n $ windspeed3pm : int  15 15 17 17 35 20 7 33 26 39 ...\n $ humidity9am  : int  69 72 72 77 70 71 76 77 76 79 ...\n $ humidity3pm  : int  64 54 71 69 75 71 72 76 79 76 ...\n $ pressure9am  : num  1015 1020 1018 1009 1019 ...\n $ pressure3pm  : num  1014 1018 1013 1004 1020 ...\n $ cloud9am     : int  8 7 6 NA NA NA NA 8 NA NA ...\n $ cloud3pm     : int  1 1 NA NA 7 NA NA NA NA NA ...\n $ temp9am      : num  19.1 19.8 23.4 24.5 20.7 20.9 22.9 21.3 21.2 23 ...\n $ temp3pm      : num  22.9 23.6 25.7 26.7 20 22.6 24.9 22.2 22.2 25.1 ...\n $ raintoday    : chr  \"No\" \"No\" \"No\" \"No\" ...\n $ risk_mm      : num  0 0 0 0 0 0.8 1.6 0 0 1 ...\n $ raintomorrow : chr  \"No\" \"No\" \"No\" \"No\" ...\n\n\n\nExample 1\n\nggplot(weather, aes(x = temp3pm)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n# Plot 1 (no facets & starting from a density plot of temp3pm)\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n# Plot 2 (no facets or densities)\nggplot(weather, aes(y = temp3pm, x = location)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n# Plot 3 (facets)\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  facet_wrap(~ location)\n\n\n\n\n\n\n\n\n\n\nExample 3\n\n# How often does it raintoday?\n# Fill your geometric layer with the color blue.\nggplot(woll, aes(x = raintoday)) + \n  geom_bar(fill = \"blue\")\n\n\n\n\n\n\n\n\n\n# If it does raintoday, what does this tell us about raintomorrow?\n# Use your intuition first\nggplot(woll, aes(x = raintoday)) + \n  geom_bar(aes(fill = raintomorrow))\n\n\n\n\n\n\n\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\n# Now compare different approaches\n\n# Default: stacked bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\n# Side-by-side bars\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n# Proportional bars\n# position = \"fill\" refers to filling the frame, nothing to do with the color-related fill\nggplot(woll, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n\n\nExample 4\n\n# THINK: What variable goes on the y-axis?\n# For the curve, try adding span = 0.5 to tweak the curvature\nggplot(woll, aes(y = temp3pm, x = date)) + \n  geom_point() + \n  geom_smooth(span = 0.5)\n\n\n\n\n\n\n\n\n\n# Instead of a curve that captures the general TREND,\n# draw a line that illustrates the movement of RAW temperatures from day to day\n# NOTE: We haven't learned this geom yet! Guess.\nggplot(woll, aes(y = temp3pm, x = date)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\n\nExample 5\n\n# Plot temp3pm vs temp9am\n# Change the code in order to indicate the location to which each data point corresponds\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n# Change the code in order to indicate the location to which each data point corresponds\n# AND identify the days on which it rained / didn't raintoday\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() +\n  facet_wrap(~ raintoday)\n\n\n\n\n\n\n\n\n\n# How many ways can you think to make that plot of temp3pm vs temp9am with info about location and rain?\n# Play around!\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location, shape = raintoday)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n\nExample 6\n\n# Change the code in order to construct a line plot of temp3pm vs date for each separate location (no points!)\nggplot(weather, aes(y = temp3pm, x = date, color = location)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\n\nExample 7\n\n# Plot the relationship of raintomorrow & raintoday\n# Change the code in order to indicate this relationship by location\nggplot(weather, aes(x = raintoday, fill = raintomorrow)) + \n  geom_bar(position = \"fill\") + \n  facet_wrap(~ location)\n\n\n\n\n\n\n\n\n\n\nExercise 1: SAT scores\n\nPart a\n\n# A histogram would work too!\nggplot(education, aes(x = sat)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\n\nPart b\naverage SAT scores range from roughly 800 to 1100. They appear bi-modal.\n\n\n\nExercise 2: SAT Scores vs Per Pupil Spending & SAT Scores vs Salaries\n\nPart a\n\n# Construct a plot of sat vs expend\n# Include a \"best fit linear regression model\"\nggplot(education, aes(y = sat, x = expend)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n# Construct a plot of sat vs salary\n# Include a \"best fit linear regression model\"\nggplot(education, aes(y = sat, x = salary)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\nPart b\nThe higher the student expenditures and teacher salaries, the worse the SAT performance.\n\n\n\nExercise 3: SAT Scores vs Per Pupil Spending and Teacher Salaries\n\nggplot(education, aes(y = sat, x = salary, color = expend)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\nExercise 4: Another Way to Incorporate Scale\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 2))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\n\n\n\n\n\n\nggplot(education, aes(y = sat, x = salary, color = cut(expend, 3))) + \n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\")\n\n\n\n\n\n\n\n\nStates with lower salaries and expenditures tend to have higher SAT scores.\n\n\nExercise 5: Finally an Explanation\n\nPart a\n\nggplot(education, aes(x = fracCat)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nPart b\nThe more students in a state that take the SAT, the lower the average scores tend to be. This is probably related to self-selection.\n\nggplot(education, aes(x = sat, fill = fracCat)) + \n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nPart c\nWhen we control for the fraction of students that take the SAT, SAT scores increase with expenditure.\n\nggplot(education, aes(y = sat, x = expend, color = fracCat)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\nPart d\nStudent participation tends to be lower among states with lower expenditures (which are likely also the states with higher ed institutions that haven’t historically required the SAT). Those same states tend to have higher SAT scores because of the self-selection of who participates.\n\n\n\nExercise 6: Heat Maps\n\nPart a\n\n# Remove the \"State\" column and use it to label the rows\n# Then scale the variables\nplot_data &lt;- education |&gt; \n  column_to_rownames(\"State\") |&gt; \n  data.matrix() |&gt; \n  scale()\n\n# Load the gplots package needed for heatmaps\nlibrary(gplots)\n\n# Construct heatmap 1\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = NA, \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n\n\n\n\n# Construct heatmap 2\nheatmap.2(plot_data,\n  dendrogram = \"none\",\n  Rowv = TRUE,             ### WE CHANGED THIS FROM NA TO TRUE\n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n\n\n\n\n# Construct heatmap 3\nheatmap.2(plot_data,\n  dendrogram = \"row\",       ### WE CHANGED THIS FROM \"none\" TO \"row\"\n  Rowv = TRUE,            \n  scale = \"column\",\n  keysize = 0.7, \n  density.info = \"none\",\n  col = hcl.colors(256), \n  margins = c(10, 20),\n  colsep = c(1:7), rowsep = (1:50), sepwidth = c(0.05, 0.05),\n  sepcolor = \"white\", trace = \"none\"\n)\n\n\n\n\n\n\n\n\n\n\nPart b\n\nSimilar values in verbal, math, and sat.\nHigh contrast (an inverse relationship) verbal/math/sat scores and the fraction of students that take the SAT.\nOutliers of Utah and California in ratio (more students per teacher).\nWhile grouped, fraction and salary are not as similar to each other as the sat scores; it is also interesting to notice states that have high ratios have generally low expenditures per student.\n\n\n\n\nExercise 7: Star Plots\n\nPart a\nMN is high on the SAT performance related metrics and low on everything else. MN is similar to Iowa, Kansas, Mississippi, Missouri, the Dakotas…\n\nstars(plot_data,\n  flip.labels = FALSE,\n  key.loc = c(10, 1.5),\n  cex = 1, \n  draw.segments = TRUE\n)\n\n\n\n\n\n\n\n\n\n\nPart b\nWhen the states are in geographical ordering, we’d notice more easily that states in similar regions of the U.S. have similar patterns of these variables.\n\nstars(plot_data,\n  flip.labels = FALSE,\n  locations = data.matrix(as.data.frame(state.center)),  # added external data to arrange by geo location\n  key.loc = c(-110, 28),\n  cex = 1, \n  draw.segments = TRUE\n)",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multivariate Viz</span>"
    ]
  },
  {
    "objectID": "activities/activity-06.html",
    "href": "activities/activity-06.html",
    "title": "4  activity-06",
    "section": "",
    "text": "4.1 Review\nIn the previous activity, we explored a Simpson’s Paradox–it seemed that:\nBUT this was explained by a confounding or omitted or lurking variable: the % of students in a state that take the SAT:\nThus, when controlling for the % of students that take the SAT, more spending is correlated with higher scores.\nLet’s explore a Simpson’s paradox related to Mac!\nBack in the 2000s, Macalester invested in insulating a few campus-owned houses, with the hopes of leading to energy savings.  Former Mac prof Danny Kaplan accessed monthly data on energy use and other info for these addresses, before and after renovations:\nmonth year  price therms hdd address renovated       date\n1     6 2005  35.21     21   0       a        no 2005-06-01\n2     7 2005  37.37     21   0       a        no 2005-07-01\n3     8 2005  36.93     21   3       a        no 2005-08-01\n4     9 2005  62.36     39  61       a        no 2005-09-01\n5    10 2005 184.15    120 416       a        no 2005-10-01\n6    11 2005 433.35    286 845       a        no 2005-11-01\nIncluded are the following variables:",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>activity-06</span>"
    ]
  },
  {
    "objectID": "activities/activity-06.html#review",
    "href": "activities/activity-06.html#review",
    "title": "4  activity-06",
    "section": "",
    "text": "States with higher spending…\ntend to have lower average SAT scores.\n\n\n\nStates with higher spending…\ntend to have a higher % of students of students that take the SAT…\nwhich then “leads to” lower average SAT scores.\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmeaning\n\n\n\n\ntherms\na measure of energy use (the more energy used, the larger the therms)\n\n\naddress\na or b\n\n\nrenovated\nwhether the location had been renovated, yes or no\n\n\nmonth\nfrom 1 to 12\n\n\nhdd\nmonthly heating degree days. a proxy measure of outside temperatures (the higher the hdd, the COLDER it was)\n\n\n\n\n\n\n\n\n\nInstructions\n\n\n\n\nConstruct a plot that addresses each research question\nInclude a 1-sentence summary of the plot.\n\n\n\n\nExample 1\nWhat was range in, and typical, energy used each month, as measured by therms? How does this differ by address?\n\n\nExample 2\nHow did energy use (therms) change over time (date) at the two addresses?\n\n\nExample 3\nHow did the typical energy use (therms) at the two addresses change before and after they were renovated?\n\n\nExample 4\nThat seems unfortunate that energy usage went up after renovations. But also fishy.\nTake 5 minutes in your groups to try and explain what’s going on here. Think: What confounding, lurking, or omitted variable related to energy usage are we ignoring here? Try to make some plots to prove your point.\n\n\nExample 5\nLet’s summarize the punchlines by filling in the ???. It seemed that:\n\nAfter renovation…\nenergy use increased.\n\nBUT this was explained by a confounding or omitted or lurking variable: ???\n\nAfter renovation…\n???…\nwhich then leads to higher energy use.\n\nThus, when controlling for ???, renovations led to decreased energy use.",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>activity-06</span>"
    ]
  },
  {
    "objectID": "activities/activity-06.html#new-stuff",
    "href": "activities/activity-06.html#new-stuff",
    "title": "4  activity-06",
    "section": "4.2 New stuff",
    "text": "4.2 New stuff\nTypes of spatial viz:\n\nPoint Maps: plotting locations of individual observations\nexample: bigfoot sightings\nContour Maps: plotting the density or distribution of observations (not the individual observations themselves)\nChoropleth Maps: plotting outcomes in different regions\n\nNYT article on effects of redlining\nMinnesota Reformer article on how Mpls / St Paul voted on 2021 ballot measures related to mayoral, policing, and rent policies\n\n\nThese spatial maps can be static or dynamic/interactive.",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>activity-06</span>"
    ]
  },
  {
    "objectID": "activities/activity-06.html#exercises",
    "href": "activities/activity-06.html#exercises",
    "title": "4  activity-06",
    "section": "4.3 Exercises",
    "text": "4.3 Exercises\n\n4.3.1 Preview\nYou’ll explore some R spatial viz tools below. In general, there are two important pieces to every map:\nPiece 1: A dataset\nThis dataset must include either:\n\nlocation coordinates for your points of interest (for point maps); or\nvariable outcomes for your regions of interest (for choropleth maps)\n\n\n\nPiece 2: A background map\nWe need latitude and longitude coordinates to specify the boundaries for your regions of interest (eg: countries, states). This is where it gets really sticky!\n\nCounty-level, state-level, country-level, continent-level info live in multiple places.\nWhere we grab this info can depend upon whether we want to make a point map or a choropleth map. (The background maps can be used somewhat interchangeably, but it requires extra code :/)\nWhere we grab this info can also depend upon the structure of our data and how much data wrangling / cleaning we’re up for. For choropleth maps, the labels of regions in our data must match those in the background map. For example, if our data labels states with their abbreviations (eg: MN) and the background map refers to them as full names in lower case (eg: minnesota), we have to wrangle our data so that it matches the background map.\n\nIn short, the code for spatial viz gets very specialized. The goal of these exercises is to:\n\nplay around and experience the wide variety of spatial viz tools out there\nunderstand the difference between point maps and choropleth maps\nhave fun\n\nYou can skip around as you wish and it’s totally fine if you don’t finish everything. Just come back at some point to play around.\n\n\n4.3.2 Part 1: Interactive points on a map with leaflet\nLeaflet is an open-source JavaScript library for creating maps. We can use it inside R through the leaflet package.\nThis uses a different plotting framework than ggplot2, but still has a tidyverse feel (which will become more clear as we learn other tidyverse tools!).\nThe general steps are as follows:\n\nCreate a map widget by calling leaflet() and telling it the data to use.\nAdd a base map using addTiles() (the default) or addProviderTiles().\nAdd layers to the map using layer functions (e.g. addMarkers(), addPolygons()).\nPrint the map widget to display it.\n\n\nExercise 1: A leaflet with markers / points\nEarlier this semester, I asked for the latitude and longitude of one of your favorite places. I rounded these to the nearest whole number, so that they’re near to but not exactly at those places. Let’s load the data and map it!\n\nfave_places &lt;- read.csv(\"https://ajohns24.github.io/data/112/our_fave_places.csv\")\n\n# Check it out\nhead(fave_places)\n\n  latitude longitude\n1       46      -123\n2       33        52\n3       48       -90\n4       36      -112\n5       59        25\n6       39      -106\n\n\n\nPart a\nYou can use a “two-finger scroll” to zoom in and out.\n\n# Load the leaflet package\nlibrary(leaflet)\n\n# Just a plotting frame\n# leaflet(data = fave_places)\n\n\n# Now what do we have?\nleaflet(data = fave_places) %&gt;% \n  addTiles()\n\n\n\n\n\n\n# Now what do we have?\n# longitude and latitude refer to the variables in our data\nleaflet(data = fave_places) %&gt;% \n  addTiles() %&gt;% \n  addMarkers(lng = ~longitude, lat = ~latitude)\n\n\n\n\n\n\n# Since we named them \"longitude\" and \"latitude\", the function\n# automatically recognizes these variables. No need to write them!\nleaflet(data = fave_places) %&gt;% \n  addTiles() %&gt;% \n  addMarkers()\n\n\n\n\n\n\n\nPart b\nPLAY AROUND! This map is interactive. Zoom in on one location. Keep zooming – what level of detail can you get into? How does that detail depend upon where you try to zoom in (thus what are the limitations of this tool)?\n\n\n\nExercise 2: Details\nWe can change all sorts of details in leaflet maps.\n\n# Load package needed to change color\nlibrary(gplots)\n\n# We can add colored circles instead of markers at each location\nleaflet(data = fave_places) %&gt;% \n  addTiles() %&gt;% \n  addCircles(color = col2hex(\"red\"))\n\n\n\n\n\n\n# We can change the background\n# Mark locations with yellow dots\n# And connect the dots, in their order in the dataset, with green lines\n# (These green lines don't mean anything here, but would if this were somebody's travel path!)\nleaflet(data = fave_places) %&gt;%\n  addProviderTiles(\"USGS\") %&gt;%\n  addCircles(weight = 10, opacity = 1, color = col2hex(\"yellow\")) %&gt;%\n  addPolylines(\n    lng = ~longitude,\n    lat = ~latitude,\n    color = col2hex(\"green\")\n  )\n\n\n\n\n\nIn general:\n\naddProviderTiles() changes the base map.\nTo explore all available provider base maps, type providers in the console. (Though some don’t work :/)\nUse addMarkers() or addCircles() to mark locations. Type ?addControl into the console to pull up a help file which summarizes the aesthetics of these markers and how you can change them. For example:\n\nweight = how thick to make the lines, points, pixels\nopacity = transparency (like alpha in ggplot2)\ncolors need to be in “hex” form. We used the col2hex() function from the gplots library to do that\n\n\n\n\nExercise 3: Your turn\nThe starbucks data, compiled by Danny Kaplan, contains information about every Starbucks in the world at the time the data were collected, including Latitude and Longitude:\n\n# Import starbucks location data\nstarbucks &lt;- read.csv(\"https://mac-stat.github.io/data/starbucks.csv\")\n\nLet’s focus on only those in Minnesota for now:\n\n# Don't worry about the syntax\nstarbucks_mn &lt;- starbucks %&gt;%   \n  filter(Country == \"US\", State.Province == \"MN\")\n\nCreate a leaflet map of the Starbucks locations in Minnesota. Keep it simple – go back to Exercise 1 for an example.\n\n\n\n4.3.3 Part 2: Static points on a map\nLeaflet is very powerful and fun. But:\n\nIt’s not great when we have lots of points to map – it takes lots of time.\nIt makes good interactive maps, but we often need a static map (eg: we can print interactive maps!).\n\nLet’s explore how to make point maps with ggplot(), not leaflet().\n\nExercise 3: A simple scatterplot\nLet’s start with the ggplot() tools we already know. Construct a scatterplot of all starbucks locations, not just those in Minnesota, with:\n\nLatitude and Longitude coordinates (which goes on the y-axis?!)\nMake the points transparent (alpha = 0.2) and smaller (size = 0.2)\n\nIt’s pretty cool that the plots we already know can provide some spatial context. But what don’t you like about this plot?\n\nggplot(starbucks, aes(x=Longitude, y = Latitude)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nExercise 4: Adding a country-level background\nLet’s add a background map of country-level boundaries.\n\n\nPart a\nFirst, we can grab country-level boundaries from the rnaturalearth package.\n\n# Load the package\nlibrary(rnaturalearth)\n\n# Get info about country boundaries across the world\n# in a \"sf\" or simple feature format\nworld_boundaries &lt;- ne_countries(returnclass = \"sf\")\n\nIn your console, type world_boundaries to check out what’s stored there. Don’t print it our in your Rmd – printing it would be really messy there (even just the head()).\n\nPart b\nRun the chunks below to build up a new map.\n\n# What does this code produce?\n# What geom are we using for the point map?\nggplot(world_boundaries) + \n  geom_sf()\n\n\n\n\n\n\n\n\n\n# Load package needed to change map theme\nlibrary(mosaic)\n\n# Add a point for each Starbucks\n# NOTE: The Starbucks info is in our starbucks data, not world_boundaries\n# How does this change how we use geom_point?!\nggplot(world_boundaries) + \n  geom_sf() + \n  geom_point(\n    data = starbucks,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.3, size = 0.2, color = \"darkgreen\"\n  ) +\n  theme_map()\n\n\n\n\n\n\n\n\n\n\nPart c\nSummarize what you learned about Starbucks from this map. There are a lot of starbucks in north america, europe, eastern china, and japan\n\n\n\nExercise 5: Zooming in on some countries\nInstead of world_boundaries &lt;- ne_countries(returnclass = 'sf') we could zoom in on…\n\nthe continent of Africa: ne_countries(continent = 'Africa', returnclass = 'sf')\na set of countries: ne_countries(country = c('france', 'united kingdom', 'germany'), returnclass = 'sf')\nboundaries within a country: ne_states(country = 'united states of america', returnclass = 'sf')\n\nOur goal here will be to map the Starbucks locations in Canada, Mexico, and the US.\n\nPart a\nTo make this map, we again need two pieces of information.\n\nData on Starbucks for only Canada, Mexico, and the US, labeled as “CA”, “MX”, “US” in the starbucks data.\n\n\n# We'll learn this syntax soon! Don't worry about it now.\nstarbucks_cma &lt;- starbucks %&gt;% \n  filter(Country %in% c('CA', 'MX', 'US'))\n\n\nA background map of state- and national-level boundaries in Canada, Mexico, and the US. This requires ne_states() in the rnaturalearth package where the countries are labeled ‘canada’, ‘mexico’, ‘united states of america’.\n\n\n# cma_boundaries &lt;- ne_states(\n#   country = c(\"canada\", \"mexico\", \"united states of america\"),\n#   returnclass = \"sf\")\n\n\n\nPart b\nMake the map!\n\n# Just the boundaries\n#ggplot(cma_boundaries) + \n # geom_sf()\n\n\n# Add the points\n# And zoom in\n#ggplot(cma_boundaries) + \n#  geom_sf() + \n#  geom_point(\n#    data = starbucks_cma,\n #   aes(x = Longitude, y = Latitude),\n  #  alpha = 0.3,\n   # size = 0.2,\n#    color = \"darkgreen\"\n # ) +\n  #coord_sf(xlim = c(-179.14, -50)) +\n  #theme_map()\n\n\n\n\nExercise 6: A state and county-level map\nLet’s get an even higher resolution map of Starbucks locations within the states of Minnesota, Wisconsin, North Dakota, and South Dakota, with a background map at the county-level.\n\n\nPart a\nTo make this map, we again need two pieces of information.\n\nData on Starbucks for only the states of interest.\n\n\nstarbucks_midwest &lt;- starbucks %&gt;% \n  filter(State.Province %in% c(\"MN\", \"ND\", \"SD\", \"WI\"))\n\n\nA background map of state- and county-level boundaries in these states. This requires st_as_sf() in the sf package, and map() in the maps package, where the countries are labeled ‘minnesota’, ‘north dakota’, etc.\n\n\n# Load packages\nlibrary(sf)\nlibrary(maps)\n\n# Get the boundaries\nmidwest_boundaries &lt;- st_as_sf(\n  maps::map(\"county\",\n            region = c(\"minnesota\", \"wisconsin\", \"north dakota\", \"south dakota\"), \n            fill = TRUE, plot = FALSE))\n\n# Check it out\nhead(midwest_boundaries)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -96.81268 ymin: 45.05167 xmax: -93.01397 ymax: 48.53526\nGeodetic CRS:  +proj=longlat +ellps=clrk66 +no_defs +type=crs\n                                     ID                           geom\nminnesota,aitkin       minnesota,aitkin MULTIPOLYGON (((-93.03689 4...\nminnesota,anoka         minnesota,anoka MULTIPOLYGON (((-93.51817 4...\nminnesota,becker       minnesota,becker MULTIPOLYGON (((-95.14537 4...\nminnesota,beltrami   minnesota,beltrami MULTIPOLYGON (((-95.58655 4...\nminnesota,benton       minnesota,benton MULTIPOLYGON (((-93.77027 4...\nminnesota,big stone minnesota,big stone MULTIPOLYGON (((-96.10794 4...\n\n\n\nPart b\nAdjust the code below to make the plot! Remove the # to run it.\n\n# ggplot(___) + \n#   geom___() + \n#   geom___(\n#     data = ___,\n#     aes(x = ___, y = ___),\n#     alpha = 0.7,\n#     size = 0.2, \n#     color = 'darkgreen'\n#   ) + \n#   theme_map()\n\n\n\n\nExercise 7: Contour maps\nEspecially when there are lots of point locations, and those locations start overlapping on a map, it can be tough to visualize areas of higher density. Consider the Starbucks locations in Canada, Mexico, and the US that we mapped earlier:\n\n# Point map (we made this earlier)\n#ggplot(cma_boundaries) + \n#  geom_sf() + \n # geom_point(\n  #  data = starbucks_cma,\n   # aes(x = Longitude, y = Latitude),\n    #alpha = 0.3,\n  #  size = 0.2,\n   # color = \"darkgreen\"\n # ) +\n#  coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n#  theme_map()\n\nNow check out the contour map.\n\n# What changed in the plot?\n# What changed in our code?!\n#ggplot(cma_boundaries) + \n # geom_sf() + \n # geom_density_2d(\n#    data = starbucks_cma,\n  #  aes(x = Longitude, y = Latitude),\n  #  size = 0.2,\n   # color = \"darkgreen\"\n # ) +\n # coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n#  theme_map()\n\n\n\n\n4.3.4 Part 3: Choropleth maps\nSpatial data isn’t always in the form of point locations! For example, recall the state and county-level data on presidential elections.\n\nelections_by_state &lt;-  read.csv(\"https://mac-stat.github.io/data/election_2020_by_state.csv\")\nelections_by_counties &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\nIn these datasets, we’re interested in the overall election outcome by region (state or county), not the specific geographic location of some observation. Let’s wrangle our data first. We’ll focus on just a few variables of interest, and create a new variable (repub_20_categories) that discretizes the repub_pct_20 variable into increments of 5 percentage points (for states) or 10 percentage points (for counties):\n\n# Don't worry about the code!\n\nelections_by_state &lt;- elections_by_state %&gt;% \n  filter(state_abbr != \"DC\") %&gt;% \n  select(state_name, state_abbr, repub_pct_20) %&gt;% \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(30, 70, by = 5), \n               labels = c(\"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                          \"50-54\", \"55-59\", \"60-64\", \"65-70\"), \n               include.lowest = TRUE))\n\nelections_by_counties &lt;- elections_by_counties %&gt;% \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) %&gt;% \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(0, 100, by = 10),\n               labels = c(\"0-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\",\n                          \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-100\"),\n               include.lowest = TRUE))\n\n\nExercise 8: State-level choropleth maps\nLet’s map the 2020 Republican support in each state, repub_pct_20.\n\n\nPart a\nWe again need two pieces of information.\n\nData on elections in each state, which we already have: elections_by_state.\nA background map of state boundaries in the US. The boundaries we used for point maps don’t work here. (Optional detail: they’re sf objects and we now need a data.frame object.) Instead, we can use the map_data() function from the ggplot2 package:\n\n\n# Get the latitude and longitude coordinates of state boundaries\nstates_map &lt;- map_data(\"state\")\n\n# Check it out\nhead(states_map)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\n\nPause\nImportant detail: Note that the region variable in states_map, and the state_name variable in elections_by_state both label states by the full name in lower case letters. This is critical to the background map and our data being able to communicate.\n\nhead(states_map)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\nhead(elections_by_state) \n\n   state_name state_abbr repub_pct_20 repub_20_categories\n1     alabama         AL        62.03               60-64\n2    arkansas         AR        62.40               60-64\n3     arizona         AZ        49.06               45-49\n4  california         CA        34.33               30-34\n5    colorado         CO        41.90               40-44\n6 connecticut         CT        39.21               35-39\n\n\n\n\nPart b\nNow map repub_pct_20 by state.\n\n# Note where the dataset, elections_by_state, is used\n# Note where the background map, states_map, is used\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_pct_20)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() \n\n\n\n\n\n\n\n\n\n# Make it nicer!\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_pct_20)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_gradientn(name = \"% Republican\", colors = c(\"blue\", \"purple\", \"red\"), values = scales::rescale(seq(0, 100, by = 5)))\n\n\n\n\n\n\n\n\nIt’s not easy to get fine control over the color scale for the quantitative repub_pct_20 variable. Instead, let’s plot the discretized version, repub_20_categories:\n\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map()\n\n\n\n\n\n\n\n\n\n# Load package needed for refining color palette\nlibrary(RColorBrewer)\n\n# Now fix the colors\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_manual(values = rev(brewer.pal(8, \"RdBu\")), name = \"% Republican\")\n\n\n\n\n\n\n\n\n\n\nPart c\nWe can add other layers, like points, on top of a choropleth map. Add a Starbucks layer! Do you notice any relationship between Starbucks and elections? Or are we just doing things at this point? ;)\n\n# Get only the starbucks data from the US\nstarbucks_us &lt;- starbucks %&gt;% \n  filter(Country == \"US\")\n\n# Map it\nggplot(elections_by_state, aes(map_id = state_name, fill = repub_20_categories)) +\n  geom_map(map = states_map) +\n  geom_point(\n    data = starbucks_us,\n    aes(x = Longitude, y = Latitude),\n    size = 0.05,\n    alpha = 0.2,\n    inherit.aes = FALSE\n  ) +\n  expand_limits(x = states_map$long, y = states_map$lat) +\n  theme_map() + \n  scale_fill_manual(values = rev(brewer.pal(8, \"RdBu\")), name = \"% Republican\")\n\n\n\n\n\n\n\n\nDetails (if you’re curious)\n\nmap_id is a required aesthetic for geom_map().\n\nIt specifies which variable in our dataset indicates the region (here state_name).\nIt connects this variable (state_name) to the region variable in our mapping background (states_map). These variables must have the same possible outcomes in order to be matched up (alabama, alaska, arizona,…).\n\nexpand_limits() assures that the map covers the entire area it’s supposed to, by pulling longitudes and latitudes from the states_map.\n\n\n\nPart d\nWe used geom_sf() for point maps. What geom do we use for choropleth maps?\n\n\n\n\nExercise 9: County-level choropleth maps\nLet’s map the 2020 Republican support in each county.\n\nPart a\nWe again need two pieces of information.\n\nData on elections in each county, which we already have: elections_by_county.\nA background map of county boundaries in the US, stored in the county_map dataset in the socviz package:\n\n\n# Get the latitude and longitude coordinates of county boundaries\nlibrary(socviz)\ndata(county_map) \n\n# Check it out\nhead(county_map)\n\n     long      lat order  hole piece            group    id\n1 1225889 -1275020     1 FALSE     1 0500000US01001.1 01001\n2 1235324 -1274008     2 FALSE     1 0500000US01001.1 01001\n3 1244873 -1272331     3 FALSE     1 0500000US01001.1 01001\n4 1244129 -1267515     4 FALSE     1 0500000US01001.1 01001\n5 1272010 -1262889     5 FALSE     1 0500000US01001.1 01001\n6 1276797 -1295514     6 FALSE     1 0500000US01001.1 01001\n\n\n\n\nPause\nImportant detail: We officially have a headache. Our county_map refers to each county by a 5-number id. Our elections_by_counties data refers to each county by a county_fips code, which is mostly the same as id, BUT drops any 0’s at the beginning of the code.\n\nhead(county_map)\n\n     long      lat order  hole piece            group    id\n1 1225889 -1275020     1 FALSE     1 0500000US01001.1 01001\n2 1235324 -1274008     2 FALSE     1 0500000US01001.1 01001\n3 1244873 -1272331     3 FALSE     1 0500000US01001.1 01001\n4 1244129 -1267515     4 FALSE     1 0500000US01001.1 01001\n5 1272010 -1262889     5 FALSE     1 0500000US01001.1 01001\n6 1276797 -1295514     6 FALSE     1 0500000US01001.1 01001\n\nhead(elections_by_counties)\n\n  state_name state_abbr    county_name county_fips repub_pct_20 median_age\n1    Alabama         AL Autauga County        1001        71.44       37.5\n2    Alabama         AL Baldwin County        1003        76.17       41.5\n3    Alabama         AL Barbour County        1005        53.45       38.3\n4    Alabama         AL    Bibb County        1007        78.43       39.4\n5    Alabama         AL  Blount County        1009        89.57       39.6\n6    Alabama         AL Bullock County        1011        24.84       39.6\n  median_rent repub_20_categories\n1         668               70-79\n2         693               70-79\n3         382               50-59\n4         351               70-79\n5         403               80-89\n6         276               20-29\n\n\nThis just means that we have to wrangle the data so that it can communicate with the background map.\n\n# Add 0's at the beginning of any fips_code that's fewer than 5 numbers long\n# Don't worry about the syntax\nelections_by_counties &lt;- elections_by_counties %&gt;% \n  mutate(county_fips = as.character(county_fips)) %&gt;% \n  mutate(county_fips = \n           ifelse(nchar(county_fips) == 4, paste0(\"0\", county_fips), county_fips))\n\n\n\nPart b\nNow map Republican support by county. Let’s go straight to the discretized repub_20_categories variable, and a good color scale.\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = repub_20_categories)) +\n  geom_map(map = county_map) +\n  scale_fill_manual(values = rev(brewer.pal(10, \"RdBu\")), name = \"% Republican\") +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal()\n\n\n\n\n\n\n\n\n\n\nExercise 10: Play around!\nConstruct county-level maps of median_rent and median_age.\n\n\nExercise 11: Choropleth maps with leaflet\nThough ggplot() is often better for this purpose, we can also make choropleth maps with leaflet(). If you’re curious, check out the leaflet documentation:\nhttps://rstudio.github.io/leaflet/choropleths.html",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>activity-06</span>"
    ]
  },
  {
    "objectID": "activities/activity-06.html#solutions",
    "href": "activities/activity-06.html#solutions",
    "title": "4  activity-06",
    "section": "4.4 Solutions",
    "text": "4.4 Solutions\n\n\nClick for Solutions\n\n\nExample 1\nBoth addresses used between 0 and 450 therms per month. There seem to be two types of months – those with lower use around 50 therms and those with higher use around 300/400 therms.\n\nggplot(energy, aes(x = therms, fill = address)) + \n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nExample 2\nEnergy use is seasonal, with higher usage in winter months. It seems that address a uses slightly more energy.\n\nggplot(energy, aes(y = therms, x = date, color = address)) + \n  geom_point()\n\n\n\n\n\n\n\nggplot(energy, aes(y = therms, x = date, color = address)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\n\nExample 3\nAt both addresses, typical energy use increased after renovations.\n\nggplot(energy, aes(y = therms, x = renovated)) + \n  geom_boxplot() + \n  facet_wrap(~ address)\n\n\n\n\n\n\n\n# A density plot isn't very helpful for comparing typical therms in this example!\nggplot(energy, aes(x = therms, fill = renovated)) + \n  geom_density(alpha = 0.5) + \n  facet_wrap(~ address)\n\n\n\n\n\n\n\n\n\n\nExample 4\nlurking variable = outdoor temperature (as reflected by hdd)\n\n# It happened to be colder outside after renovations (higher hdd)\nggplot(energy, aes(y = hdd, x = renovated)) + \n  geom_boxplot() + \n  facet_wrap(~ address)\n\n\n\n\n\n\n\n# When controlling for outside temps (via hdd), energy use decreased post-renovation\nggplot(energy, aes(y = therms, x = hdd, color = renovated)) + \n  geom_point(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  facet_wrap(~ address)\n\n\n\n\n\n\n\n\n\n\nExample 5\nBUT this was explained by a confounding or omitted or lurking variable: hdd (outdoor temperature)\n\nAfter renovation…\nit happened to be colder…\nwhich then leads to higher energy use.\n\nThus, when controlling for outdoor temps, renovations led to decreased energy use.\n\n\nExercise 3: Your turn\n\nleaflet(data = starbucks_mn) %&gt;% \n  addTiles() %&gt;% \n  addMarkers()\n\n\n\n\n\n\n\nExercise 3: A simple scatterplot\nIt would be nice to also have some actual reference maps of countries in the background.\n\nggplot(starbucks, aes(y = Latitude, x = Longitude)) + \n  geom_point(size = 0.5)\n\n\n\n\n\n\n\n\n\n\nExercise 6: A state and county-level map\n\nPart b\nAdjust the code below to make the plot! Remove the # to run it.\n\nggplot(midwest_boundaries) +\n  geom_sf() +\n  geom_point(\n    data = starbucks_midwest,\n    aes(x = Longitude, y = Latitude),\n    alpha = 0.7,\n    size = 0.2,\n    color = 'darkgreen'\n  ) +\n  theme_map()\n\n\n\n\n\n\n\n\n\n\n\nExercise 7: Contour maps\nEspecially when there are lots of point locations, and those locations start overlapping on a map, it can be tough to visualize areas of higher density. Consider the Starbucks locations in Canada, Mexico, and the US that we mapped earlier:\n\n# Point map (we made this earlier)\n#ggplot(cma_boundaries) + \n # geom_sf() + \n  #geom_point(\n   # data = starbucks_cma,\n    #aes(x = Longitude, y = Latitude),\n    #alpha = 0.3,\n#    size = 0.2,\n #   color = \"darkgreen\"\n  #) +\n  #coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n  #theme_map()\n\nNow check out the contour map.\n\n# What changed in the plot?\n# What changed in our code?!\n#ggplot(cma_boundaries) + \n # geom_sf() + \n#  geom_density_2d(\n  #  data = starbucks_cma,\n#    aes(x = Longitude, y = Latitude),\n #   size = 0.2,\n  #  color = \"darkgreen\"\n # ) +\n # coord_sf(xlim = c(-179.14, -50), ylim = c(14.54, 83.11)) +\n # theme_map()\n\n\n\nExercises Part 3: Choropleth maps\nSpatial data isn’t always in the form of point locations! For example, recall the state and county-level data on presidential elections.\n\nelections_by_state &lt;-  read.csv(\"https://mac-stat.github.io/data/election_2020_by_state.csv\")\nelections_by_counties &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\n\nIn these datasets, we’re interested in the overall election outcome by region (state or county), not the specific geographic location of some observation. Let’s wrangle our data first.\nWe’ll focus on just a few variables of interest, and create a new variable (repub_20_categories) that discretizes the repub_pct_20 variable into increments of 5 percentage points (for states) or 10 percentage points (for counties):\n\n# Don't worry about the code!\n\nelections_by_state &lt;- elections_by_state %&gt;% \n  filter(state_abbr != \"DC\") %&gt;% \n  select(state_name, state_abbr, repub_pct_20) %&gt;% \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(30, 70, by = 5), \n               labels = c(\"30-34\", \"35-39\", \"40-44\", \"45-49\",\n                          \"50-54\", \"55-59\", \"60-64\", \"65-70\"), \n               include.lowest = TRUE))\n\nelections_by_counties &lt;- elections_by_counties %&gt;% \n  select(state_name, state_abbr, county_name, county_fips,\n          repub_pct_20, median_age, median_rent) %&gt;% \n  mutate(repub_20_categories = \n           cut(repub_pct_20, \n               breaks = seq(0, 100, by = 10),\n               labels = c(\"0-9\", \"10-19\", \"20-29\", \"30-39\", \"40-49\",\n                          \"50-59\", \"60-69\", \"70-79\", \"80-89\", \"90-100\"),\n               include.lowest = TRUE))\n\n# Add 0's at the beginning of any fips_code that's fewer than 5 numbers long\n# Don't worry about the syntax\nelections_by_counties &lt;- elections_by_counties %&gt;% \n  mutate(county_fips = as.character(county_fips)) %&gt;% \n  mutate(county_fips = \n           ifelse(nchar(county_fips) == 4, paste0(\"0\", county_fips), county_fips))\n\n\n\nExercise 8: State-level choropleth maps\n\nPart d\ngeom_map()\n\n\n\nExercise 10: Play around!\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = median_rent)) +\n  geom_map(map = county_map) +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal() + \n  scale_fill_gradientn(name = \"median rent\", colors = c(\"white\", \"lightgreen\", \"darkgreen\"))\n\n\n\n\n\n\n\nggplot(elections_by_counties, aes(map_id = county_fips, fill = median_age)) +\n  geom_map(map = county_map) +\n  expand_limits(x = county_map$long, y = county_map$lat) +\n  theme_map() +\n  theme(legend.position = \"right\") + \n  coord_equal() + \n  scale_fill_gradientn(name = \"median age\", colors = terrain.colors(10))",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>activity-06</span>"
    ]
  },
  {
    "objectID": "activities/activity-07.html",
    "href": "activities/activity-07.html",
    "title": "5  activity-07",
    "section": "",
    "text": "5.1 Warm-up\nRecall: Benefits of Visualizations",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>activity-07</span>"
    ]
  },
  {
    "objectID": "activities/activity-07.html#warm-up",
    "href": "activities/activity-07.html#warm-up",
    "title": "5  activity-07",
    "section": "",
    "text": "Understand what we’re working with:\n\nscales & typical outcomes\noutliers, i.e. unusual cases\npatterns & relationships\n\nRefine research questions & inform next steps of our analysis.\nCommunicate our findings and tell a story.\n\n\nNo One Right Viz\nThere is no one right way to visualize a data set, eg, check out the 100 ways used to visualize one dataset: https://100.datavizproject.com/ The visualized data was featured in this TidyTuesday!\n\nActivity: Plot Critique\nIn groups:\n\nScroll through the plots.\nIdentify at least 1 plot that you feel illuminates some important aspect of the data.\nIdentify at least 1 plot that does NOT illuminate the data in a good way.\n\n\n\n\nUgly, Bad, Wrong Viz\nOne way to identify effective viz is to understand what makes a viz ineffective. In the Fundamentals of Data Visualization, Wilke breaks down ineffective viz into 3 categories:\n\nWrong\nThe viz is “objectively incorrect”, as in the numbers / trends being displayed are wrong.\nBad\nThe viz is “unclear, confusing, overly complicated, or deceiving”.\nUgly\nThe viz correct and clear but The aesthetics are problematic.\n\n\nActivity: Critical Analysis\nLet’s try some critical analysis on specific examples. For your assigned viz, identify the following:\n\nThe story the viz is trying to communicate.\nWhether the viz is good, ugly, bad, wrong, or some combination.\nAreas for improvement.\n\n\n\n\n\n\nIMAGE 1. Source: N. Yau, Visualize This, 2011, p. 223-225.\n\n\n\n\n\n\n\n\n\nIMAGE 2. Source: N. Yau, Visualize This, 2011, p. 242.\n\n\n\n\n\n\n\n\n\nIMAGE 3. Climate change.\n\n\n\n\n\n\n\n\n\nIMAGE 4. Source: http://viz.wtf/\n\n\n\n\n\n\n\n\n\nIMAGE 5. Source: N. Yau, Visualize This, 2011, p. 220.\n\n\n\n\n\n\n\n\n\nIMAGE 6. Source: (https://www.reddit.com/r/dataisugly/comments/vlirox/0_1_19_20_39/)\n\n\n\n\n\n\nFollow-up to Climate Change Plot\n\n\n\n\n\nIMAGE 3. Climate change.\n\n\n\n\n\n\n\nEffective & Ineffective Viz Examples\n\nExamples of good viz:\n\nFlowingData’s “Best visualizations of…”\n\nExamples of bad viz:\n\nWTF Visualizations\nBad viz in the wild\n\n\n\n\nEffective Viz\nYou can take a whole course in Data Viz at Mac! The topic of effective viz is too big and nuanced to boil down into a simple list. Here are some basics:\n\nProfessionalism\nOnce you’re ready to “share out” your viz, it should have…\n\nmeaningful axis labels\na figure caption (depending upon where the viz will appear)\n\n\n\nAccessibility\nOnce you’re ready to “share out” your viz, it should…\n\nhave “alt text”, a written description of the viz that can be read out by a screen reader (video)\nuse a color palette that is distinguishable across common forms of color blindness\n\n\n\nDesign Details\nIn designing your viz, think about comparison. Good viz make it easy for people to perceive things that are similar and things that are different.\n\n\nEthics\nMichael Correll of Tableau Research (pdf) wrote “Data visualizations have a potentially enormous influence on how data are used to make decisions across all areas of human endeavor.” Thus ethics are critical from the data we use, to the plots we build, to the way in which we communicate this work. This is a very broad topic, and we’ll focus here on data visualization alone. Relatedly, and at a minimum:\n\nData viz should not mislead, i.e. “wrong” viz are unethical.**\nYet ethics in data viz goes much deeper. Correll describes three related principles to strive for:\n\nVisibility\nMake the invisible visible. Visualize hidden labor, hidden uncertainty, hidden impacts. Credit your sources, data and otherwise.\nPrivacy\nCollect data with empathy. Encourage small Data, anthropomorphize data, obfuscate data to protect privacy.\nPower\nChallenge structures of power. Support data due process, act as data advocates, pressure unethical analytical behavior.\n\nTo this list, Data Feminism authors Catherine D’Ignazio and Lauren F. Klein added:\n\nEmotion & Embodiment\nValue multiple forms of knowledge, including the knowledge that comes from people as living, feeling bodies in the world.\nPluralism\nThe most complete knowledge comes from synthesizing multiple perspectives\nContext\nData are not neutral or objective",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>activity-07</span>"
    ]
  },
  {
    "objectID": "activities/activity-07.html#exercises",
    "href": "activities/activity-07.html#exercises",
    "title": "5  activity-07",
    "section": "5.2 Exercises",
    "text": "5.2 Exercises\n\nExercise 1: Professionalism\nLet’s examine weather in 3 Australian locations.\nThe following plot is fine for things like homework or just playing around. But we’ll make it more “professional” looking below.\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nPart a\nReplace A, B, C, and D in the code below to:\n\nAdd a short, but descriptive title. Under 10 words.\nChange the x- and y-axis labels, currently just the names of the variables in the dataset. These should be short and include units.\nChange the legend title to “Location” (just for practice, not because it’s better than “location”).\n\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() + \n  labs(x = \"Temperature at 9am (ºC)\", y = \"Temperature at 3pm (ºC)\", title = \"Temperature Variation at 9am and 3pm\", color = \"Location\")  \n\n\n\n\n\n\n\n\n\n\nPart b\nWhen we’re including our plot in an article, paper, book, or other similar outlet, we should (and are expected to) provide a more descriptive figure caption. Typically, this is instead of a title and is more descriptive of what exactly is being plotted.\n\nAdd a figure caption in the top of the chunk.\nInclude your x-axis, y-axis, and legend labels from Part a.\nRender your Rmd and check out how the figure caption appears.\n\n\nggplot(weather, aes(y = temp3pm, x = temp9am, color = location)) + \n  geom_point() + \n  labs(x = \"Temperature at 9am (ºC)\", y = \"Temperature at 3pm (ºC)\", color = \"Location\")  \n\n\n\n\nScatterplot of temperature variation at 9am vs 3pm at different Australian locations\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Accessibility\nLet’s now make a graphic more accessible.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\")  \n\n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\n\n\nPart a\nLet’s add some alt text that can be picked up by screen readers. This is a great resource on writing alt text for data viz. In short, whereas figure captions are quick descriptions which assume that the viz is accessible, alt text is a longer description which assumes the viz is not accessible. Alt text should concisely articulate:\n\nWhat your visualization is (e.g. a density plot of 3pm temperatures in Hobart, Uluru, and Wollongong, Australia).\nA 1-sentence description of the most important takeaway.\nA link to your data source if it’s not already in the caption.\n\nAdd appropriate alt text at the top of the chunk, in fig-alt. Then knit your Rmd, and hover over the image in your knitted html file to check out the alt text.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\")  \n\n\n\n\nDensity plots of 3pm temperatures in 3 Australian locations.\n\n\n\n\n\n\n\n\n\n\n\nPart b\nColor is another important accessibility consideration. Let’s check out the color accessibility of our density plot.\n\nRun the ggplot() code from Part a in your console. The viz will pop up in the Plots tab.\nIn the Plots tab, click “Export” then “Save as image”. Save the image somewhere.\nNavigate to https://www.color-blindness.com/coblis-color-blindness-simulator/\nAbove the image of crayons (I think it’s crayons?), click “Choose file” and choose the plot file you just saved.\nClick the various simulator buttons (eg: Red-Weak/Protanomaly) to check out how the colors in this plot might appear to others.\nSummarize what you learn. What impact might our color choices have on one’s ability to interpret the viz?\n\n\n\nPart c\nWe can change our color schemes! There are many color-blind friendly palettes in R. In the future, we’ll set a default, more color-blind friendly color theme at the top of our Rmds. We can also do this individually for any plot that uses color. Run the chunks below to explore various options.\n\nggplot(weather, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\") + \n  scale_fill_viridis_d()    \n\n\n\n\n\n\n\n\n\n# In the color scale line:\n# Change \"fill\" to \"color\" since we use color in the aes()\n# Change \"d\" (discrete) to \"c\" (continuous) since maxtemp is on a continuous scale\nggplot(weather, aes(y = temp3pm, x = temp9am, color = maxtemp)) + \n  geom_point(alpha = 0.5) + \n  labs(x = \"3pm temperature (Celsius)\") + \n  scale_color_viridis_c()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3: Ethics\nLet’s scratch the surface of ethics in data viz. Central to this discussion is the consideration of impact.\n\nPart a\nAt a minimum, our data viz should not mislead. Reconsider the climate change example from above. Why is this plot unethical and what impact might it have on policy, public opinion, etc?\n\n\n\nPart b\nAgain, data viz ethical considerations go beyond whether or not a plot is misleading. As described in the warm-up, we need to consider: visibility, privacy, power, emotion & embodiment, pluralism, & context. Depending upon the audience and goals of a data viz, addressing these points might require more nuance. Mainly, the viz tools we’ve learned are a great base or foundation, but aren’t the only approaches to data viz. \nPick one or more of the following examples of data viz to discuss with your group. How do the approaches taken:\n\nemphasize one or more of: visibility, privacy, power, emotion, embodiment, pluralism, and/or context?\nimprove upon what we might be able to convey with a simpler bar chart, scatterplot, etc?\n\n\n\n\nExample: W.E.B. Du Bois (1868–1963)\nDu Bois (“Doo Boys”) was a “sociologist, socialist, historian, civil rights activist, Pan-Africanist, author, writer, and editor”1. He was also a pioneer in elevating emotion and embodiment in data visualization. For the Paris World Fair of 1900, Du Bois and his team of students from Atlanta University presented 60 data visualizations of the Black experience in America, less than 50 years after the abolishment of slavery. Du Bois noted: “I wanted to set down its aim and method in some outstanding way which would bring my work to notice by the thinking world.” That is, he wanted to increase the impact of his work by partnering technical visualizations with design that better connects to lived experiences. NOTE: This work uses language common to that time period and addresses the topic of slavery. Check out:\n\nA complete set of the data visualizations provided by Anthony Starks (@ajstarks).\nAn article by Allen Hillery (@AlDatavizguy).\n\n\n\n\nExample: One person’s experience with long COVID\nNYT article\n\n\n\nExample: Decolonizing data viz\nblog post\n\n\n\nExample: Visualizing climate change through art\nFutures North with Prof John Kim & Mac students (by Prof Kim, Mac research students)\n\n\n\nExample: Personal data collection\nDear Data\n\n\n\n\n\nPart c\nFor a deeper treatment of similar topics, and more examples, read Data Feminism.\n\n\n\n\n\n\n\n\nExercise 4: Critique\nPractice critiquing some more complicated data viz listed at Modern Data Science with R, Exercise 2.5.\nThink about the following questions:\n\nWhat story does the data graphic tell? What is the main message that you take away from it?\nCan the data graphic be described in terms of the Grammar of Graphics (frame, glyphs, aesthetics, facet, scale, guide)? If so, please describe.\nCritique and/or praise the visualization choices made by the designer. Do they work? Are they misleading? Thought-provoking? Are there things that you would have done differently?\n\n\n\n\n\n\n\n\nExercise 5: Design Details\nThis final exercise is just “food for thought”. It’s more of a discussion than an exercise, and gets into some of the finer design details and data viz theory. Go as deep or not deep as you’d like here.\nIn refining the details of our data viz, Visualize This and Storytelling with Data provide some of their guiding principles. But again, every context is different.\n\nPut yourself in a reader’s shoes. What parts of the data need explanation?\nShine a light on your data. Try to remove any “chart junk” that distracts from the data.\nVary color and style to emphasize the viz elements that are most important to the story you’re telling.\nIt is easier to judge length than it is to judge area or angles.\nBe thoughtful about how your categories are ordered for categorical data.\n\nGetting into even more of the nitty gritty, we need to be mindful of what geometric elements and aesthetics we use. The following elements/aesthetics are listed in roughly descending order of human ability to perceive and compare nearby objects:2\n\nPosition\nLength\nAngle\nDirection\nShape (but only a very few different shapes)\nArea\nVolume\nShade\nColor. (Color is the most difficult, because it is a 3-dimensional quantity.)\n\nFinally, here are some facts to keep in mind about visual perception from Now You See It.\n\nPart a: Selectivity\nVisual perception is selective, and our attention is often drawn to contrasts from the norm.\nImplication: We should design visualizations so that the features we want to highlight stand out in contrast from those that are not worth the audience’s attention.\nExample: What stands out in this example image? This is originally from C. Ware, Information Visualization: Perception for Design, 2004? Source: S. Few, Now You See It, 2009, p. 33.\n\n\n\nPart b: Familiarity\nOur eyes are drawn to familiar patterns. We observe what we know and expect.\nImplication: Visualizations work best when they display information as patterns that familiar and easy to spot.\nExample: Do you notice anything embedded in this rose image from coolbubble.com? Source: S. Few, Now You See It, 2009, p. 34.\n\n\n\nPart c: Revisit\nRevisit Part b. Do you notice anything in the shadows? Go to https://mac-stat.github.io/images/112/rose2.png for an image.\n\n\n\n\n\n\n\n\nWrapping up\nIf you finish early:\n\nWork on homework if not done already\nComplete any activities you haven’t finished yet, eg, spatial viz, the optional but fun exercises in the Multivariate viz and Bivariate viz activities.\nIf you’ve done all that, explore some datasets in TidyTuesday.",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>activity-07</span>"
    ]
  },
  {
    "objectID": "activities/activity-07.html#solutions",
    "href": "activities/activity-07.html#solutions",
    "title": "5  activity-07",
    "section": "5.3 Solutions",
    "text": "5.3 Solutions\nThe exercises today are discussion based. There are no “solutions”. Happy to chat in office hours about any ideas here!",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>activity-07</span>"
    ]
  },
  {
    "objectID": "activities/activity-07.html#footnotes",
    "href": "activities/activity-07.html#footnotes",
    "title": "5  activity-07",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/W._E._B._Du_Bois↩︎\nB. S. Baumer, D. T. Kaplan, and N. J. Horton, Modern Data Science with R, 2017, p. 15.↩︎",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>activity-07</span>"
    ]
  },
  {
    "objectID": "activities/activity-08.html",
    "href": "activities/activity-08.html",
    "title": "6  Wrangling Verbs",
    "section": "",
    "text": "6.1 Data Wrangling Motivation\nRecall the elections data by U.S. county:\n# Load tidyverse & data\nlibrary(tidyverse)\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\")\nWe’ve used data viz to explore some general patterns in the election outcomes. For example, a map!\n# Get a background map\nlibrary(socviz)\ndata(county_map)\n\n# Make a choropleth map\nlibrary(RColorBrewer)  # For the color scale\nlibrary(ggthemes) # For theme_map\nelections |&gt; \n  mutate(county_fips = as.character(county_fips)) |&gt; \n  mutate(county_fips = \n           ifelse(nchar(county_fips) == 4, paste0(\"0\", county_fips), county_fips)) |&gt; \n  ggplot(aes(map_id = county_fips, fill = cut(repub_pct_20, breaks = seq(0, 100, by = 10)))) +\n    geom_map(map = county_map) +\n    scale_fill_manual(values = rev(brewer.pal(10, \"RdBu\")), name = \"% Republican\") +\n    expand_limits(x = county_map$long, y = county_map$lat)  + \n    theme_map() +\n    theme(legend.position = \"right\") + \n    coord_equal()\nConsider some fairly basic follow-up questions, each of which we cannot answer precisely (or sometimes even at all) using our data viz tools:",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wrangling Verbs</span>"
    ]
  },
  {
    "objectID": "activities/activity-08.html#data-wrangling-motivation",
    "href": "activities/activity-08.html#data-wrangling-motivation",
    "title": "6  Wrangling Verbs",
    "section": "",
    "text": "How many total people voted for the Democratic and Republican candidates in 2020?\nWhat about in each state?\nIn just the state of Minnesota:\n\nWhich counties had the highest and lowest Democratic vote in 2020?\nHow did the Democratic vote in each county change from 2016 to 2020?\n\n\n\n\nGoals of Unit 2: Data Wrangling\nWe really cannot do anything with data (viz, modeling, etc) unless we can wrangle the data. The following is a typical quote. I agree with the 90% – data wrangling isn’t something we have to do before we can do data science, it is data science! But let’s rethink the 10% – data wrangling is a fun and empowering puzzle!\n\nThe goals of Unit 2 are to explore how to:\n\nGet data into the tidy shape / format we need for analysis. For example, we might want to:\n\nkeep only certain observations\ndefine new variables\nreformat or “clean” existing variables\ncombine various datasets\nprocess “string” or text data\n\nNumerically (not just visually) explore and summarize various characteristics of the variables in our dataset.\n\n\n\n\n\n\n\n\nTidyverse Wrangling Tools\nWe’ll continue to use packages that are part of the tidyverse which share a common general grammar and structure.",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wrangling Verbs</span>"
    ]
  },
  {
    "objectID": "activities/activity-08.html#warm-up",
    "href": "activities/activity-08.html#warm-up",
    "title": "6  Wrangling Verbs",
    "section": "6.2 Warm-Up",
    "text": "6.2 Warm-Up\nThere are lots and lots of steps that can go into data wrangling, thus lots and lots of relevant R functions. BUT just 6 functions can get us very far. People refer to these as the 6 main wrangling verbs or functions:\n\nwhy “verbs”? in the tidyverse grammar, functions serve as action words\nthe 6 verbs are all stored in the dplyr package within the tidyverse\neach verb acts on a data frame and returns a data frame\n\n\n\n\n\n\n\nverb\naction\n\n\n\n\narrange\narrange the rows according to some column\n\n\nfilter\nfilter out or obtain a subset of the rows\n\n\nselect\nselect a subset of columns\n\n\nmutate\nmutate or create a column\n\n\nsummarize\ncalculate a numerical summary of a column\n\n\ngroup_by\ngroup the rows by a specified column\n\n\n\n\n\n\n\n\nEXAMPLE 1\nWhich verb would help us…\n\nkeep only information about state names, county names, and the 2020 and 2016 Democratic support (not the 2012 results, demographics, etc)\nget only the data on Minnesota\ndefine a new variable which calculates the change in Democratic support from 2016 to 2020, using dem_pct_20 and dem_pct_16\nsort the counties from highest to lowest Democratic support\ndetermine the total number of votes cast across all counties\n\n\n\n\n\n\nEXAMPLE 2: select columns\nTo get a sense for the code structure, let’s explore a couple verbs together. To start, let’s simplify our dataset to include only some variables of interest. Specifically, select() only the columns relevant to state names, county names, and the 2020 and 2016 Democratic support:\n\n# What's the first argument? The second?\nselect(elections, c(state_name, county_name, dem_pct_20, dem_pct_16))\n\nLet’s re-do this with the pipe function |&gt;:\n\nelections |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16)\n\n\n\n\n\n\n\n\n\n\n\n\nPipe Function |&gt;\n\n\n\n|&gt; “passes” objects, usually datasets, to a function:\nobject |&gt; function() is the same as function(object)\n\n\n\n\n\n\n\nEXAMPLE 3: filter rows\nLet’s filter() out only the rows related to Minnesota (MN):\n\n# Without a pipe\nfilter(elections, state_name == \"Minnesota\")\n\n\n# With a pipe\nelections |&gt; \n  filter(state_name == \"Minnesota\")\n\n\n\n\n\n\n\n\n\n\n\n\n== vs =\n\n\n\nWe use a == b to check whether a matches b.\nWe use a = b to define that a is equal to b. We typically use = for this purpose inside a function, and &lt;- for this purpose outside a function.\n\n# Ex: \"=\" defines x\nx = 2\nx\n\n[1] 2\n\n\n\n# Ex: \"==\" checks whether x is/matches 3\nx == 3\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\nEXAMPLE 4: filter and select\nLet’s combine select() and filter() to create a new dataset with info about the county names, and 2020 and 2016 Democratic support among Minnesota counties.\n\n# Without pipes\nfilter(select(elections, c(state_name, county_name, dem_pct_20, dem_pct_16)), state_name == \"Minnesota\")\n\n\n# With pipes: all verbs in 1 row\nelections |&gt; select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt; filter(state_name == \"Minnesota\")\n\n\n# With pipes: each verb in a new row\nelections |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt; \n  filter(state_name == \"Minnesota\")\n\n\n# We can even do this with UN-tidyverse code in \"base\" R\nelections[elections$state_name == \"Minnesota\", c(1, 4, 8, 12)]\n\n\n\n\n\n\n\nReflection\n\n\n\nWhy will we typically use:\n\ntidyverse code\nthe pipe function |&gt;\neach verb on a new row\n\n\n\n\n\n\n\n\nEXAMPLE 5: order of operations\nSometimes, the order of operations matters. For example: putting on socks, then shoes produces a different result than putting on shoes, then socks. Sometimes order doesn’t matter. For example: Pouring cereal into a bowl, then adding milk produces the same result as pouring milk into a bowl, then adding cereal (though one order is obviously better than the other ;)) Above (also copied below), we selected some columns and then filtered some rows:\n\nelections |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt; \n  filter(state_name == \"Minnesota\")\n\nWould we get the same result if we reversed select() and filter()? Think first, then try it.\n\n# Try it\n\n\n\n\n\n\nEXAMPLE 6: Storing the results\nTypically:\n\nWe want to store our data wrangling results.\nIt’s good practice to do so under a new name. We want to preserve, thus don’t want to overwrite, the original data (especially if our code contains errors!!).\n\n\n# Store the results\nmn &lt;- elections |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt; \n  filter(state_name == \"Minnesota\")\n\n# Always check it out to confirm it's what you want it to be!\nhead(mn)\n\n  state_name      county_name dem_pct_20 dem_pct_16\n1  Minnesota    Aitkin County      35.98      34.12\n2  Minnesota     Anoka County      47.79      41.01\n3  Minnesota    Becker County      33.96      30.47\n4  Minnesota  Beltrami County      47.24      40.76\n5  Minnesota    Benton County      32.70      28.33\n6  Minnesota Big Stone County      35.41      33.75\n\nnrow(mn)\n\n[1] 87\n\nnrow(elections)\n\n[1] 3109",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wrangling Verbs</span>"
    ]
  },
  {
    "objectID": "activities/activity-08.html#exercises",
    "href": "activities/activity-08.html#exercises",
    "title": "6  Wrangling Verbs",
    "section": "6.3 Exercises",
    "text": "6.3 Exercises\n\nExercise 1: select practice\n6 verbs: select, filter, arrange, mutate, summarize, group_by\nUse select() to create a simplified dataset that we’ll use throughout the exercises below.\n\nStore this dataset as elections_small.\nOnly keep the following variables: state_name, county_name, total_votes_20, repub_pct_20, dem_pct_20, total_votes_16, dem_pct_16\n\n\n# Define elections_small\n#___ &lt;- ___ |&gt;\n#  ___(___)\n\nelections_small &lt;- elections %&gt;% \n  select(state_name, county_name, total_votes_20, repub_pct_20, dem_pct_20, total_votes_16, dem_pct_16)\n# Check out the first 6 rows to confirm your code did what you think it did!\nhead(elections_small)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16\n1          24661      23.96\n2          94090      19.57\n3          10390      46.66\n4           8748      21.42\n5          25384       8.47\n6           4701      75.09\n\n\n\n\n\n\n\n\n\nExercise 2: filter demo\n6 verbs: select, filter, arrange, mutate, summarize, group_by\nWhereas select() selects certain variables or columns, filter() keeps certain units of observation or rows relative to their outcome on certain variables. To this end, we must:\n\nIdentify the variable(s) that are relevant to the filter.\nUse a “logical comparison operator” to define which values of the variable to keep:\n\n\n\nsymbol\nmeaning\n\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n%in% c(???, ???)\na list of multiple values\n\n\n\nUse quotes \"\" when specifying outcomes of interest for a categorical variable.\n\n\n\n\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\n\n\n\n\n# Keep only data on counties in Hawaii\nelections_small |&gt;\n  filter(state_name == \"Hawaii\")\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1     Hawaii   Hawaii County          87814        30.63      66.88\n2     Hawaii Honolulu County         382114        35.66      62.51\n3     Hawaii    Kauai County          33497        34.58      63.36\n4     Hawaii     Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          64865      63.61\n2         285683      61.48\n3          26335      62.49\n4          51942      64.45\n\n\n\n# What does this do?\nelections_small |&gt; \n   filter(state_name %in% c(\"Hawaii\", \"Delaware\"))\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1   Delaware       Kent County          87025        47.12      51.19\n2   Delaware New Castle County         287633        30.72      67.81\n3   Delaware     Sussex County         129352        55.07      43.82\n4     Hawaii     Hawaii County          87814        30.63      66.88\n5     Hawaii   Honolulu County         382114        35.66      62.51\n6     Hawaii      Kauai County          33497        34.58      63.36\n7     Hawaii       Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          74253      44.91\n2         261468      62.30\n3         105814      37.17\n4          64865      63.61\n5         285683      61.48\n6          26335      62.49\n7          51942      64.45\n\n\n\n# Keep only data on counties where the Republican got MORE THAN 93.97% of the vote in 2020\n# THINK: What variable is relevant here?\nelections_small %&gt;% \n  filter(repub_pct_20 &gt; 93.97)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  Borden County            416        95.43       3.85\n2      Texas    King County            159        94.97       5.03\n3      Texas Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            365       8.49\n2            159       3.14\n3            550       3.64\n\n\n\n# Keep only data on counties where the Republican got AT LEAST 93.97% of the vote in 2020\n# This should have 1 more row than your answer above\nelections_small %&gt;% \n  filter(repub_pct_20 &gt;= 93.97)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Montana Garfield County            813        93.97       5.04\n2      Texas   Borden County            416        95.43       3.85\n3      Texas     King County            159        94.97       5.03\n4      Texas  Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            715       4.76\n2            365       8.49\n3            159       3.14\n4            550       3.64\n\n\nWe can also filter with respect to 2 rules! Here, think what variables are relevant.\n\n# Keep only data on counties in Texas where the Democrat got more than 65% of the vote in 2020\n# Do this 2 ways.\n# Method 1: 2 filters with 1 condition each\nelections_small |&gt; \n  filter(state_name == \"Texas\") |&gt; \n  filter(dem_pct_20 &gt; 65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n# Method 2: 1 filter with 2 conditions\nelections_small |&gt; \n  filter(state_name == \"Texas\", dem_pct_20 &gt; 65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n\n\n\n\n\n\n\n\nExercise 3: arrange demo\n6 verbs: select, filter, arrange, mutate, summarize, group_by\narrange() arranges or sorts the rows in a dataset according to a given column or variable, in ascending or descending order:\narrange(variable), arrange(desc(variable))\n\n# Arrange the counties in elections_small from lowest to highest percentage of 2020 Republican support\n# Print out just the first 6 rows\nelections_small |&gt; \n   arrange(repub_pct_20) |&gt; \n   head()\n\n            state_name            county_name total_votes_20 repub_pct_20\n1 District of Columbia   District of Columbia         344356         5.40\n2             Maryland Prince George's County         424855         8.73\n3             Maryland         Baltimore city         237461        10.69\n4             Virginia        Petersburg city          14118        11.22\n5             New York        New York County         694904        12.26\n6           California   San Francisco County         443458        12.72\n  dem_pct_20 total_votes_16 dem_pct_16\n1      92.15         280272      92.85\n2      89.26         351091      89.33\n3      87.28         208980      85.44\n4      87.75          13717      87.52\n5      86.78         591368      87.17\n6      85.27         365295      85.53\n\n\n\n# Arrange the counties in elections_small from highest to lowest percentage of 2020 Republican support\n# Print out just the first 6 rows\nelections_small |&gt; \n   arrange(desc(repub_pct_20)) |&gt; \n   head()\n\n  state_name      county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas   Roberts County            550        96.18       3.09\n2      Texas    Borden County            416        95.43       3.85\n3      Texas      King County            159        94.97       5.03\n4    Montana  Garfield County            813        93.97       5.04\n5      Texas Glasscock County            653        93.57       5.97\n6   Nebraska     Grant County            402        93.28       4.98\n  total_votes_16 dem_pct_16\n1            550       3.64\n2            365       8.49\n3            159       3.14\n4            715       4.76\n5            602       5.65\n6            394       5.08\n\n\n\n\n\n\n\n\n\nExercise 4: mutate demo\n6 verbs: select, filter, arrange, mutate, summarize, group_by\nmutate() can either transform / mutate an existing variable (column), or define a new variable based on existing ones.\n\nPart a\n\n# What did this code do?\n elections_small |&gt; \n   mutate(diff_20 = repub_pct_20 - dem_pct_20) |&gt; \n   head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 diff_20\n1          24661      23.96   44.42\n2          94090      19.57   53.76\n3          10390      46.66    7.66\n4           8748      21.42   57.73\n5          25384       8.47   80.00\n6           4701      75.09  -49.86\n\n\n\n# What did this code do?\nelections_small |&gt; \n   mutate(repub_votes_20 = round(total_votes_20 * repub_pct_20/100)) |&gt; \n   head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_votes_20\n1          24661      23.96          19839\n2          94090      19.57          83542\n3          10390      46.66           5622\n4           8748      21.42           7525\n5          25384       8.47          24711\n6           4701      75.09           1146\n\n\n\n# What did this code do?\nelections_small |&gt; \n   mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt; \n   head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_win_20\n1          24661      23.96         TRUE\n2          94090      19.57         TRUE\n3          10390      46.66         TRUE\n4           8748      21.42         TRUE\n5          25384       8.47         TRUE\n6           4701      75.09        FALSE\n\n\n\n\nPart b\n\n# You try\n# Define a variable that calculates the change in Dem support in 2020 vs 2016\nelections_small |&gt; \n  mutate(dem_support_change = dem_pct_20 - dem_pct_16) |&gt; \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 dem_support_change\n1          24661      23.96               3.06\n2          94090      19.57               2.84\n3          10390      46.66              -0.87\n4           8748      21.42              -0.72\n5          25384       8.47               1.10\n6           4701      75.09              -0.39\n\n\n\n# You try\n# Define a variable that determines whether the Dem support was higher in 2020 than in 2016 (TRUE/FALSE)\nelections_small |&gt; \n  mutate(dem_higher_20 = dem_pct_20 &gt; dem_pct_16) |&gt; \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 dem_higher_20\n1          24661      23.96          TRUE\n2          94090      19.57          TRUE\n3          10390      46.66         FALSE\n4           8748      21.42         FALSE\n5          25384       8.47          TRUE\n6           4701      75.09         FALSE\n\n\n\n\n\n\n\n\n\n\nExercise 5: Pipe series\nLet’s now combine these verbs into a pipe series!\n\nPart a\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE running the below chunk, what do you think it will produce?\n\n\n\n elections_small |&gt; \n   filter(state_name == \"Wisconsin\",\n          repub_pct_20 &lt; dem_pct_20) |&gt; \n   arrange(desc(total_votes_20)) |&gt; \n   head()\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1  Wisconsin  Milwaukee County         458971        29.27      69.13\n2  Wisconsin       Dane County         344791        22.85      75.46\n3  Wisconsin       Rock County          85360        43.51      54.66\n4  Wisconsin  La Crosse County          67884        42.25      55.75\n5  Wisconsin Eau Claire County          58275        43.49      54.26\n6  Wisconsin    Portage County          40603        47.53      50.31\n  total_votes_16 dem_pct_16\n1         434970      66.44\n2         304729      71.38\n3          75043      52.42\n4          62785      51.61\n5          54080      50.43\n6          38123      48.59\n\n\n\n\nPart b\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE trying, what do you think will happen if you change the order of filter and arrange:\n\nthe results will be the same\nwe’ll get an error\nwe won’t get an error, but the results will be different\n\n\n\n\n# Now try it. Change the order of filter and arrange below.\n elections_small |&gt; \n   filter(state_name == \"Wisconsin\",\n          repub_pct_20 &lt; dem_pct_20) |&gt; \n   arrange(desc(total_votes_20)) |&gt; \n   head()\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1  Wisconsin  Milwaukee County         458971        29.27      69.13\n2  Wisconsin       Dane County         344791        22.85      75.46\n3  Wisconsin       Rock County          85360        43.51      54.66\n4  Wisconsin  La Crosse County          67884        42.25      55.75\n5  Wisconsin Eau Claire County          58275        43.49      54.26\n6  Wisconsin    Portage County          40603        47.53      50.31\n  total_votes_16 dem_pct_16\n1         434970      66.44\n2         304729      71.38\n3          75043      52.42\n4          62785      51.61\n5          54080      50.43\n6          38123      48.59\n\n\n\n\nPart c\nSo the order of filter() and arrange() did not matter – rerranging them produces the same results. BUT what is one advantage of filtering before arranging?\nIf it’s a larger dataset it can be more efficient to remove rows before arranging them.\n\n\nPart d\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE running the below chunk, what do you think it will produce?\n\n\n\nelections_small |&gt; \n   filter(state_name == \"Delaware\") |&gt; \n   mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt; \n   select(county_name, repub_pct_20, dem_pct_20, repub_win_20)\n\n        county_name repub_pct_20 dem_pct_20 repub_win_20\n1       Kent County        47.12      51.19        FALSE\n2 New Castle County        30.72      67.81        FALSE\n3     Sussex County        55.07      43.82         TRUE\n\n\n\n\nPart e\n\n\n\n\n\n\nThink then Run\n\n\n\nBEFORE trying, what do you think will happen if you change the order of mutate and select:\n\nthe results will be the same\nwe’ll get an error\nwe won’t get an error, but the results will be different\n\n\n\n\n# Now try it. Change the order of mutate and select below.\nelections_small |&gt; \n   filter(state_name == \"Delaware\") |&gt; \n   mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt; \n   select(county_name, repub_pct_20, dem_pct_20, repub_win_20)\n\n        county_name repub_pct_20 dem_pct_20 repub_win_20\n1       Kent County        47.12      51.19        FALSE\n2 New Castle County        30.72      67.81        FALSE\n3     Sussex County        55.07      43.82         TRUE\n\n\n\n\n\n\n\n\n\n\nExercise 6: DIY pipe series\nWe’ve now learned 4 of the 6 wrangling verbs: select, filter, mutate, arrange. Let’s practice combining these into pipe series. Here are some hot tips:\n\nBefore writing any code, translate the prompt: how many distinct wrangling steps are needed and what verb do we need in each step?\nAdd each verb one at a time – don’t try writing a whole chunk at once.\n\n\n\nPart a\nShow just the counties in Minnesota and their Democratic 2020 vote percentage, from highest to lowest. Your answer should have just 2 columns.\n\nelections_small %&gt;% \n  filter(state_name == \"Minnesota\") %&gt;% \n  select(state_name, dem_pct_20)\n\n   state_name dem_pct_20\n1   Minnesota      35.98\n2   Minnesota      47.79\n3   Minnesota      33.96\n4   Minnesota      47.24\n5   Minnesota      32.70\n6   Minnesota      35.41\n7   Minnesota      50.84\n8   Minnesota      32.48\n9   Minnesota      49.58\n10  Minnesota      46.37\n11  Minnesota      34.68\n12  Minnesota      33.67\n13  Minnesota      34.15\n14  Minnesota      50.74\n15  Minnesota      26.76\n16  Minnesota      65.58\n17  Minnesota      30.03\n18  Minnesota      34.17\n19  Minnesota      55.73\n20  Minnesota      33.47\n21  Minnesota      32.56\n22  Minnesota      31.98\n23  Minnesota      37.48\n24  Minnesota      40.96\n25  Minnesota      41.23\n26  Minnesota      35.58\n27  Minnesota      70.46\n28  Minnesota      42.42\n29  Minnesota      34.42\n30  Minnesota      29.45\n31  Minnesota      40.61\n32  Minnesota      29.99\n33  Minnesota      30.02\n34  Minnesota      36.12\n35  Minnesota      38.12\n36  Minnesota      38.41\n37  Minnesota      35.79\n38  Minnesota      50.64\n39  Minnesota      27.87\n40  Minnesota      33.73\n41  Minnesota      30.08\n42  Minnesota      35.94\n43  Minnesota      30.64\n44  Minnesota      48.26\n45  Minnesota      25.33\n46  Minnesota      30.02\n47  Minnesota      28.58\n48  Minnesota      29.98\n49  Minnesota      22.33\n50  Minnesota      46.00\n51  Minnesota      29.60\n52  Minnesota      50.31\n53  Minnesota      33.65\n54  Minnesota      40.80\n55  Minnesota      54.16\n56  Minnesota      32.85\n57  Minnesota      35.29\n58  Minnesota      33.87\n59  Minnesota      26.44\n60  Minnesota      34.88\n61  Minnesota      35.27\n62  Minnesota      71.50\n63  Minnesota      31.47\n64  Minnesota      28.43\n65  Minnesota      30.71\n66  Minnesota      48.76\n67  Minnesota      29.69\n68  Minnesota      25.98\n69  Minnesota      56.64\n70  Minnesota      45.52\n71  Minnesota      32.48\n72  Minnesota      28.60\n73  Minnesota      37.58\n74  Minnesota      37.47\n75  Minnesota      37.80\n76  Minnesota      34.35\n77  Minnesota      24.79\n78  Minnesota      35.46\n79  Minnesota      35.78\n80  Minnesota      26.35\n81  Minnesota      33.65\n82  Minnesota      53.46\n83  Minnesota      38.20\n84  Minnesota      29.91\n85  Minnesota      49.07\n86  Minnesota      34.49\n87  Minnesota      30.54\n\n\n\nPart b\nCreate a new dataset named mn_wi that sorts the counties in Minnesota and Wisconsin from lowest to highest in terms of the change in Democratic vote percentage in 2020 vs 2016. This dataset should include the following variables (and only these variables): state_name, county_name, dem_pct_20, dem_pct_16, and a variable measuring the change in Democratic vote percentage in 2020 vs 2016.\n\n# Define the dataset\n# Only store the results once you're confident that they're correct\nmn_wi &lt;- elections_small %&gt;% \n  select(state_name, county_name, dem_pct_20, dem_pct_16) %&gt;% \n  filter(state_name %in% c(\"Minnesota\", \"Wisconsin\")) %&gt;% \n  mutate(dem_pct_change = dem_pct_20 - dem_pct_16) %&gt;% \n  arrange(dem_pct_change)\n\n# Check out the first 6 rows to confirm your results\nhead(mn_wi)\n\n  state_name        county_name dem_pct_20 dem_pct_16 dem_pct_change\n1  Minnesota     Stevens County      37.80      39.55          -1.75\n2  Wisconsin      Forest County      34.06      35.12          -1.06\n3  Wisconsin    Kewaunee County      32.87      33.73          -0.86\n4  Wisconsin       Clark County      30.37      31.19          -0.82\n5  Wisconsin       Adams County      36.63      37.40          -0.77\n6  Wisconsin Trempealeau County      40.86      41.57          -0.71\n\n\n\n\nPart c\nConstruct and discuss a plot of the county-level change in Democratic vote percent in 2020 vs 2016, and how this differs between Minnesota and Wisconsin.\n\n\n\n\n\n\n\n\nExercise 7: summarize demo\n6 verbs: select, filter, arrange, mutate, summarize, group_by\nLet’s talk about the last 2 verbs. summarize() (or equivalently summarise()) takes an entire data frame as input and outputs a single row with one or more summary statistics. For each chunk below, indicate what the code does.\n\n# What does this do?\n# elections_small |&gt; \n#   summarize(median(repub_pct_20))\n\n\n# What does this do?\n# elections_small |&gt; \n#   summarize(median_repub = median(repub_pct_20))\n\n\n# What does this do?\n# elections_small |&gt; \n#   summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20))\n\n\n\n\n\n\n\n\nExercise 8: summarize + group_by demo\n6 verbs: select, filter, arrange, mutate, summarize, group_by\nFinally, group_by() groups the units of observation or rows of a data frame by a specified set of variables. Alone, this function doesn’t change the appearance of our dataset or seem to do anything at all:\n\n# elections_small |&gt; \n#   group_by(state_name) |&gt; \n#   head()\n\nThough it does change the underlying structure of the dataset:\n\n# Check out the structure before and after group_by\n# elections_small |&gt; \n#   class()\n# \n# elections_small |&gt; \n#   group_by(state_name) |&gt; \n#   class()\n\nWhere it really shines is in partnership with summarize().\n\n# What does this do?\n# (What if we didn't use group_by?)\n# elections_small |&gt; \n#   group_by(state_name) |&gt; \n#   summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20)) \n\n\n\n\n\n\n\nReflect\n\n\n\nNotice that group_by() with summarize() produces new data frame or tibble! But the units of observation are now states instead of counties within states.\n\n\n\n\n\n\n\n\n\nExercise 9: DIY\nLet’s practice (some of) our 6 verbs: select, filter, arrange, mutate, summarize, group_by Remember:\n\nBefore writing any code, translate the given prompts: how many distinct wrangling steps are needed and what verb do we need in each step?\nAdd each verb one at a time.\n\n\nPart a\nNOTE: Part a is a challenge exercise. If you get really stuck, move on to Part b which is the same overall question, but with hints.\n\n# Sort the *states* from the most to least total votes cast in 2020\n\n\n# In 2020, what were the total number of votes for the Democratic candidate and the total number of votes for the Republican candidate in each *state*?\n\n\n# What states did the Democratic candidate win in 2020?\n\n\n\nPart b\n\n# Sort the states from the most to least total votes cast in 2020\n# HINT: Calculate the total number of votes in each state, then sort\n\n\n# In 2020, what were the total number of votes for the Democratic candidate and the total number of votes for the Republican candidate in each state?\n# HINT: First calculate the number of Dem and Repub votes in each *county*\n# Then group and summarize these by state\n\n\n# What states did the Democratic candidate win in 2020?\n# HINT: Start with the results from the previous chunk, and then keep only some rows\n\n\n\n\n\n\n\n\n\nExercise 10: Practice on new data\nRecall the World Cup football/soccer data from TidyTuesday:\n\nworld_cup &lt;- read.csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-11-29/worldcups.csv\")\n\nYou can find a codebook here. Use (some of) our 6 verbs (select, filter, arrange, mutate, summarize, group_by) and data viz to address the following prompts.\n\n# In what years did Brazil win the World Cup?\n\n\n# What were the 6 World Cups with the highest attendance?\n\n\n# Construct a univariate plot of goals_scored (no wrangling necessary)\n# This provides a visual summary of how the number of goals_scored varies from World Cup to World Cup\n\n\n# Let's follow up the plot with some more precise numerical summaries\n# Calculate the min, median, and max number of goals_scored across all World Cups\n# NOTE: Visually compare these numerical summaries to what you observed in the plot\n\n\n# Construct a bivariate plot of how the number of goals_scored in the World Cup has changed over the years\n# No wrangling necessary\n\n\n# Our above summaries might be a bit misleading.\n# The number of games played at the World Cup varies.\n# Construct a bivariate plot of how the typical number of goals per game has changed over the years\n\n\n\n\n\n\n\n\nExercise 11: Practice on your data\nReturn to the TidyTuesday data you’re using in Homework 3. Use your new wrangling skills to play around. What new insights can you gain?!",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wrangling Verbs</span>"
    ]
  },
  {
    "objectID": "activities/activity-08.html#solutions",
    "href": "activities/activity-08.html#solutions",
    "title": "6  Wrangling Verbs",
    "section": "6.4 Solutions",
    "text": "6.4 Solutions\n\n\nClick for Solutions\n\n\n6.4.1 EXAMPLE 1\n\nselect\nfilter\nmutate\narrange\nsummarize\n\n\n\n\n\n\n\n\nExercise 1: select practice\n\n# Define elections_small\nelections_small &lt;- elections |&gt;\n  select(state_name, county_name, total_votes_20, repub_pct_20, dem_pct_20, total_votes_16, dem_pct_16)\n\n# Check out the first 6 rows to confirm your code did what you think it did!\nhead(elections_small)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16\n1          24661      23.96\n2          94090      19.57\n3          10390      46.66\n4           8748      21.42\n5          25384       8.47\n6           4701      75.09\n\n\n\n\n\n\n\n\n\nExercise 2: filter demo\n\n# Keep only data on counties in Hawaii\nelections_small |&gt;\n filter(state_name == \"Hawaii\")\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1     Hawaii   Hawaii County          87814        30.63      66.88\n2     Hawaii Honolulu County         382114        35.66      62.51\n3     Hawaii    Kauai County          33497        34.58      63.36\n4     Hawaii     Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          64865      63.61\n2         285683      61.48\n3          26335      62.49\n4          51942      64.45\n\n\n\n# Keep counties in Hawaii AND Delaware\nelections_small |&gt; \n  filter(state_name %in% c(\"Hawaii\", \"Delaware\"))\n\n  state_name       county_name total_votes_20 repub_pct_20 dem_pct_20\n1   Delaware       Kent County          87025        47.12      51.19\n2   Delaware New Castle County         287633        30.72      67.81\n3   Delaware     Sussex County         129352        55.07      43.82\n4     Hawaii     Hawaii County          87814        30.63      66.88\n5     Hawaii   Honolulu County         382114        35.66      62.51\n6     Hawaii      Kauai County          33497        34.58      63.36\n7     Hawaii       Maui County          71044        31.14      66.59\n  total_votes_16 dem_pct_16\n1          74253      44.91\n2         261468      62.30\n3         105814      37.17\n4          64865      63.61\n5         285683      61.48\n6          26335      62.49\n7          51942      64.45\n\n\n\n# Keep only data on counties where the Republican got MORE THAN 93.97% of the vote in 2020\nelections_small |&gt; \n  filter(repub_pct_20 &gt; 93.97)\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  Borden County            416        95.43       3.85\n2      Texas    King County            159        94.97       5.03\n3      Texas Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            365       8.49\n2            159       3.14\n3            550       3.64\n\n\n\n# Keep only data on counties where the Republican got AT LEAST 93.97% of the vote in 2020\n# This should have 1 more row than your answer above\nelections_small |&gt; \n  filter(repub_pct_20 &gt;= 93.97)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Montana Garfield County            813        93.97       5.04\n2      Texas   Borden County            416        95.43       3.85\n3      Texas     King County            159        94.97       5.03\n4      Texas  Roberts County            550        96.18       3.09\n  total_votes_16 dem_pct_16\n1            715       4.76\n2            365       8.49\n3            159       3.14\n4            550       3.64\n\n\n\n# Keep only data on counties in Texas where the Democrat got more than 65% of the vote in 2020\n# Do this 2 ways.\n# Method 1: 2 filters with 1 condition each\nelections_small |&gt;\n filter(state_name == \"Texas\") |&gt;\n filter(dem_pct_20 &gt; 65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n# Method 2: 1 filter with 2 conditions\nelections_small |&gt;\n filter(state_name == \"Texas\", dem_pct_20 &gt; 65)\n\n  state_name     county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas  El Paso County         267215        31.56      66.66\n2      Texas Presidio County           2217        32.52      65.99\n3      Texas   Travis County         610349        26.43      71.41\n4      Texas   Zavala County           4379        34.03      65.40\n  total_votes_16 dem_pct_16\n1         210458      69.14\n2           2203      66.18\n3         462511      66.26\n4           3390      77.67\n\n\n\n\n\n\n\n\n\nExercise 3: arrange demo\n\n# Arrange the counties in elections_small from lowest to highest percentage of 2020 Republican support\n# Print out just the first 6 rows\nelections_small |&gt;\n  arrange(repub_pct_20) |&gt;\n  head()\n\n            state_name            county_name total_votes_20 repub_pct_20\n1 District of Columbia   District of Columbia         344356         5.40\n2             Maryland Prince George's County         424855         8.73\n3             Maryland         Baltimore city         237461        10.69\n4             Virginia        Petersburg city          14118        11.22\n5             New York        New York County         694904        12.26\n6           California   San Francisco County         443458        12.72\n  dem_pct_20 total_votes_16 dem_pct_16\n1      92.15         280272      92.85\n2      89.26         351091      89.33\n3      87.28         208980      85.44\n4      87.75          13717      87.52\n5      86.78         591368      87.17\n6      85.27         365295      85.53\n\n\n\n# Arrange the counties in elections_small from highest to lowest percentage of 2020 Republican support\n# Print out just the first 6 rows\nelections_small |&gt;\n  arrange(desc(repub_pct_20)) |&gt;\n  head()\n\n  state_name      county_name total_votes_20 repub_pct_20 dem_pct_20\n1      Texas   Roberts County            550        96.18       3.09\n2      Texas    Borden County            416        95.43       3.85\n3      Texas      King County            159        94.97       5.03\n4    Montana  Garfield County            813        93.97       5.04\n5      Texas Glasscock County            653        93.57       5.97\n6   Nebraska     Grant County            402        93.28       4.98\n  total_votes_16 dem_pct_16\n1            550       3.64\n2            365       8.49\n3            159       3.14\n4            715       4.76\n5            602       5.65\n6            394       5.08\n\n\n\n\n\n\n\n\n\nExercise 4: mutate demo\n\n# Define diff_20, the difference btwn the Repub and Dem percent in 2020\nelections_small |&gt; \n  mutate(diff_20 = repub_pct_20 - dem_pct_20) |&gt; \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 diff_20\n1          24661      23.96   44.42\n2          94090      19.57   53.76\n3          10390      46.66    7.66\n4           8748      21.42   57.73\n5          25384       8.47   80.00\n6           4701      75.09  -49.86\n\n\n\n# Define repub_votes_20, the number (not percent) of Repub votes in 2020\nelections_small |&gt; \n  mutate(repub_votes_20 = round(total_votes_20 * repub_pct_20/100)) |&gt; \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_votes_20\n1          24661      23.96          19839\n2          94090      19.57          83542\n3          10390      46.66           5622\n4           8748      21.42           7525\n5          25384       8.47          24711\n6           4701      75.09           1146\n\n\n\n# Define repub_win_20, whether the Repub won in 2020 (TRUE or FALSE!)\nelections_small |&gt; \n  mutate(repub_win_20 = repub_pct_20 &gt; dem_pct_20) |&gt; \n  head()\n\n  state_name    county_name total_votes_20 repub_pct_20 dem_pct_20\n1    Alabama Autauga County          27770        71.44      27.02\n2    Alabama Baldwin County         109679        76.17      22.41\n3    Alabama Barbour County          10518        53.45      45.79\n4    Alabama    Bibb County           9595        78.43      20.70\n5    Alabama  Blount County          27588        89.57       9.57\n6    Alabama Bullock County           4613        24.84      74.70\n  total_votes_16 dem_pct_16 repub_win_20\n1          24661      23.96         TRUE\n2          94090      19.57         TRUE\n3          10390      46.66         TRUE\n4           8748      21.42         TRUE\n5          25384       8.47         TRUE\n6           4701      75.09        FALSE\n\n\n\n\n\n\n\n\n\nExercise 5: Pipe series\n\nPart c\nIt’s more “computationally efficient” to get rid of some rows before arranging.\n\n\nPart e\nWe can’t select a variable before we define it!\n\n\n\n\n\n\n\n\nExercise 6: DIY pipe series\n\nPart a\nHere’s my translation:\n\njust the counties in Minnesota —&gt; filter\njust the counties in Minnesota and their Democratic 2020 vote percentage —&gt; select\nfrom highest to lowest —&gt; arrange\n\n\n# Remember to try this 1 line at a time\nelections_small |&gt; \n  filter(state_name == \"Minnesota\") |&gt; \n  select(county_name, dem_pct_20) |&gt; \n  arrange(desc(dem_pct_20))\n\n                county_name dem_pct_20\n1             Ramsey County      71.50\n2           Hennepin County      70.46\n3               Cook County      65.58\n4          St. Louis County      56.64\n5             Dakota County      55.73\n6            Olmsted County      54.16\n7         Washington County      53.46\n8         Blue Earth County      50.84\n9               Clay County      50.74\n10              Lake County      50.64\n11          Nicollet County      50.31\n12           Carlton County      49.58\n13            Winona County      49.07\n14              Rice County      48.76\n15          Mahnomen County      48.26\n16             Anoka County      47.79\n17          Beltrami County      47.24\n18            Carver County      46.37\n19             Mower County      46.00\n20             Scott County      45.52\n21           Houston County      42.42\n22           Goodhue County      41.23\n23          Freeborn County      40.96\n24            Norman County      40.80\n25            Itasca County      40.61\n26       Koochiching County      38.41\n27          Watonwan County      38.20\n28           Kittson County      38.12\n29           Stevens County      37.80\n30           Stearns County      37.58\n31          Fillmore County      37.48\n32            Steele County      37.47\n33         Kandiyohi County      36.12\n34            Aitkin County      35.98\n35              Lyon County      35.94\n36     Lac qui Parle County      35.79\n37           Wabasha County      35.78\n38             Grant County      35.58\n39          Traverse County      35.46\n40         Big Stone County      35.41\n41        Pennington County      35.29\n42              Pope County      35.27\n43              Polk County      34.88\n44              Cass County      34.68\n45            Wright County      34.49\n46           Hubbard County      34.42\n47             Swift County      34.35\n48         Crow Wing County      34.17\n49           Chisago County      34.15\n50            Becker County      33.96\n51              Pine County      33.87\n52          Le Sueur County      33.73\n53          Chippewa County      33.67\n54            Nobles County      33.65\n55            Waseca County      33.65\n56             Dodge County      33.47\n57        Otter Tail County      32.85\n58            Benton County      32.70\n59           Douglas County      32.56\n60             Brown County      32.48\n61         Sherburne County      32.48\n62         Faribault County      31.98\n63          Red Lake County      31.47\n64          Renville County      30.71\n65            McLeod County      30.64\n66   Yellow Medicine County      30.54\n67           Lincoln County      30.08\n68        Cottonwood County      30.03\n69           Kanabec County      30.02\n70            Martin County      30.02\n71           Jackson County      29.99\n72        Mille Lacs County      29.98\n73            Wilkin County      29.91\n74              Rock County      29.69\n75            Murray County      29.60\n76            Isanti County      29.45\n77            Sibley County      28.60\n78            Meeker County      28.58\n79           Redwood County      28.43\n80 Lake of the Woods County      27.87\n81        Clearwater County      26.76\n82         Pipestone County      26.44\n83            Wadena County      26.35\n84            Roseau County      25.98\n85          Marshall County      25.33\n86              Todd County      24.79\n87          Morrison County      22.33\n\n\n\n\nPart b\nHere’s my translation:\n\ncounties in Minnesota and Wisconsin —&gt; filter\nchange in Democratic vote percentage in 2020 vs 2016 —&gt; mutate (we don’t already have this)\nsorts the counties from highest to lowest —&gt; arrange\ninclude the following variables (and only these variables) —&gt; select\n\n\n# Remember to try this 1 line at a time before storing!\nmn_wi &lt;- elections_small |&gt; \n  filter(state_name %in% c(\"Minnesota\", \"Wisconsin\")) |&gt; \n  select(state_name, county_name, dem_pct_20, dem_pct_16) |&gt;\n  mutate(dem_change = dem_pct_20 - dem_pct_16) |&gt; \n  arrange(dem_change)\n  \n# Check it out\nhead(mn_wi)\n\n  state_name        county_name dem_pct_20 dem_pct_16 dem_change\n1  Minnesota     Stevens County      37.80      39.55      -1.75\n2  Wisconsin      Forest County      34.06      35.12      -1.06\n3  Wisconsin    Kewaunee County      32.87      33.73      -0.86\n4  Wisconsin       Clark County      30.37      31.19      -0.82\n5  Wisconsin       Adams County      36.63      37.40      -0.77\n6  Wisconsin Trempealeau County      40.86      41.57      -0.71\n\n\n\n\nPart c\nThere was a stronger Dem shift from 2016 to 2020 in Minnesota. Further, in most counties across both states, the percent Dem tended to be higher in 2020 than in 2016.\n\nggplot(mn_wi, aes(x = dem_change, fill = state_name)) + \n  geom_density(alpha = 0.5)\n\n\n\n\n\n\n\nggplot(mn_wi, aes(y = dem_change, x = state_name)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7: summarize demo\n\n# Calculate the median Repub vote percentage in 2020 across all counties\nelections_small |&gt; \n  summarize(median(repub_pct_20))\n\n  median(repub_pct_20)\n1                68.29\n\n\n\n# Calculate the median Repub vote percentage in 2020 across all counties\n# AND name it \"median_repub\"\nelections_small |&gt; \n  summarize(median_repub = median(repub_pct_20))\n\n  median_repub\n1        68.29\n\n\n\n# Calculate the median Repub vote percentage in 2020 across all counties\n# AND the total number of votes across all counties\n# AND name the results\nelections_small |&gt; \n  summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20))\n\n  median_repub total_votes\n1        68.29   157949293\n\n\n\n\n\n\n\n\n\nExercise 8: summarize + group_by demo\n\n# Calculate the median 2020 Repub percent and total votes BY STATE\nelections_small |&gt; \n  group_by(state_name) |&gt; \n  summarize(median_repub = median(repub_pct_20), total_votes = sum(total_votes_20)) \n\n# A tibble: 50 × 3\n   state_name           median_repub total_votes\n   &lt;chr&gt;                       &lt;dbl&gt;       &lt;int&gt;\n 1 Alabama                      70.6     2323304\n 2 Arizona                      57.9     3387326\n 3 Arkansas                     72.1     1219069\n 4 California                   44.8    17495906\n 5 Colorado                     56.2     3256953\n 6 Connecticut                  41.0     1824280\n 7 Delaware                     47.1      504010\n 8 District of Columbia          5.4      344356\n 9 Florida                      64.6    11067456\n10 Georgia                      68       4997716\n# ℹ 40 more rows\n\n\n\n\n\n\n\n\n\nExercise 9: DIY\n\nPart a\n\n# Sort the states from the most to least total votes in 2020\nelections_small |&gt; \n  group_by(state_name) |&gt; \n  summarize(total = sum(total_votes_20)) |&gt; \n  arrange(desc(total))\n\n# A tibble: 50 × 2\n   state_name        total\n   &lt;chr&gt;             &lt;int&gt;\n 1 California     17495906\n 2 Texas          11317911\n 3 Florida        11067456\n 4 New York        8616205\n 5 Pennsylvania    6925255\n 6 Illinois        6038850\n 7 Ohio            5922202\n 8 Michigan        5539302\n 9 North Carolina  5524801\n10 Georgia         4997716\n# ℹ 40 more rows\n\n\n\n# In 2020, what were the total number of votes for the Democratic candidate and the total number of votes for the Republican candidate in each *state*?\nelections_small |&gt; \n  mutate(dem_votes_20 = round(total_votes_20 * dem_pct_20 / 100), \n         repub_votes_20 = round(total_votes_20 * repub_pct_20 / 100)) |&gt; \n  group_by(state_name) |&gt; \n  summarize(dem_total = sum(dem_votes_20),\n            repub_total = sum(repub_votes_20))\n\n# A tibble: 50 × 3\n   state_name           dem_total repub_total\n   &lt;chr&gt;                    &lt;dbl&gt;       &lt;dbl&gt;\n 1 Alabama                 849664     1441155\n 2 Arizona                1672127     1661671\n 3 Arkansas                423919      760641\n 4 California            11109642     6006031\n 5 Colorado               1804393     1364627\n 6 Connecticut            1080677      715315\n 7 Delaware                296274      200601\n 8 District of Columbia    317324       18595\n 9 Florida                5297131     5668600\n10 Georgia                2473661     2461869\n# ℹ 40 more rows\n\n\n\n# What states did the Democratic candidate win in 2020?\nelections_small |&gt; \n  mutate(dem_votes_20 = round(total_votes_20 * dem_pct_20 / 100), \n         repub_votes_20 = round(total_votes_20 * repub_pct_20 / 100)) |&gt; \n  group_by(state_name) |&gt; \n  summarize(dem_total = sum(dem_votes_20),\n            repub_total = sum(repub_votes_20)) |&gt; \n  filter(dem_total &gt; repub_total)\n\n# A tibble: 26 × 3\n   state_name           dem_total repub_total\n   &lt;chr&gt;                    &lt;dbl&gt;       &lt;dbl&gt;\n 1 Arizona                1672127     1661671\n 2 California            11109642     6006031\n 3 Colorado               1804393     1364627\n 4 Connecticut            1080677      715315\n 5 Delaware                296274      200601\n 6 District of Columbia    317324       18595\n 7 Georgia                2473661     2461869\n 8 Hawaii                  366121      196865\n 9 Illinois               3471916     2446931\n10 Maine                   430466      359897\n# ℹ 16 more rows\n\n\n\n\n\n\n\n\n\n\nExercise 10: Practice on new data\n\n# In what years did Brazil win the World Cup?\nworld_cup |&gt; \n  filter(winner == \"Brazil\")\n\n  year               host winner         second        third       fourth\n1 1958             Sweden Brazil         Sweden       France West Germany\n2 1962              Chile Brazil Czechoslovakia        Chile   Yugoslavia\n3 1970             Mexico Brazil          Italy West Germany      Uruguay\n4 1994                USA Brazil          Italy       Sweden     Bulgaria\n5 2002 Japan, South Korea Brazil        Germany       Turkey  South Korea\n  goals_scored teams games attendance\n1          126    16    35     868000\n2           89    16    32     776000\n3           95    16    32    1673975\n4          141    24    52    3568567\n5          161    32    64    2724604\n\n\n\n# What were the 6 World Cups with the highest attendance?\nworld_cup |&gt; \n  arrange(desc(attendance)) |&gt; \n  head()\n\n  year               host  winner    second       third      fourth\n1 1994                USA  Brazil     Italy      Sweden    Bulgaria\n2 2014             Brazil Germany Argentina Netherlands      Brazil\n3 2006            Germany   Italy    France     Germany    Portugal\n4 2018             Russia  France   Croatia     Belgium     England\n5 1998             France  France    Brazil     Croatia Netherlands\n6 2002 Japan, South Korea  Brazil   Germany      Turkey South Korea\n  goals_scored teams games attendance\n1          141    24    52    3568567\n2          171    32    64    3441450\n3          147    32    64    3367000\n4          169    32    64    3031768\n5          171    32    64    2859234\n6          161    32    64    2724604\n\n\n\n# Construct a univariate plot of goals_scored (no wrangling necessary)\n# This provides a visual summary of how the number of goals_scored varies from World Cup to World Cup\nggplot(world_cup, aes(x = goals_scored)) + \n  geom_histogram(color = \"white\")\n\n\n\n\n\n\n\n\n\n# Let's follow up the plot with some more precise numerical summaries\n# Calculate the min, median, and max number of goals_scored across all World Cups\n# NOTE: Visually compare these numerical summaries to what you observed in the plot\nworld_cup |&gt; \n  summarize(min(goals_scored), median(goals_scored), max(goals_scored))\n\n  min(goals_scored) median(goals_scored) max(goals_scored)\n1                70                  126               171\n\n\n\n# Construct a bivariate plot of how the number of goals_scored in the World Cup has changed over the years\n# No wrangling necessary\nggplot(world_cup, aes(x = year, y = goals_scored)) + \n  geom_point() + \n  geom_line()\n\n\n\n\n\n\n\n\n\n# Our above summaries might be a bit misleading.\n# The number of games played at the World Cup varies.\n# Construct a bivariate plot of how the typical number of goals per game has changed over the years\nper_game_data &lt;- world_cup |&gt; \n  mutate(goals_per_game = goals_scored / games)\n\nggplot(per_game_data, aes(x = year, y = goals_per_game)) + \n  geom_point() + \n  geom_line()",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Wrangling Verbs</span>"
    ]
  },
  {
    "objectID": "activities/activity-09.html",
    "href": "activities/activity-09.html",
    "title": "7  Wrangling Practice",
    "section": "",
    "text": "7.1 Warm-up",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Wrangling Practice</span>"
    ]
  },
  {
    "objectID": "activities/activity-09.html#warm-up",
    "href": "activities/activity-09.html#warm-up",
    "title": "7  Wrangling Practice",
    "section": "",
    "text": "Data Science Process\nBelow is the visual representation of the data science process we saw earlier. Which stage are we in currently?\n\nRecall that wrangling is important. It is much of what we spend our efforts on in Data Science. There are lots of steps, hence R functions, that can go into data wrangling. But we can get far with the following 6 wrangling verbs:\n\n\n\nverb\naction\n\n\n\n\narrange\narrange the rows according to some column\n\n\nfilter\nfilter out or obtain a subset of the rows\n\n\nselect\nselect a subset of columns\n\n\nmutate\nmutate or create a column\n\n\nsummarize\ncalculate a numerical summary of a column\n\n\ngroup_by\ngroup the rows by a specified column\n\n\n\n\n\nExample 1: Single Verb\nLet’s start by working with some TidyTuesday data on penguins. This data includes information about penguins’ flippers (“arms”) and bills (“mouths” or “beaks”). Let’s import this using read_csv(), a function in the tidyverse package. For the most part, this is similar to read.csv(), though read_csv() can be more efficient at importing large datasets.\n\nlibrary(tidyverse)\npenguins &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv')\n\n# Check it out\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\n\n\n\n\n\n\nCheck Understanding\n\n\n\nConstruct a plot that allows us to examine how the relationship between body mass and bill length varies by species and sex.\n\n\n\n\n\n\n\n\nCheck Understanding\n\n\n\nUse the 6 wrangling verbs to address each task in the code chunk below. You can tack on |&gt; head() to print out just 6 rows to keep your rendered document manageable. Most of these require just 1 verb.\n\n\n\n# Get data on only Adelie penguins that weigh more than 4700g\n\n\n# Get data on penguin body mass only\n# Show just the first 6 rows\n\n\n# Sort the penguins from smallest to largest body mass\n# Show just the first 6 rows\n\n\n\n# Calculate the average body mass across all penguins\n# Note: na.rm = TRUE removes the NAs from the calculation\n\n\n\n# Calculate the average body mass by species\n\n\n\n# Create a new column that records body mass in kilograms, not grams\n# NOTE: there are 1000 g in 1 kg\n# Show just the first 6 rows\n\n\n\n\n\n\n\nCheck Understanding\n\n\n\nHow many penguins of each species do we have? Create a viz that addresses this question.\n\n\n\nggplot(penguins, aes(x = species))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Understanding\n\n\n\nCan we use the 6 verbs to calculate exactly how many penguins in each species?\nHINT: n() calculates group size.\n\n\n\n\n\n\n\n\ncount verb\n\n\n\nThe count() verb provides a handy shortcut!\n\npenguins |&gt; \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\n\n\n\n\nExample 2: Multiple Verbs\n\n\n\n\n\n\nCheck Understanding\n\n\n\nLet’s practice combining some verbs. For each task:\n\nTranslate the prompt into our 6 verbs. That is, think before you type.\nBuild your code line by line. It’s important to understand what’s being piped into each function!\nAsk what you can rearrange and still get the same result.\nRead your final code like a paragraph / a conversation. Would another person be able to follow your logic?\n\n\n\n\n# Sort Gentoo penguins from biggest to smallest with respect to their \n# bill length in cm (there are 10 mm in a cm)\n\n\n# Sort the species from smallest to biggest with respect to their \n# average bill length in cm\n\n\n\nExample 3: Interpret Code\nLet’s practice reading and making sense of somebody else’s code. What do you think this produces?\n\nHow many columns? Rows?\nWhat are the column names?\nWhat’s represented in each row?\n\nOnce you’ve thought about it, put the code inside a chunk and run it!\npenguins %&gt;% filter(species == “Chinstrap”) %&gt;% group_by(sex) %&gt;% summarize(min = min(body_mass_g), max = max(body_mass_g)) %&gt;% mutate(range = max - min)",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Wrangling Practice</span>"
    ]
  },
  {
    "objectID": "activities/activity-09.html#exercises-part-1-same-verbs-new-tricks",
    "href": "activities/activity-09.html#exercises-part-1-same-verbs-new-tricks",
    "title": "7  Wrangling Practice",
    "section": "7.2 Exercises Part 1: Same Verbs, New Tricks",
    "text": "7.2 Exercises Part 1: Same Verbs, New Tricks\n\nExercise 1: More Filtering\nRecall the “logical comparison operators” we can use to filter() our data:\n\n\n\nsymbol\nmeaning\n\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n%in% c(, )\na list of multiple values\n\n\n\n\nPart a\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\n\n# Create a dataset with just Adelie and Chinstrap using %in%\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt; \n   filter(species %in% c(\"Adelie\", \"Chinstrap\")) |&gt; \n   count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\n\n# Create a dataset with just Adelie and Chinstrap using !=\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt;\n  filter(species != \"Gentoo\") |&gt;\n  count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\n\n\nPart b\nNotice that some of our penguins have missing (NA) data on some values:\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\nThere are many ways to handle this. The right approach depends upon your research goals. A general rule is: Only get rid of observations with missing data if they’re missing data on variables you need for the specific task at hand!\nExample 1\nSuppose our research focus is just on body_mass_g. Two penguins are missing this info:\n\n# NOTE the use of is.na()\npenguins |&gt; \n  summarize(sum(is.na(body_mass_g)))\n\n# A tibble: 1 × 1\n  `sum(is.na(body_mass_g))`\n                      &lt;int&gt;\n1                         2\n\n\nLet’s define a new dataset that removes these penguins:\n\n# NOTE the use of is.na()\npenguins_w_body_mass &lt;- penguins |&gt; \n  filter(!is.na(body_mass_g))\n\n# Compare the number of penguins in this vs the original data\nnrow(penguins_w_body_mass)\n\n[1] 342\n\nnrow(penguins)\n\n[1] 344\n\n\nNote that some penguins in penguins_w_body_mass are missing info on sex, but we don’t care since that’s not related to our research question:\n\npenguins_w_body_mass |&gt; \n  summarize(sum(is.na(sex)))\n\n# A tibble: 1 × 1\n  `sum(is.na(sex))`\n              &lt;int&gt;\n1                 9\n\n\nExample 2\nIn the very rare case that we need complete information on every variable for the specific task at hand, we can use na.omit() to get rid of any penguin that’s missing info on any variable:\n\npenguins_complete &lt;- penguins |&gt; \n  na.omit()\n\nHow many penguins did this eliminate?\n\nnrow(penguins_complete)\n\n[1] 333\n\nnrow(penguins)\n\n[1] 344\n\n\n\n\nPart c\nExplain why we should only use na.omit() in extreme circumstances.\nit can remove a lot of observations\n\n\n\n\n\n\n\n\nExercise 2: More Selecting\nBeing able to select() only certain columns can help simplify our data. This is especially important when we’re working with lots of columns (which we haven’t done yet). It can also get tedious to type out every column of interest. Here are some shortcuts:\n\n- removes a given variable and keeps all others (e.g. select(-island))\nstarts_with(\"___\"), ends_with(\"___\"), or contains(\"___\") selects only the columns that either start with, end with, or simply contain the given string of characters\n\nUse these shortcuts to create the following datasets.\n\n# First: recall the variable names\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\n# Use a shortcut to keep everything but the year and island variables\npenguins %&gt;% \n  select(-year & -island)\n\n# A tibble: 344 × 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; \n 1 Adelie            39.1          18.7               181        3750 male  \n 2 Adelie            39.5          17.4               186        3800 female\n 3 Adelie            40.3          18                 195        3250 female\n 4 Adelie            NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie            36.7          19.3               193        3450 female\n 6 Adelie            39.3          20.6               190        3650 male  \n 7 Adelie            38.9          17.8               181        3625 female\n 8 Adelie            39.2          19.6               195        4675 male  \n 9 Adelie            34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie            42            20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and the penguin characteristics measured in mm\npenguins %&gt;% \n  select(species | ends_with(\"mm\"))\n\n# A tibble: 344 × 4\n   species bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1          18.7               181\n 2 Adelie            39.5          17.4               186\n 3 Adelie            40.3          18                 195\n 4 Adelie            NA            NA                  NA\n 5 Adelie            36.7          19.3               193\n 6 Adelie            39.3          20.6               190\n 7 Adelie            38.9          17.8               181\n 8 Adelie            39.2          19.6               195\n 9 Adelie            34.1          18.1               193\n10 Adelie            42            20.2               190\n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and bill-related measurements\npenguins %&gt;% \n  select(species | starts_with(\"bill\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm bill_depth_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie            39.1          18.7\n 2 Adelie            39.5          17.4\n 3 Adelie            40.3          18  \n 4 Adelie            NA            NA  \n 5 Adelie            36.7          19.3\n 6 Adelie            39.3          20.6\n 7 Adelie            38.9          17.8\n 8 Adelie            39.2          19.6\n 9 Adelie            34.1          18.1\n10 Adelie            42            20.2\n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and the length-related characteristics\npenguins %&gt;% \n  select(species | contains(\"length\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1               181\n 2 Adelie            39.5               186\n 3 Adelie            40.3               195\n 4 Adelie            NA                  NA\n 5 Adelie            36.7               193\n 6 Adelie            39.3               190\n 7 Adelie            38.9               181\n 8 Adelie            39.2               195\n 9 Adelie            34.1               193\n10 Adelie            42                 190\n# ℹ 334 more rows\n\n\n\n\n\n\n\n\n\nExercise 3: Arranging, Counting, & Grouping by Multiple Variables\nWe’ve done examples where we need to filter() by more than one variable, or select() more than one variable. Use your intuition for how we can arrange(), count(), and group_by() more than one variable.\n\n# Change this code to sort the penguins by species, and then island name\n# NOTE: The first row should be an Adelie penguin living on Biscoe island\npenguins |&gt; \n  arrange(species) %&gt;% \n  arrange(island)\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n 1 Adelie  Biscoe           37.8          18.3               174        3400\n 2 Adelie  Biscoe           37.7          18.7               180        3600\n 3 Adelie  Biscoe           35.9          19.2               189        3800\n 4 Adelie  Biscoe           38.2          18.1               185        3950\n 5 Adelie  Biscoe           38.8          17.2               180        3800\n 6 Adelie  Biscoe           35.3          18.9               187        3800\n 7 Adelie  Biscoe           40.6          18.6               183        3550\n 8 Adelie  Biscoe           40.5          17.9               187        3200\n 9 Adelie  Biscoe           37.9          18.6               172        3150\n10 Adelie  Biscoe           40.5          18.9               180        3950\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\n\n# Change this code to count the number of male/female penguins observed for each species\npenguins |&gt; \n  group_by(sex) %&gt;%\n  count(species)\n\n# A tibble: 8 × 3\n# Groups:   sex [3]\n  sex    species       n\n  &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;\n1 female Adelie       73\n2 female Chinstrap    34\n3 female Gentoo       58\n4 male   Adelie       73\n5 male   Chinstrap    34\n6 male   Gentoo       61\n7 &lt;NA&gt;   Adelie        6\n8 &lt;NA&gt;   Gentoo        5\n\n\n\n# Change this code to calculate the average body mass by species and sex\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(mean = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex     mean\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 Adelie    female 3369.\n2 Adelie    male   4043.\n3 Adelie    &lt;NA&gt;   3540 \n4 Chinstrap female 3527.\n5 Chinstrap male   3939.\n6 Gentoo    female 4680.\n7 Gentoo    male   5485.\n8 Gentoo    &lt;NA&gt;   4588.\n\n\n\n\n\n\n\n\n\nExercise 4: Dates\nBefore some wrangling practice, let’s explore another important concept: working with or mutating date variables. Dates are a whole special object type or class in R that automatically respect the order of time.\n\n# Get today's date\nas.Date(today())\n\n[1] \"2024-10-04\"\n\n# Let's store this as \"today\" so we can work with it below\ntoday &lt;- as.Date(today())\n\n# Check out the class of this object\nclass(today)\n\n[1] \"Date\"\n\n\nThe lubridate package inside tidyverse contains functions that can extract various information from dates. Let’s learn about some of the most common functions by applying them to today. For each, make a comment on what the function does\n\nyear(today)\n\n[1] 2024\n\n\n\n# What do these lines produce / what's their difference?\nmonth(today)\n\n[1] 10\n\nmonth(today, label = TRUE)\n\n[1] Oct\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n\n\n# What does this number mean?\nweek(today)\n\n[1] 40\n\n\n\n# What do these lines produce / what's their difference?\nmday(today)\n\n[1] 4\n\nyday(today)  # This is often called the \"Julian day\"\n\n[1] 278\n\n\n\n# What do these lines produce / what's their difference?\nwday(today)\n\n[1] 6\n\nwday(today, label = TRUE)\n\n[1] Fri\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\n\n# What do the results of these 2 lines tell us?\ntoday &gt;= ymd(\"2024-02-14\")\n\n[1] TRUE\n\ntoday &lt; ymd(\"2024-02-14\")\n\n[1] FALSE",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Wrangling Practice</span>"
    ]
  },
  {
    "objectID": "activities/activity-09.html#exercises-part-2-application",
    "href": "activities/activity-09.html#exercises-part-2-application",
    "title": "7  Wrangling Practice",
    "section": "7.3 Exercises Part 2: Application",
    "text": "7.3 Exercises Part 2: Application\nThe remaining exercises are similar to some of those on the homework. Hence, the solutions are not provided. Let’s apply these ideas to the daily Birthdays dataset in the mosaic package.\n\nlibrary(mosaic)\ndata(\"Birthdays\")\nhead(Birthdays)\n\n  state year month day       date wday births\n1    AK 1969     1   1 1969-01-01  Wed     14\n2    AL 1969     1   1 1969-01-01  Wed    174\n3    AR 1969     1   1 1969-01-01  Wed     78\n4    AZ 1969     1   1 1969-01-01  Wed     84\n5    CA 1969     1   1 1969-01-01  Wed    824\n6    CO 1969     1   1 1969-01-01  Wed    100\n\n\nBirthdays gives the number of births recorded on each day of the year in each state from 1969 to 19881. We can use our wrangling skills to understand some drivers of daily births. Putting these all together can be challenging! Remember the following ways to make tasks more manageable:\n\nTranslate the prompt into our 6 verbs (and count()). That is, think before you type.\nBuild your code line by line. It’s important to understand what’s being piped into each function!\n\n\nExercise 5: Warming up\n\n# How many days of data do we have for each state?\n\n\n# How many total births were there in this time period?\n\n\n# How many total births were there per state in this time period, sorted from low to high?\n\n\n\nExercise 6: Homework Reprise\nCreate a new dataset named daily_births that includes the total number of births per day (across all states) and the corresponding day of the week, eg, Mon. NOTE: Name the column with total births so that it’s easier to wrangle and plot.\nUsing this data, construct a plot of births over time, indicating the day of week.\n\n\nExercise 7: Wrangle & Plot\nFor each prompt below, you can decide whether you want to: (1) wrangle and store data, then plot; or (2) wrangle data and pipe directly into ggplot. For example:\n\npenguins |&gt; \n  filter(species != \"Gentoo\") |&gt; \n  ggplot(aes(y = bill_length_mm, x = bill_depth_mm, color = species)) + \n    geom_point()\n\n\n\n\n\n\n\n\n\nPart a\nCalculate the total number of births in each month and year, eg, Jan 1969, Feb 1969, …. Label month by names not numbers, eg, Jan not 1. Then, plot the births by month and comment on what you learn.\n\n\nPart b\nIn 1988, calculate the total number of births per week in each state. Get rid of week “53”, which isn’t a complete week! Then, make a line plot of births by week for each state and comment on what you learn. For example, do you notice any seasonal trends? Are these the same in every state? Any outliers?\n\n\nPart c\nRepeat the above for just Minnesota (MN) and Louisiana (LA). MN has one of the coldest climates and LA has one of the warmest. How do their seasonal trends compare? Do you think these trends are similar in other colder and warmer states? Try it!\n\n\n\nExercise 8: More Practice\n\nPart a\nCreate a dataset with only births in Massachusetts (MA) in 1979 and sort the days from those with the most births to those with the fewest.\n\n\nPart b\nMake a table showing the five states with the most births between September 9, 1979 and September 12, 1979, including the 9th and 12th. Arrange the table in descending order of births.",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Wrangling Practice</span>"
    ]
  },
  {
    "objectID": "activities/activity-09.html#solutions",
    "href": "activities/activity-09.html#solutions",
    "title": "7  Wrangling Practice",
    "section": "7.4 Solutions",
    "text": "7.4 Solutions\n\n\nClick for Solutions\n\n\nExample 1: Single Verb\n\nggplot(penguins, aes(y = body_mass_g, x = bill_length_mm, color = species)) + \n  geom_point() + \n  facet_wrap(~ sex)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Get data on only Adelie penguins that weigh more than 4700g\npenguins |&gt; \n  filter(species == \"Adelie\", body_mass_g &gt; 4700)\n\n# A tibble: 2 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Biscoe           41              20               203        4725\n2 Adelie  Biscoe           43.2            19               197        4775\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n# Get data on penguin body mass only\n# Show just the first 6 rows\npenguins |&gt; \n  select(body_mass_g) |&gt; \n  head()\n\n# A tibble: 6 × 1\n  body_mass_g\n        &lt;dbl&gt;\n1        3750\n2        3800\n3        3250\n4          NA\n5        3450\n6        3650\n\n# Sort the penguins from smallest to largest body mass\n# Show just the first 6 rows\npenguins |&gt; \n  arrange(body_mass_g) |&gt; \n  head()\n\n# A tibble: 6 × 8\n  species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Chinstrap Dream               46.9          16.6               192        2700\n2 Adelie    Biscoe              36.5          16.6               181        2850\n3 Adelie    Biscoe              36.4          17.1               184        2850\n4 Adelie    Biscoe              34.5          18.1               187        2900\n5 Adelie    Dream               33.1          16.1               178        2900\n6 Adelie    Torgersen           38.6          17                 188        2900\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n# Calculate the average body mass across all penguins\n# Note: na.rm = TRUE removes the NAs from the calculation\npenguins |&gt; \n  summarize(mean = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1 4202.\n\n# Calculate the average body mass by species\npenguins |&gt; \n  group_by(species) |&gt; \n  summarize(mean = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  species    mean\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Adelie    3701.\n2 Chinstrap 3733.\n3 Gentoo    5076.\n\n# Create a new column that records body mass in kilograms, not grams\n# NOTE: there are 1000 g in 1 kg\n# Show just the first 6 rows\npenguins |&gt; \n  mutate(body_mass_kg = body_mass_g/1000) |&gt; \n  head()\n\n# A tibble: 6 × 9\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;, body_mass_kg &lt;dbl&gt;\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = species)) + \n  geom_bar()\n\n\n\n\n\n\n\npenguins |&gt; \n  group_by(species) |&gt; \n  summarize(n())\n\n# A tibble: 3 × 2\n  species   `n()`\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\npenguins |&gt; \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\n\n\nExample 2: Multiple Verbs\n\n# Sort Gentoo penguins from biggest to smallest with respect to their \n# bill length in cm (there are 10 mm in a cm)\npenguins |&gt; \n  filter(species == \"Gentoo\") |&gt; \n  mutate(bill_length_cm = bill_length_mm / 10) |&gt; \n  arrange(desc(bill_length_cm))\n\n# A tibble: 124 × 9\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n 1 Gentoo  Biscoe           59.6          17                 230        6050\n 2 Gentoo  Biscoe           55.9          17                 228        5600\n 3 Gentoo  Biscoe           55.1          16                 230        5850\n 4 Gentoo  Biscoe           54.3          15.7               231        5650\n 5 Gentoo  Biscoe           53.4          15.8               219        5500\n 6 Gentoo  Biscoe           52.5          15.6               221        5450\n 7 Gentoo  Biscoe           52.2          17.1               228        5400\n 8 Gentoo  Biscoe           52.1          17                 230        5550\n 9 Gentoo  Biscoe           51.5          16.3               230        5500\n10 Gentoo  Biscoe           51.3          14.2               218        5300\n# ℹ 114 more rows\n# ℹ 3 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;, bill_length_cm &lt;dbl&gt;\n\n# Sort the species from smallest to biggest with respect to their \n# average bill length in cm\npenguins |&gt; \n  mutate(bill_length_cm = bill_length_mm / 10) |&gt; \n  group_by(species) |&gt; \n  summarize(mean_bill_length = mean(bill_length_cm, na.rm = TRUE)) |&gt; \n  arrange(desc(mean_bill_length))\n\n# A tibble: 3 × 2\n  species   mean_bill_length\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Chinstrap             4.88\n2 Gentoo                4.75\n3 Adelie                3.88\n\n\n\n\nExample 3: Interpret Code\n\n\nExercise 1: More Filtering\n\nPart a\n\n# Create a dataset with just Adelie and Chinstrap using %in%\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt;\n  filter(species %in% c(\"Adelie\", \"Chinstrap\")) |&gt;\n  count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\n\n# Create a dataset with just Adelie and Chinstrap using !=\n# Pipe this into `count(species)` to confirm that you only have these 2 species\npenguins |&gt;\n  filter(species != \"Gentoo\") |&gt;\n  count(species)\n\n# A tibble: 2 × 2\n  species       n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n\n\n\n\nPart b\n\n\nPart c\nIt might get rid of data points even if they have complete information on the variables we need, just because they’re missing info on variables we don’t need.\n\n\n\nExercise 2: More selecting\n\n# First: recall the variable names\nnames(penguins)\n\n[1] \"species\"           \"island\"            \"bill_length_mm\"   \n[4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"      \n[7] \"sex\"               \"year\"             \n\n\n\n# Use a shortcut to keep everything but the year and island variables\npenguins |&gt; \n  select(-year, -island)\n\n# A tibble: 344 × 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; \n 1 Adelie            39.1          18.7               181        3750 male  \n 2 Adelie            39.5          17.4               186        3800 female\n 3 Adelie            40.3          18                 195        3250 female\n 4 Adelie            NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie            36.7          19.3               193        3450 female\n 6 Adelie            39.3          20.6               190        3650 male  \n 7 Adelie            38.9          17.8               181        3625 female\n 8 Adelie            39.2          19.6               195        4675 male  \n 9 Adelie            34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie            42            20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and the penguin characteristics measured in mm\npenguins |&gt; \n  select(species, ends_with(\"mm\"))\n\n# A tibble: 344 × 4\n   species bill_length_mm bill_depth_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1          18.7               181\n 2 Adelie            39.5          17.4               186\n 3 Adelie            40.3          18                 195\n 4 Adelie            NA            NA                  NA\n 5 Adelie            36.7          19.3               193\n 6 Adelie            39.3          20.6               190\n 7 Adelie            38.9          17.8               181\n 8 Adelie            39.2          19.6               195\n 9 Adelie            34.1          18.1               193\n10 Adelie            42            20.2               190\n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and bill-related measurements\npenguins |&gt; \n  select(species, starts_with(\"bill\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm bill_depth_mm\n   &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n 1 Adelie            39.1          18.7\n 2 Adelie            39.5          17.4\n 3 Adelie            40.3          18  \n 4 Adelie            NA            NA  \n 5 Adelie            36.7          19.3\n 6 Adelie            39.3          20.6\n 7 Adelie            38.9          17.8\n 8 Adelie            39.2          19.6\n 9 Adelie            34.1          18.1\n10 Adelie            42            20.2\n# ℹ 334 more rows\n\n\n\n# Use a shortcut to keep only species and the length-related characteristics\npenguins |&gt; \n  select(species, contains(\"length\"))\n\n# A tibble: 344 × 3\n   species bill_length_mm flipper_length_mm\n   &lt;chr&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1 Adelie            39.1               181\n 2 Adelie            39.5               186\n 3 Adelie            40.3               195\n 4 Adelie            NA                  NA\n 5 Adelie            36.7               193\n 6 Adelie            39.3               190\n 7 Adelie            38.9               181\n 8 Adelie            39.2               195\n 9 Adelie            34.1               193\n10 Adelie            42                 190\n# ℹ 334 more rows\n\n\n\n\nExercise 3: Arranging, counting, & grouping by multiple variables\n\n# Change this code to sort the penguins by species, and then island name\n# NOTE: The first row should be an Adelie penguin living on Biscoe island\npenguins |&gt; \n  arrange(species, island) |&gt; \n  head()\n\n# A tibble: 6 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Biscoe           37.8          18.3               174        3400\n2 Adelie  Biscoe           37.7          18.7               180        3600\n3 Adelie  Biscoe           35.9          19.2               189        3800\n4 Adelie  Biscoe           38.2          18.1               185        3950\n5 Adelie  Biscoe           38.8          17.2               180        3800\n6 Adelie  Biscoe           35.3          18.9               187        3800\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n\n\n# Change this code to count the number of male/female penguins observed for each species\npenguins |&gt; \n  count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n\n\n# Change this code to calculate the average body mass by species and sex\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(mean = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex     mean\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n1 Adelie    female 3369.\n2 Adelie    male   4043.\n3 Adelie    &lt;NA&gt;   3540 \n4 Chinstrap female 3527.\n5 Chinstrap male   3939.\n6 Gentoo    female 4680.\n7 Gentoo    male   5485.\n8 Gentoo    &lt;NA&gt;   4588.\n\n\n\n\nExercise 4: Dates\n\n# Get today's date\nas.Date(today())\n\n[1] \"2024-10-04\"\n\n# Let's store this as \"today\" so we can work with it below\ntoday &lt;- as.Date(today())\n\n# Check out the class of this object\nclass(today)\n\n[1] \"Date\"\n\n\n\n# Records just the 4-digit year\nyear(today)\n\n[1] 2024\n\n\n\n# Today's month, as a number or label\nmonth(today)\n\n[1] 10\n\nmonth(today, label = TRUE)\n\n[1] Oct\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n\n\n# This is the week of the year (1-52)\nweek(today)\n\n[1] 40\n\n\n\n# Day of the month (1-31) and day of the year (1-366)\nmday(today)\n\n[1] 4\n\nyday(today)  # This is often called the \"Julian day\"\n\n[1] 278\n\n\n\n# Day of the week as a number or label\nwday(today)\n\n[1] 6\n\nwday(today, label = TRUE)\n\n[1] Fri\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\n\n# today is on or after Feb 14, 2024\ntoday &gt;= ymd(\"2024-02-14\")\n\n[1] TRUE\n\n# today is not before Feb 14, 2024\ntoday &lt; ymd(\"2024-02-14\")\n\n[1] FALSE",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Wrangling Practice</span>"
    ]
  },
  {
    "objectID": "activities/activity-09.html#footnotes",
    "href": "activities/activity-09.html#footnotes",
    "title": "7  Wrangling Practice",
    "section": "",
    "text": "The fivethirtyeight package has more recent data.↩︎",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Wrangling Practice</span>"
    ]
  },
  {
    "objectID": "activities/activity-10.html",
    "href": "activities/activity-10.html",
    "title": "8  Reshaping Data",
    "section": "",
    "text": "8.1 Warm-up\nEXAMPLE 1: warm-up counts and proportions\nRecall the penguins we worked with last class:\nlibrary(tidyverse)\npenguins &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-07-28/penguins.csv')\nTally up the number of male/female penguins by species in 2 ways:\n# Using count()\n\n\n# Using group_by() and summarize()\nDefine a new column that includes the proportion or relative frequencies of male/female penguins in each species.\nEXAMPLE 2: New data\nWhat will the following code do? Think about it before running.\npenguin_avg &lt;- penguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(avg_body_mass = mean(body_mass_g, na.rm = TRUE)) |&gt; \n  na.omit()\nEXAMPLE 3: units of observation\nTo get the information on average body masses, we reshaped our original data.\n# Units of observation = ???\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n# Units of observation = ???\nhead(penguin_avg)\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\nReshaping data\nThere are two general types of reshaped data:\nEXAMPLE 4: reshape it with your mind\nLet’s calculate the difference in average body mass, male vs female, for each species. Since penguin_avg is small, we could do these calculations by hand. But this doesn’t scale up to bigger datasets.\npenguin_avg\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\nWider vs Longer formats\nMaking our data longer or wider reshapes the data, changing the units of observation while retaining all raw information:\nEXAMPLE 5: pivot wider\nBecause it’s a small enough dataset to examine all at once, let’s start with our penguin_avg data:\npenguin_avg\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\nWith the goal of being able to calculate the difference in average body mass, male vs female, for each species, let’s make the dataset wider. That is, let’s get one row per species with separate columns for the average body mass by sex. Put this code into a chunk and run it:\npenguin_avg |&gt; pivot_wider(names_from = sex, values_from = avg_body_mass)\nFOLLOW-UP:\nEXAMPLE 6: Pivot longer\nLet’s store our wide data:\npenguin_avg_wide &lt;- penguin_avg |&gt; \n  pivot_wider(names_from = sex, values_from = avg_body_mass)\n\npenguin_avg_wide\n\n# A tibble: 3 × 3\n# Groups:   species [3]\n  species   female  male\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     3369. 4043.\n2 Chinstrap  3527. 3939.\n3 Gentoo     4680. 5485.\nSuppose we wanted to change this data back to a longer format. In general, this happens when some variables (here female and male) represent two categories or values of some broader variable (here sex), and we want to combine them into that 1 variable without losing any information. Let’s pivot_longer():\n# We can either communicate which variables we WANT to collect into a single column (female, male)\npenguin_avg_wide |&gt; \n  pivot_longer(cols = c(female, male), names_to = \"sex\", values_to = \"avg_body_mass\")\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n# Or which variable(s) we do NOT want to collect into a single column (sex)\npenguin_avg_wide |&gt; \n  pivot_longer(cols = -species, names_to = \"sex\", values_to = \"avg_body_mass\")\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\nFOLLOW-UP:\nEXAMPLE 7: Practice\nLet’s make up some data on the orders of 2 different customers at 3 different restaurants:\nfood &lt;- data.frame(\n  customer = rep(c(\"A\", \"B\"), each = 3),\n  restaurant = rep(c(\"Shish\", \"FrenchMeadow\", \"DunnBros\"), 2),\n  order = c(\"falafel\", \"salad\", \"coffee\", \"baklava\", \"pastry\", \"tea\")\n)\nfood\n\n  customer   restaurant   order\n1        A        Shish falafel\n2        A FrenchMeadow   salad\n3        A     DunnBros  coffee\n4        B        Shish baklava\n5        B FrenchMeadow  pastry\n6        B     DunnBros     tea\nThe units of observation in food are customer / restaurant combinations. Wrangle this data so that the units of observation are customers, spreading the restaurants into separate columns.\nConsider 2 more customers:\nmore_food &lt;- data.frame(\n  customer = c(\"C\", \"D\"),\n  Shish = c(\"coffee\", \"maza\"),\n  FrenchMeadow = c(\"soup\", \"sandwich\"),\n  DunnBros = c(\"cookie\", \"coffee\")\n)\nmore_food\n\n  customer  Shish FrenchMeadow DunnBros\n1        C coffee         soup   cookie\n2        D   maza     sandwich   coffee\nWrangle this data so that the 3 restaurant columns are combined into 1, hence the units of observation are customer / restaurant combinations.",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "activities/activity-10.html#warm-up",
    "href": "activities/activity-10.html#warm-up",
    "title": "8  Reshaping Data",
    "section": "",
    "text": "We can’t do this by adjusting our count() code, but can adjust the group_by() and summarize() code since it’s still tracking the group categories in the background.\nDoes the order of species and sex in group_by() matter?\n\n\n\n\n\n\n\n\n\nDid the reshaping process change the units of observation?\n\n\n\nDid the reshaping process result in any information loss from the original data?\n\n\n\n\n\naggregate data\nFor example, using group_by() with summarize() gains aggregate information about our observations but loses data on individual observations.\nraw data, reshaped\nWe often want to retain all information on individual observations, but need to reshape it in order to perform the task at hand.\n\n\n\n\n\nSketch out (on paper, in your head, anything) how this data would need to be reshaped, without losing any information, in order to calculate the differences in average body mass using our wrangling verbs. Make it as specific as possible, with column labels, entries, correct numbers, etc.\nIdentify the units of observation.\n\n\n\n\n\n\nMake the data longer, i.e. combine values from multiple variables into 1 variable. EXAMPLE: 1999 and 2000 represent two years. We want to combine their results into 1 variable without losing any information.\n\n\n\nMake the data wider, i.e. spread out the values across new variables. EXAMPLE: cases and pop represent two categories within type. To compare or combine their count outcomes side-by-side, we can separate them into their own variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPivot Wider\n\n\n\n\nnames_from = the variable whose values we want to separate into their own columns, i.e. where we want to get the new column names from\nvalues_from = which variable to take the new column values from\n\n\n\n\n\nWhat are the units of observation?\nDid we lose any information when we widened the data?\nUse the wide data to calculate the difference in average body mass, male vs female, for each species.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPivot Longer\n\n\n\n\ncols = the columns (variables) to collect into a single, new variable. we can also specify what variables we don’t want to collect\nnames_to = the name of the new variable which will include the names or labels of the collected variables\nvalues_to = the name of the new variable which will include the values of the collected variables\n\n\n\n\n\nWhat are the units of observation?\nDid we lose any information when we lengthened the data?\nWhy did we put the variables in quotes “” here but not when we used pivot_wider()?",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "activities/activity-10.html#exercises",
    "href": "activities/activity-10.html#exercises",
    "title": "8  Reshaping Data",
    "section": "8.2 Exercises",
    "text": "8.2 Exercises\n\nExercise 1: What’s the problem?\nConsider data on a sleep study in which subjects received only 3 hours of sleep per night. Each day, their reaction time to a stimulus (in ms) was recorded.1\n\nsleep_wide &lt;- read.csv(\"https://mac-stat.github.io/data/sleep_wide.csv\")\n\nhead(sleep_wide)\n\n  Subject  day_0  day_1  day_2  day_3  day_4  day_5  day_6  day_7  day_8  day_9\n1     308 249.56 258.70 250.80 321.44 356.85 414.69 382.20 290.15 430.59 466.35\n2     309 222.73 205.27 202.98 204.71 207.72 215.96 213.63 217.73 224.30 237.31\n3     310 199.05 194.33 234.32 232.84 229.31 220.46 235.42 255.75 261.01 247.52\n4     330 321.54 300.40 283.86 285.13 285.80 297.59 280.24 318.26 305.35 354.05\n5     331 287.61 285.00 301.82 320.12 316.28 293.32 290.08 334.82 293.75 371.58\n6     332 234.86 242.81 272.96 309.77 317.46 310.00 454.16 346.83 330.30 253.86\n\n\n\nPart a\nWhat are the units of observation in sleep_wide? each subject\n\n\nPart b\nSuppose I ask you to plot each subject’s reaction time (y-axis) vs the number of days of sleep restriction (x-axis). “Sketch” out in words what the first few rows of the data need to look like in order to do this. It might help to think about what you’d need to complete the plotting frame:\nggplot(___, aes(y = ___, x = ___, color = ___))\n\n\nPart c\nHow can you obtain the dataset you sketched in part b?\n\njust using sleep_wide\npivot_longer()\npivot_wider()\n\n\n\n\n\n\n\n\n\nExercise 2: Pivot longer\nTo plot reaction time by day for each subject, we need to reshape the data into a long format where each row represents a subject/day combination. Specifically, we want a dataset with 3 columns and a first few rows that look something like this:\n\n\n\nSubject\nday\nreaction_time\n\n\n\n\n308\n0\n249.56\n\n\n308\n1\n258.70\n\n\n308\n2\n250.80\n\n\n\n\nPart a\nUse pivot_longer() to create the long-format dataset above. Show the first 3 lines (head(3)), which should be similar to those above. Follow-up: Thinking forward to plotting reaction time vs day for each subject, what would you like to fix / change about this dataset?\n\n# For cols, try 2 appproaches: using - and starts_with\nsleep_wide |&gt;\n  pivot_longer(cols = -Subject, names_to = \"day\", values_to = \"reaction_time\") |&gt;\n  head(3)\n\n# A tibble: 3 × 3\n  Subject day   reaction_time\n    &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n1     308 day_0          250.\n2     308 day_1          259.\n3     308 day_2          251.\n\n\n\n\nPart b\nRun this chunk:\n\nsleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\")\n\nhead(sleep_long)\n\n# A tibble: 6 × 3\n  Subject day   reaction_time\n    &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n1     308 0              250.\n2     308 1              259.\n3     308 2              251.\n4     308 3              321.\n5     308 4              357.\n6     308 5              415.\n\n\nFollow-up:\n\nBesides putting each argument on a different line for readability and storing the results, what changed in the code?\nHow did this impact how the values are recorded in the day column?\n\n\n\nPart c\nUsing sleep_long, construct a line plot of reaction time vs day for each subject. This will look goofy no matter what you do. Why? HINT: look back at head(sleep_long). What class or type of variables are Subject and day? What do we want them to be?\n\nggplot(sleep_long, aes(x= day, y = reaction_time)) +\n  geom_smooth() +\n  facet_wrap(~Subject)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3: Changing variable classes & plotting\nLet’s finalize sleep_long by mutating the Subject variable to be a factor (categorical) and the day variable to be numeric (quantitative). Take note of the mutate() code! You’ll use this type of code a lot.\n\nsleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\") |&gt; \n  mutate(Subject = as.factor(Subject), day = as.numeric(day))\n\n# Check it out\n# Same data, different class\nhead(sleep_long)\n\n# A tibble: 6 × 3\n  Subject   day reaction_time\n  &lt;fct&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1 308         0          250.\n2 308         1          259.\n3 308         2          251.\n4 308         3          321.\n5 308         4          357.\n6 308         5          415.\n\n\n\nPart a\nNow make some plots.\n\n# Make a line plot of reaction time by day for each subject\n# Put these all on the same frame\nggplot(sleep_long, aes(x= day, y = reaction_time, color = Subject)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n# Make a line plot of reaction time by day for each subject\n# Put these all on separate frames (one per subject)\nggplot(sleep_long, aes(x= day, y = reaction_time)) +\n  geom_line() +\n  facet_wrap(~Subject)\n\n\n\n\n\n\n\n\n\n\nPart b\nSummarize what you learned from the plots. For example:\n\nWhat’s the general relationship between reaction time and sleep?\nIs this the same for everybody? What differs?\n\n\n\n\n\n\n\n\n\nExercise 4: Pivot wider\nMake the data wide again, with each day becoming its own column.\n\nPart a\nAdjust the code below. What don’t you like about the column labels?\n\nsleep_long |&gt;\n  pivot_wider(names_from = day, values_from = reaction_time) |&gt;\n  head()\n\n# A tibble: 6 × 11\n  Subject   `0`   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\n\n\nPart b\nUsing your intuition, adjust your code from part a to name the reaction time columns “day_0”, “day_1”, etc.\n\nsleep_long |&gt;\n  pivot_wider(names_from = day, values_from = reaction_time, names_prefix = \"day_\") |&gt;\n  head()\n\n# A tibble: 6 × 11\n  Subject day_0 day_1 day_2 day_3 day_4 day_5 day_6 day_7 day_8 day_9\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\n\n\n\n\n\n\n\n\nExercise 5: Practice with Billboard charts\nLoad data on songs that hit the billboard charts around the year 2000. Included for each song is the artist name, track name, the date it hit the charts (date.enter), and wk-related variables that indicate rankings in each subsequent week on the charts:\n\n# Load data\nlibrary(tidyr)\ndata(\"billboard\")\n\n# Check it out\nhead(billboard)\n\n# A tibble: 6 × 79\n  artist      track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n  &lt;chr&gt;       &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2 Pac       Baby… 2000-02-26      87    82    72    77    87    94    99    NA\n2 2Ge+her     The … 2000-09-02      91    87    92    NA    NA    NA    NA    NA\n3 3 Doors Do… Kryp… 2000-04-08      81    70    68    67    66    57    54    53\n4 3 Doors Do… Loser 2000-10-21      76    76    72    69    67    65    55    59\n5 504 Boyz    Wobb… 2000-04-15      57    34    25    17    17    31    36    49\n6 98^0        Give… 2000-08-19      51    39    34    26    26    19     2     2\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;,\n#   wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, …\n\n\nIn using this data, you’ll need to determine if and when the data needs to be reshaped for the task at hand.\n\nPart a\nConstruct and summarize a plot of how a song’s Billboard ranking its 2nd week on the chart (y-axis) is related to its ranking the 1st week on the charts (x-axis). Add a reference line geom_abline(intercept = 0, slope = 1). Songs above this line improved their rankings from the 1st to 2nd week.\n\nggplot(billboard, aes(x = wk1, y = wk2)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1)\n\n\n\n\n\n\n\n\n\n\nPart b\nUse your wrangling tools to identify which songs are those above the line in Part a, i.e. with rankings that went up from week 1 to week 2.\n\nbillboard %&gt;% \n  filter(wk2 &gt; wk1) %&gt;% \n  select(track, wk1, wk2)\n\n# A tibble: 7 × 3\n  track                 wk1   wk2\n  &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n1 Crybaby                28    34\n2 A Little Gasoline      75    82\n3 The Quittin' Kind      98   100\n4 L.A. Song              99   100\n5 Hey Papi               98   100\n6 Callin' Me             83    89\n7 Nothing As It Seems    49    70\n\n\n\n\nPart c\nDefine a new dataset, nov_1999, which:\n\nonly includes data on songs that entered the Billboard charts on November 6, 1999\nkeeps all variables except track and date.entered. HINT: How can you avoid writing out all the variable names you want to keep?\n\n\n# Define nov_1999\nnov_1999 &lt;- billboard %&gt;% \n  filter(date.entered == \"1999-11-06\") %&gt;% \n  select(-track, -date.entered)\n\n# Confirm that nov_1999 has 2 rows (songs) and 77 columns\ndim(nov_1999)\n\n[1]  2 77\n\n\n\n\nPart d\nCreate and discuss a visualization of the rankings (y-axis) over time (x-axis) for the 2 songs in nov_1999. There are hints below (if you scroll), but you’re encouraged to play around and use as few hints as possible.\nHints:\n\nShould you first pivot wider or longer?\nOnce you pivot, the week number is turned into a character variable. How can you change it to a number?\n\n\n\n\n\n\n\n\n\nExercise 6: Practice with the Daily Show\nThe data associated with this article is available in the fivethirtyeight package, and is loaded into daily below. It includes a list of every guest to ever appear on Jon Stewart’s The Daily Show, a “late-night talk and satirical news” program (per Wikipedia). Check out the dataset and note that when multiple people appeared together, each person receives their own line:\n\nlibrary(fivethirtyeight)\ndata(\"daily_show_guests\")\ndaily &lt;- daily_show_guests\n\nIn analyzing this data, you’ll need to determine if and when the data needs to be reshaped.\n\nPart a\nIdentify the 15 guests that appeared the most. (This isn’t a very diverse guest list!)\n\n\nPart b\nCHALLENGE: Create the following data set containing 19 columns:\n\nThe first column should have the 15 guests with the highest number of total appearances on the show, listed in descending order of number of appearances.\n17 columns should show the number of appearances of the corresponding guest in each year from 1999 to 2015 (one per column).\nAnother column should show the total number of appearances for the corresponding guest over the entire duration of the show.\n\nThere are hints below (if you scroll), but you’re encouraged to play around and use as few hints as possible.\nHINTS: There are lots of ways to do this. You don’t necessarily need all of these hints.\n\nFirst obtain the number of times a guest appears each year.\nTo this, add a new column which includes the total number of times a guest appears across all years.\nPivot (longer or wider?). When you do, use values_fill = 0 to replace NA values with 0.\nArrange, then and keep the top 15.\n\n\n\nPart c\nLet’s recreate the first figure from the article. This groups all guests into 3 broader occupational categories. However, our current data has 18 categories:\n\ndaily |&gt; \n  count(group)\n\n# A tibble: 18 × 2\n   group              n\n   &lt;chr&gt;          &lt;int&gt;\n 1 Academic         103\n 2 Acting           930\n 3 Advocacy          24\n 4 Athletics         52\n 5 Business          25\n 6 Clergy             8\n 7 Comedy           150\n 8 Consultant        18\n 9 Government        40\n10 Media            751\n11 Military          16\n12 Misc              45\n13 Musician         123\n14 Political Aide    36\n15 Politician       308\n16 Science           28\n17 media              5\n18 &lt;NA&gt;              31\n\n\nLet’s define a new dataset that includes a new variable, broad_group, that buckets these 18 categories into the 3 bigger ones used in the article. And get rid of any rows missing information on broad_group. You’ll learn the code soon! For now, just run this chunk:\n\nplot_data &lt;- daily |&gt; \n  mutate(broad_group = case_when(\n    group %in% c(\"Acting\", \"Athletics\", \"Comedy\", \"Musician\") ~ \"Acting, Comedy & Music\",\n    group %in% c(\"Media\", \"media\", \"Science\", \"Academic\", \"Consultant\", \"Clergy\") ~ \"Media\",\n    group %in% c(\"Politician\", \"Political Aide\", \"Government\", \"Military\", \"Business\", \"Advocacy\") ~ \"Government and Politics\",\n    .default = NA\n  )) |&gt; \n  filter(!is.na(broad_group))\n\nNow, using the broad_group variable in plot_data, recreate the graphic from the article, with three different lines showing the fraction of guests in each group over time. Note: You’ll have to wrangle the data first.",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "activities/activity-10.html#solutions",
    "href": "activities/activity-10.html#solutions",
    "title": "8  Reshaping Data",
    "section": "8.3 Solutions",
    "text": "8.3 Solutions\n\n\nClick for Solutions\n\n\nEXAMPLE 1: warm-up counts and proportions\n\n# Using count()\npenguins |&gt; \n  count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n# Using group_by() and summarize()\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(n())\n\n# A tibble: 8 × 3\n# Groups:   species [3]\n  species   sex    `n()`\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    &lt;NA&gt;       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    &lt;NA&gt;       5\n\n# Relative frequencies\npenguins |&gt; \n  group_by(species, sex) |&gt; \n  summarize(n = n()) |&gt; \n  mutate(proportion = n / sum(n))\n\n# A tibble: 8 × 4\n# Groups:   species [3]\n  species   sex        n proportion\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 Adelie    female    73     0.480 \n2 Adelie    male      73     0.480 \n3 Adelie    &lt;NA&gt;       6     0.0395\n4 Chinstrap female    34     0.5   \n5 Chinstrap male      34     0.5   \n6 Gentoo    female    58     0.468 \n7 Gentoo    male      61     0.492 \n8 Gentoo    &lt;NA&gt;       5     0.0403\n\n# Changing the order calculates the proportion of species within each sex\npenguins |&gt; \n  group_by(sex, species) |&gt; \n  summarize(n = n()) |&gt; \n  mutate(proportion = n / sum(n))\n\n# A tibble: 8 × 4\n# Groups:   sex [3]\n  sex    species       n proportion\n  &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;\n1 female Adelie       73      0.442\n2 female Chinstrap    34      0.206\n3 female Gentoo       58      0.352\n4 male   Adelie       73      0.435\n5 male   Chinstrap    34      0.202\n6 male   Gentoo       61      0.363\n7 &lt;NA&gt;   Adelie        6      0.545\n8 &lt;NA&gt;   Gentoo        5      0.455\n\n\n\n\n\n\n\n\n\nEXAMPLE 3: units of observation\n\n# Units of observation = penguins\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;\n\n# Units of observation = species/sex combos\nhead(penguin_avg)\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n\n\n\n\n\n\n\n\nEXAMPLE 5: pivot wider\n\npenguin_avg |&gt; \n  pivot_wider(names_from = sex, values_from = avg_body_mass)\n\n# A tibble: 3 × 3\n# Groups:   species [3]\n  species   female  male\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     3369. 4043.\n2 Chinstrap  3527. 3939.\n3 Gentoo     4680. 5485.\n\n\nFOLLOW-UP:\n\nWhat are the units of observation? species\nDid we lose any information when we widened the data? no\nUse the wide data to calculate the difference in average body mass, male vs female, for each species.\n\n\npenguin_avg |&gt; \n  pivot_wider(names_from = sex, values_from = avg_body_mass) |&gt; \n  mutate(diff = male - female)\n\n# A tibble: 3 × 4\n# Groups:   species [3]\n  species   female  male  diff\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     3369. 4043.  675.\n2 Chinstrap  3527. 3939.  412.\n3 Gentoo     4680. 5485.  805.\n\n\n\n\n\n\n\n\n\nEXAMPLE 6: Pivot longer\n\n# We can either communicate which variables we WANT to collect into a single column (female, male)\npenguin_avg_wide |&gt; \n  pivot_longer(cols = c(female, male), names_to = \"sex\", values_to = \"avg_body_mass\")\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n# Or which variable(s) we do NOT want to collect into a single column (sex)\npenguin_avg_wide |&gt; \n  pivot_longer(cols = -species, names_to = \"sex\", values_to = \"avg_body_mass\")\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    avg_body_mass\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 Adelie    female         3369.\n2 Adelie    male           4043.\n3 Chinstrap female         3527.\n4 Chinstrap male           3939.\n5 Gentoo    female         4680.\n6 Gentoo    male           5485.\n\n\nFOLLOW-UP:\n\nWhat are the units of observation? species/sex combos\nDid we lose any information when we lengthened the data? no\n\n\n\n\n\n\nEXAMPLE 7: Practice\n\nfood &lt;- data.frame(\n  customer = rep(c(\"A\", \"B\"), each = 3),\n  restaurant = rep(c(\"Shish\", \"FrenchMeadow\", \"DunnBros\"), 2),\n  order = c(\"falafel\", \"salad\", \"coffee\", \"baklava\", \"pastry\", \"tea\")\n)\n\nfood\n\n  customer   restaurant   order\n1        A        Shish falafel\n2        A FrenchMeadow   salad\n3        A     DunnBros  coffee\n4        B        Shish baklava\n5        B FrenchMeadow  pastry\n6        B     DunnBros     tea\n\nfood |&gt; \n  pivot_wider(names_from = restaurant, values_from = order)\n\n# A tibble: 2 × 4\n  customer Shish   FrenchMeadow DunnBros\n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;   \n1 A        falafel salad        coffee  \n2 B        baklava pastry       tea     \n\n\n\nmore_food &lt;- data.frame(\n  customer = c(\"C\", \"D\"),\n  Shish = c(\"coffee\", \"maza\"),\n  FrenchMeadow = c(\"soup\", \"sandwich\"),\n  DunnBros = c(\"cookie\", \"coffee\")\n)\n\nmore_food\n\n  customer  Shish FrenchMeadow DunnBros\n1        C coffee         soup   cookie\n2        D   maza     sandwich   coffee\n\nmore_food |&gt; \n  pivot_longer(cols = -customer, names_to = \"restaurant\", values_to = \"order\")\n\n# A tibble: 6 × 3\n  customer restaurant   order   \n  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;   \n1 C        Shish        coffee  \n2 C        FrenchMeadow soup    \n3 C        DunnBros     cookie  \n4 D        Shish        maza    \n5 D        FrenchMeadow sandwich\n6 D        DunnBros     coffee  \n\n\n\n\n\n\n\n\n\nExercise 1: What’s the problem?\n\nPart a\nsubjects/people\n\n\nPart c\npivot_longer()\n\n\n\n\n\n\n\n\nExercise 2: Pivot longer\n\nPart a\n\n# For cols, try 2 appproaches: using - and starts_with\nsleep_wide |&gt;\n  pivot_longer(cols = -Subject, names_to = \"day\", values_to = \"reaction_time\")\n\n# A tibble: 180 × 3\n   Subject day   reaction_time\n     &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1     308 day_0          250.\n 2     308 day_1          259.\n 3     308 day_2          251.\n 4     308 day_3          321.\n 5     308 day_4          357.\n 6     308 day_5          415.\n 7     308 day_6          382.\n 8     308 day_7          290.\n 9     308 day_8          431.\n10     308 day_9          466.\n# ℹ 170 more rows\n\nsleep_wide |&gt;\n  pivot_longer(cols = starts_with(\"day\"), names_to = \"day\", values_to = \"reaction_time\")\n\n# A tibble: 180 × 3\n   Subject day   reaction_time\n     &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1     308 day_0          250.\n 2     308 day_1          259.\n 3     308 day_2          251.\n 4     308 day_3          321.\n 5     308 day_4          357.\n 6     308 day_5          415.\n 7     308 day_6          382.\n 8     308 day_7          290.\n 9     308 day_8          431.\n10     308 day_9          466.\n# ℹ 170 more rows\n\n\n\n\nPart b\nAdding names_prefix = \"day_\" removed “day_” from the start of the day entries. did this impact how the values are recorded in the day column?\n\nsleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\") \n\n\n\nPart c\nSubject is an integer and day is a character. We want them to be categorical (factor) and numeric, respectively.\n\nggplot(sleep_long, aes(y = reaction_time, x = day, color = Subject)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3: Changing variable classes & plotting\n\nsleep_long &lt;- sleep_wide |&gt;\n  pivot_longer(cols = -Subject,\n               names_to = \"day\",\n               names_prefix = \"day_\",\n               values_to = \"reaction_time\") |&gt; \n  mutate(Subject = as.factor(Subject), day = as.numeric(day))\n\n\nPart a\nNow make some plots.\n\n# Make a line plot of reaction time by day for each subject\n# Put these all on the same frame\nggplot(sleep_long, aes(y = reaction_time, x = day, color = Subject)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\n# Make a line plot of reaction time by day for each subject\n# Put these all on separate frames (one per subject)\nggplot(sleep_long, aes(y = reaction_time, x = day, color = Subject)) + \n  geom_line() + \n  facet_wrap(~ Subject)\n\n\n\n\n\n\n\n\n\n\nPart b\nReaction time increases (worsens) with a lack of sleep. Some subjects seem to be more impacted than others by lack of sleep, and some tend to have faster/slower reaction times in general.\n\n\n\n\n\n\n\n\nExercise 4: Pivot wider\n\nPart a\n\nsleep_long |&gt;\n  pivot_wider(names_from = day, values_from = reaction_time) |&gt;\n  head()\n\n# A tibble: 6 × 11\n  Subject   `0`   `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\n\n\nPart b\n\nsleep_long |&gt;\n  pivot_wider(names_from = day, values_from = reaction_time, names_prefix = \"day_\") |&gt;\n  head()\n\n# A tibble: 6 × 11\n  Subject day_0 day_1 day_2 day_3 day_4 day_5 day_6 day_7 day_8 day_9\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 308      250.  259.  251.  321.  357.  415.  382.  290.  431.  466.\n2 309      223.  205.  203.  205.  208.  216.  214.  218.  224.  237.\n3 310      199.  194.  234.  233.  229.  220.  235.  256.  261.  248.\n4 330      322.  300.  284.  285.  286.  298.  280.  318.  305.  354.\n5 331      288.  285   302.  320.  316.  293.  290.  335.  294.  372.\n6 332      235.  243.  273.  310.  317.  310   454.  347.  330.  254.\n\n\n\n\n\n\n\n\n\n\nExercise 5: Practice with Billboard charts\n\nPart a\nThe higher a song’s week 1 rating, the higher its week 2 rating tends to be. But almost all song’s rankings drop from week 1 to week 2.\n\nggplot(billboard, aes(y = wk2, x = wk1)) + \n  geom_point() +\n  geom_abline(intercept = 0, slope = 1)\n\n\n\n\n\n\n\n\n\n\nPart b\n\nbillboard |&gt; \n  filter(wk2 &gt; wk1)\n\n# A tibble: 7 × 79\n  artist      track date.entered   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8\n  &lt;chr&gt;       &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Carey, Mar… Cryb… 2000-06-24      28    34    48    62    77    90    95    NA\n2 Clark, Ter… A Li… 2000-12-16      75    82    88    96    99    99    NA    NA\n3 Diffie, Joe The … 2000-01-01      98   100   100    90    93    94    NA    NA\n4 Hart, Beth  L.A.… 1999-11-27      99   100    98    99    99    99    98    90\n5 Jay-Z       Hey … 2000-08-12      98   100    98    94    83    83    80    78\n6 Lil' Zane   Call… 2000-07-29      83    89    57    40    34    21    33    46\n7 Pearl Jam   Noth… 2000-05-13      49    70    84    89    93    91    NA    NA\n# ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;,\n#   wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;,\n#   wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;,\n#   wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;,\n#   wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;,\n#   wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;,\n#   wk43 &lt;dbl&gt;, wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, …\n\n\n\n\nPart c\n\n# Define nov_1999\nnov_1999 &lt;- billboard |&gt; \n  filter(date.entered == \"1999-11-06\") |&gt; \n  select(-track, -date.entered)\n\n# Or\nnov_1999 &lt;- billboard |&gt; \n  filter(date.entered == \"1999-11-06\") |&gt; \n  select(artist, starts_with(\"wk\"))\n\n\n# Confirm that nov_1999 has 2 rows (songs) and 77 columns\ndim(nov_1999)\n\n[1]  2 77\n\n\n\n\nPart c\n\nnov_1999 |&gt; \n  pivot_longer(cols = -artist, names_to = \"week\", names_prefix = \"wk\", values_to = \"ranking\") |&gt; \n  mutate(week = as.numeric(week)) |&gt; \n  ggplot(aes(y = ranking, x = week, color = artist)) + \n    geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6: Practice with the Daily Show\n\nPart a\n\ndaily |&gt; \n  count(raw_guest_list) |&gt; \n  arrange(desc(n)) |&gt; \n  head(15)\n\n# A tibble: 15 × 2\n   raw_guest_list        n\n   &lt;chr&gt;             &lt;int&gt;\n 1 Fareed Zakaria       19\n 2 Denis Leary          17\n 3 Brian Williams       16\n 4 Paul Rudd            13\n 5 Ricky Gervais        13\n 6 Tom Brokaw           12\n 7 Bill O'Reilly        10\n 8 Reza Aslan           10\n 9 Richard Lewis        10\n10 Will Ferrell         10\n11 Sarah Vowell          9\n12 Adam Sandler          8\n13 Ben Affleck           8\n14 Louis C.K.            8\n15 Maggie Gyllenhaal     8\n\n\n\n\nPart b\n\ndaily |&gt; \n  count(year, raw_guest_list) |&gt; \n  group_by(raw_guest_list) |&gt; \n  mutate(total = sum(n)) |&gt;\n  pivot_wider(names_from = year, \n              values_from = n,\n              values_fill = 0) |&gt; \n  arrange(desc(total)) |&gt; \n  head(15)\n\n# A tibble: 15 × 19\n# Groups:   raw_guest_list [15]\n   raw_guest_list  total `1999` `2000` `2001` `2002` `2003` `2004` `2005` `2006`\n   &lt;chr&gt;           &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 Fareed Zakaria     19      0      0      1      0      1      2      2      2\n 2 Denis Leary        17      1      0      1      2      1      0      0      1\n 3 Brian Williams     16      0      0      0      0      1      1      2      1\n 4 Paul Rudd          13      1      0      1      1      1      1      1      0\n 5 Ricky Gervais      13      0      0      0      0      0      0      1      2\n 6 Tom Brokaw         12      0      0      0      1      0      2      1      0\n 7 Richard Lewis      10      1      0      2      2      1      1      0      0\n 8 Will Ferrell       10      0      1      1      0      1      1      1      1\n 9 Bill O'Reilly      10      0      0      1      1      0      1      1      0\n10 Reza Aslan         10      0      0      0      0      0      0      1      2\n11 Sarah Vowell        9      0      0      0      1      0      1      1      1\n12 Adam Sandler        8      1      2      0      1      0      0      0      1\n13 Ben Affleck         8      0      0      0      0      2      0      0      1\n14 Maggie Gyllenh…     8      0      0      0      0      1      0      1      1\n15 Louis C.K.          8      0      0      0      0      0      0      0      1\n# ℹ 9 more variables: `2007` &lt;int&gt;, `2008` &lt;int&gt;, `2009` &lt;int&gt;, `2010` &lt;int&gt;,\n#   `2011` &lt;int&gt;, `2012` &lt;int&gt;, `2013` &lt;int&gt;, `2014` &lt;int&gt;, `2015` &lt;int&gt;\n\n\n\n\nPart c\n\nplot_data |&gt;\n  group_by(year, broad_group) |&gt;\n  summarise(n = n()) |&gt;\n  mutate(freq = n / sum(n)) |&gt; \n  ggplot(aes(y = freq, x = year, color = broad_group)) + \n    geom_line()",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "activities/activity-10.html#footnotes",
    "href": "activities/activity-10.html#footnotes",
    "title": "8  Reshaping Data",
    "section": "",
    "text": "Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003) Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research 12, 1–12.↩︎",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Reshaping Data</span>"
    ]
  },
  {
    "objectID": "activities/activity-11.html",
    "href": "activities/activity-11.html",
    "title": "9  Joining Data",
    "section": "",
    "text": "9.1 Warm-up\nWhere are we? Data preparation\nThus far, we’ve learned how to:\nMotivation\nIn practice, we often have to collect and combine data from various sources in order to address our research questions. Example:\nEXAMPLE 1\nConsider the following (made up) data on students and course enrollments:\nstudents_1 &lt;- data.frame(\n  student = c(\"A\", \"B\", \"C\"),\n  class = c(\"STAT 101\", \"GEOL 101\", \"ANTH 101\")\n)\n\n# Check it out\nstudents_1\n\n  student    class\n1       A STAT 101\n2       B GEOL 101\n3       C ANTH 101\nenrollments_1 &lt;- data.frame(\n  class = c(\"STAT 101\", \"ART 101\", \"GEOL 101\"),\n  enrollment = c(18, 17, 24)\n)\n\n# Check it out\nenrollments_1\n\n     class enrollment\n1 STAT 101         18\n2  ART 101         17\n3 GEOL 101         24\nOur goal is to combine or join these datasets into one. For reference, here they are side by side:\nFirst, consider the following:\nEXAMPLE 2\nThere are various ways to join these datasets:\nLet’s learn by doing. First, try the left_join() function:\nlibrary(tidyverse)\nstudents_1 |&gt; \n  left_join(enrollments_1)\n\n  student    class enrollment\n1       A STAT 101         18\n2       B GEOL 101         24\n3       C ANTH 101         NA\nEXAMPLE 3\nNext, explore how our datasets are joined using inner_join():\nstudents_1 |&gt; \n  inner_join(enrollments_1)\n\n  student    class enrollment\n1       A STAT 101         18\n2       B GEOL 101         24\nEXAMPLE 4\nNext, explore how our datasets are joined using full_join():\nstudents_1 |&gt; \n  full_join(enrollments_1)\n\n  student    class enrollment\n1       A STAT 101         18\n2       B GEOL 101         24\n3       C ANTH 101         NA\n4    &lt;NA&gt;  ART 101         17\nMutating joins: left, inner, full\nMutating joins add new variables (columns) to the left data table from matching observations in the right table:\nleft_data |&gt; mutating_join(right_data)\nThe most common mutating joins are:\nNOTE: When an observation in the left table has multiple matches in the right table, these mutating joins produce a separate observation in the new table for each match.\nEXAMPLE 5\nMutating joins combine information, thus increase the number of columns in a dataset (like mutate()). Filtering joins keep only certain observations in one dataset (like filter()), not based on rules related to any variables in the dataset, but on the observations that exist in another dataset. This is useful when we merely care about the membership or non-membership of an observation in the other dataset, not the raw data itself.\nIn our example data, suppose enrollments_1 only included courses being taught in the Theater building:\nstudents_1 |&gt; \n  semi_join(enrollments_1)\n\n  student    class\n1       A STAT 101\n2       B GEOL 101\nEXAMPLE 6\nLet’s try another filtering join for our example data:\nstudents_1 |&gt; \n  anti_join(enrollments_1)\n\n  student    class\n1       C ANTH 101\nFiltering joins: semi, anti\nFiltering joins keep specific observations from the left table based on whether they match an observation in the right table.\nA SUMMARY OF ALL OF OUR JOINS",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "activities/activity-11.html#warm-up",
    "href": "activities/activity-11.html#warm-up",
    "title": "9  Joining Data",
    "section": "",
    "text": "arrange() our data in a meaningful order\nsubset the data to only filter() the rows and select() the columns of interest\nmutate() existing variables and define new variables\nsummarize() various aspects of a variable, both overall and by group (group_by())\nreshape our data to fit the task at hand (pivot_longer(), pivot_wider())\n\n\n\n\n\nWhat are the best predictors of album sales?\nCombine:\n\nSpotify data on individual songs (eg: popularity, genre, characteristics)\nsales data on individual songs\n\nWhat are the best predictors of flight delays?\nCombine:\n\ndata on individual flights including airline, starting airport, and destination airport\ndata on different airlines (eg: ticket prices, reliability, etc)\ndata on different airports (eg: location, reliability, etc)\n\n\n\n\n\n\n\n\n\n\n\nWhat variable or key do these datasets have in common? Thus by what information can we match the observations in these datasets?\nRelative to this key, what info does students_1 have that enrollments_1 doesn’t?\nRelative to this key, what info does enrollments_1 have that students_1 doesn’t?\n\n\n\n\n\n\n\n\nWhat did this do? What are the roles of students_1 (the left table) and enrollments_1 (the right table)?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\n\n\n\n\n\nWhat did this do? What are the roles of students_1 (the left table) and enrollments_1 (the right table)?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\n\n\n\n\n\nWhat did this do? What are the roles of students_1 (the left table) and enrollments_1 (the right table)?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\n\n\n\n\n\nleft_join()\nKeeps all observations from the left, but discards any observations in the right that do not have a match in the left.1\ninner_join()\nKeeps only the observations from the left with a match in the right.\nfull_join()\nKeeps all observations from the left and the right. (This is less common than left_join() and inner_join()).\n\n\n\n\n\n\n\n\n\nWhat did this do? What info would it give us?\nHow does semi_join() differ from inner_join()?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\n\n\n\n\n\nWhat did this do? What info would it give us?\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try.\n\n\n\n\n\nsemi_join()\nDiscards any observations in the left table that do not have a match in the right table. If there are multiple matches of right cases to a left case, it keeps just one copy of the left case.\nanti_join()\nDiscards any observations in the left table that do have a match in the right table.",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "activities/activity-11.html#exercises",
    "href": "activities/activity-11.html#exercises",
    "title": "9  Joining Data",
    "section": "9.2 Exercises",
    "text": "9.2 Exercises\n\nExercise 1: Where are my keys?\n\nPart a\nDefine two new datasets, with different students and courses:\n\nstudents_2 &lt;- data.frame(\n  student = c(\"D\", \"E\", \"F\"),\n  class = c(\"COMP 101\", \"BIOL 101\", \"POLI 101\")\n)\n\n# Check it out\nstudents_2\n\n  student    class\n1       D COMP 101\n2       E BIOL 101\n3       F POLI 101\n\nenrollments_2 &lt;- data.frame(\n  course = c(\"ART 101\", \"BIOL 101\", \"COMP 101\"),\n  enrollment = c(18, 20, 19)\n)\n\n# Check it out\nenrollments_2\n\n    course enrollment\n1  ART 101         18\n2 BIOL 101         20\n3 COMP 101         19\n\n\nTo connect the course enrollments to the students’ courses, try do a left_join(). You get an error! Identify the problem by reviewing the error message and the datasets we’re trying to join.\n\n# eval = FALSE: don't evaluate this chunk when knitting. it produces an error.\nstudents_2 |&gt; \n  left_join(enrollments_2)\n\n\n\nPart b\nThe problem is that course name, the key or variable that links these two datasets, is labeled differently: class in the students_2 data and course in the enrollments_2 data. Thus we have to specify these keys in our code:\n\nstudents_2 |&gt; \n  left_join(enrollments_2, by = c(\"class\" = \"course\"))\n\n  student    class enrollment\n1       D COMP 101         19\n2       E BIOL 101         20\n3       F POLI 101         NA\n\n\n\n# The order of the keys is important:\n# by = c(\"left data key\" = \"right data key\")\n# The order is mixed up here, thus we get an error:\nstudents_2 |&gt; \n  left_join(enrollments_2, by = c(\"course\" = \"class\"))\n\n\n\nPart c\nDefine another set of fake data which adds grade information:\n\n# Add student grades in each course\nstudents_3 &lt;- data.frame(\n  student = c(\"Y\", \"Y\", \"Z\", \"Z\"),\n  class = c(\"COMP 101\", \"BIOL 101\", \"POLI 101\", \"COMP 101\"),\n  grade = c(\"B\", \"S\", \"C\", \"A\")\n)\n\n# Check it out\nstudents_3\n\n  student    class grade\n1       Y COMP 101     B\n2       Y BIOL 101     S\n3       Z POLI 101     C\n4       Z COMP 101     A\n\n# Add average grades in each course\nenrollments_3 &lt;- data.frame(\n  class = c(\"ART 101\", \"BIOL 101\",\"COMP 101\"),\n  grade = c(\"B\", \"A\", \"A-\"),\n  enrollment = c(20, 18, 19)\n)\n\n# Check it out\nenrollments_3\n\n     class grade enrollment\n1  ART 101     B         20\n2 BIOL 101     A         18\n3 COMP 101    A-         19\n\n\nTry doing a left_join() to link the students’ classes to their enrollment info. Did this work? Try and figure out the culprit by examining the output.\n\nstudents_3 |&gt; \n  left_join(enrollments_3)\n\n  student    class grade enrollment\n1       Y COMP 101     B         NA\n2       Y BIOL 101     S         NA\n3       Z POLI 101     C         NA\n4       Z COMP 101     A         NA\n\n\n\n\nPart d\nThe issue here is that our datasets have 2 column names in common: class and grade. BUT grade is measuring 2 different things here: individual student grades in students_3 and average student grades in enrollments_3. Thus it doesn’t make sense to try to join the datasets with respect to this variable. We can again solve this by specifying that we want to join the datasets using the class variable or key. What are grade.x and grade.y?\n\nstudents_3 |&gt; \n  left_join(enrollments_3, by = c(\"class\" = \"class\"))\n\n  student    class grade.x grade.y enrollment\n1       Y COMP 101       B      A-         19\n2       Y BIOL 101       S       A         18\n3       Z POLI 101       C    &lt;NA&gt;         NA\n4       Z COMP 101       A      A-         19\n\n\n\n\n\n\n\n\n\n\nExercise 2: More small practice\nBefore applying these ideas to bigger datasets, let’s practice identifying which join is appropriate in different scenarios. Define the following fake data on voters (people who have voted) and contact info for voting age adults (people who could vote):\n\n# People who have voted\nvoters &lt;- data.frame(\n  id = c(\"A\", \"D\", \"E\", \"F\", \"G\"),\n  times_voted = c(2, 4, 17, 6, 20)\n)\n\nvoters\n\n  id times_voted\n1  A           2\n2  D           4\n3  E          17\n4  F           6\n5  G          20\n\n# Contact info for voting age adults\ncontact &lt;- data.frame(\n  name = c(\"A\", \"B\", \"C\", \"D\"),\n  address = c(\"summit\", \"grand\", \"snelling\", \"fairview\"),\n  age = c(24, 89, 43, 38)\n)\n\ncontact\n\n  name  address age\n1    A   summit  24\n2    B    grand  89\n3    C snelling  43\n4    D fairview  38\n\n\nUse the appropriate join for each prompt below. In each case, think before you type:\n\nWhat dataset goes on the left?\nWhat do you want the resulting dataset to look like? How many rows and columns will it have?\n\n\n# 1. We want contact info for people who HAVEN'T voted\ncontact %&gt;% \n  anti_join(voters, c(\"name\" = \"id\"))\n\n  name  address age\n1    B    grand  89\n2    C snelling  43\n\n# 2. We want contact info for people who HAVE voted\ncontact %&gt;%  \n  semi_join(voters, by = c(\"name\" = \"id\"))\n\n  name  address age\n1    A   summit  24\n2    D fairview  38\n\n# 3. We want any data available on each person\nvoters %&gt;% \n  full_join(contact, c('id' = 'name'))\n\n  id times_voted  address age\n1  A           2   summit  24\n2  D           4 fairview  38\n3  E          17     &lt;NA&gt;  NA\n4  F           6     &lt;NA&gt;  NA\n5  G          20     &lt;NA&gt;  NA\n6  B          NA    grand  89\n7  C          NA snelling  43\n\n# 4. When possible, we want to add contact info to the voting roster\nvoters %&gt;% \n  left_join(contact, c(\"id\" = \"name\"))\n\n  id times_voted  address age\n1  A           2   summit  24\n2  D           4 fairview  38\n3  E          17     &lt;NA&gt;  NA\n4  F           6     &lt;NA&gt;  NA\n5  G          20     &lt;NA&gt;  NA\n\n\n\n\n\n\n\n\n\nExercise 3: Bigger datasets\nLet’s apply these ideas to some bigger datasets. In grades, each row is a student-class pair with information on:\n\nsid = student ID\ngrade = student’s grade\nsessionID = an identifier of the class section\n\n\n\n     sid grade   sessionID\n1 S31185    D+ session1784\n2 S31185    B+ session1785\n3 S31185    A- session1791\n4 S31185    B+ session1792\n5 S31185    B- session1794\n6 S31185    C+ session1795\n\n\nIn courses, each row corresponds to a class section with information on:\n\nsessionID = an identifier of the class section\ndept = department\nlevel = course level (eg: 100)\nsem = semester\nenroll = enrollment (number of students)\niid = instructor ID\n\n\n\n    sessionID dept level    sem enroll     iid\n1 session1784    M   100 FA1991     22 inst265\n2 session1785    k   100 FA1991     52 inst458\n3 session1791    J   100 FA1993     22 inst223\n4 session1792    J   300 FA1993     20 inst235\n5 session1794    J   200 FA1993     22 inst234\n6 session1795    J   200 SP1994     26 inst230\n\n\nUse R code to take a quick glance at the data.\n\n# How many observations (rows) and variables (columns) are there in the grades data?\ndim(grades)\n\n[1] 5844    3\n\n# How many observations (rows) and variables (columns) are there in the courses data?\ndim(courses)\n\n[1] 1718    6\n\n\n\n\n\n\n\n\n\nExercise 4: Class size\nHow big are the classes?\n\nPart a\nBefore digging in, note that some courses are listed twice in the courses data:\n\ncourses |&gt; \n  count(sessionID) |&gt; \n  filter(n &gt; 1)\n\n     sessionID n\n1  session2047 2\n2  session2067 2\n3  session2448 2\n4  session2509 2\n5  session2541 2\n6  session2824 2\n7  session2826 2\n8  session2862 2\n9  session2897 2\n10 session3046 2\n11 session3057 2\n12 session3123 2\n13 session3243 2\n14 session3257 2\n15 session3387 2\n16 session3400 2\n17 session3414 2\n18 session3430 2\n19 session3489 2\n20 session3524 2\n21 session3629 2\n22 session3643 2\n23 session3821 2\n\n\nIf we pick out just 1 of these, we learn that some courses are cross-listed in multiple departments:\n\ncourses |&gt; \n  filter(sessionID == \"session2047\")\n\nFor our class size exploration, obtain the total enrollments in each sessionID, combining any cross-listed sections. Save this as courses_combined. NOTE: There’s no joining to do here!\n\ncourses_combined &lt;- courses |&gt;\n  group_by(sessionID) |&gt;\n  summarize(enroll = sum(enroll))\n\n#Check that this has 1695 rows and 2 columns\ndim(courses_combined)\n\n[1] 1695    2\n\n\n\n\nPart b\nLet’s first examine the question of class size from the administration’s viewpoint. To this end, calculate the median class size across all class sections. (The median is the middle or 50th percentile. Unlike the mean, it’s not skewed by outliers.) THINK FIRST:\n\nWhich of the 2 datasets do you need to answer this question? One? Both?\nIf you need course information, use courses_combined not courses.\nDo you have to do any joining? If so, which dataset will go on the left, i.e. which dataset includes your primary observations of interest? Which join function will you need?\n\n\ncourses_combined %&gt;% \n  summarize(median(enroll))\n\n# A tibble: 1 × 1\n  `median(enroll)`\n             &lt;int&gt;\n1               18\n\n\n\n\nPart c\nBut how big are classes from the student perspective? To this end, calculate the median class size for each individual student. Once you have the correct output, store it as student_class_size. THINK FIRST:\n\nWhich of the 2 datasets do you need to answer this question? One? Both?\nIf you need course information, use courses_combined not courses.\nDo you have to do any joining? If so, which dataset will go on the left, i.e. which dataset includes your primary observations of interest? Which join function will you need?\n\n\nstudent_class_size &lt;- grades %&gt;% \n  inner_join(courses_combined) %&gt;% \n  group_by(sid) %&gt;% \n  summarize(med_class_size = median(enroll))\n\nstudent_class_size\n\n# A tibble: 443 × 2\n   sid    med_class_size\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 S31185           23.5\n 2 S31188           21  \n 3 S31191           25  \n 4 S31194           15  \n 5 S31197           24  \n 6 S31200           21  \n 7 S31203           20  \n 8 S31206           17  \n 9 S31209           20  \n10 S31212           21  \n# ℹ 433 more rows\n\n\n\n\nPart d\nThe median class size varies from student to student. To get a sense for the typical student experience and range in student experiences, construct and discuss a histogram of the median class sizes experienced by the students.\n\nggplot(student_class_size, aes(x = med_class_size)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5: Narrowing in on classes\n\nPart a\nShow data on the students that enrolled in session1986. THINK FIRST: Which of the 2 datasets do you need to answer this question? One? Both?\n\ngrades %&gt;% \n  filter(sessionID == \"session1986\")\n\n     sid grade   sessionID\n1 S31401    B+ session1986\n2 S32247     B session1986\n\n\n\n\nPart b\nBelow is a dataset with all courses in department E:\n\ndept_E &lt;- courses |&gt; \n  filter(dept == \"E\")\n\nWhat students enrolled in classes in department E? (We just want info on the students, not the classes.)\n\ngrades %&gt;% \n  semi_join(dept_E)\n\n      sid grade   sessionID\n1  S31245     A session2326\n2  S31470     B session3658\n3  S31470     B session3798\n4  S31470     A session3799\n5  S31938     A session2326\n6  S31968     A session3104\n7  S32022     A session3798\n8  S32046    A- session2326\n9  S32226     A session2326\n10 S32415     B session2835\n11 S32415    B+ session3799\n12 S32484    A- session3658\n\n\n\n\n\n\n\n\n\n\nExercise 6: All the wrangling\nUse all of your wrangling skills to answer the following prompts! THINK FIRST:\n\nThink about what tables you might need to join (if any). Identify the corresponding variables to match.\nYou’ll need an extra table to convert grades to grade point averages:\n\n\ngpa_conversion &lt;- tibble(\n  grade = c(\"A+\", \"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"AU\", \"S\"), \n  gp = c(4.3, 4, 3.7, 3.3, 3, 2.7, 2.3, 2, 1.7, 1.3, 1, 0.7, 0, NA, NA)\n)\n\ngpa_conversion\n\n# A tibble: 15 × 2\n   grade    gp\n   &lt;chr&gt; &lt;dbl&gt;\n 1 A+      4.3\n 2 A       4  \n 3 A-      3.7\n 4 B+      3.3\n 5 B       3  \n 6 B-      2.7\n 7 C+      2.3\n 8 C       2  \n 9 C-      1.7\n10 D+      1.3\n11 D       1  \n12 D-      0.7\n13 NC      0  \n14 AU     NA  \n15 S      NA  \n\n\n\nPart a\nHow many total student enrollments are there in each department? Order from high to low.\n\ngrades %&gt;% \n  full_join(courses) %&gt;% \n  group_by(dept) %&gt;% \n  summarize(student_enroll = sum(enroll))\n\n# A tibble: 40 × 2\n   dept  student_enroll\n   &lt;chr&gt;          &lt;int&gt;\n 1 A                 29\n 2 B                342\n 3 C               6045\n 4 D               7167\n 5 E                221\n 6 F               8123\n 7 G               8301\n 8 H               1988\n 9 I                455\n10 J               2779\n# ℹ 30 more rows\n\n\n\n\nPart b\nWhat’s the grade-point average (GPA) for each student?\n\ngrades %&gt;% \n  full_join(gpa_conversion) %&gt;% \n  select(sid, gp)\n\n        sid  gp\n1    S31185 1.3\n2    S31185 3.3\n3    S31185 3.7\n4    S31185 3.3\n5    S31185 2.7\n6    S31185 2.3\n7    S31185 2.7\n8    S31185 0.0\n9    S31188 2.0\n10   S31188 2.3\n11   S31188 0.0\n12   S31188 3.7\n13   S31188 3.3\n14   S31188 4.0\n15   S31188 3.0\n16   S31188  NA\n17   S31188 3.7\n18   S31188 4.0\n19   S31188 3.7\n20   S31188 3.3\n21   S31188 2.7\n22   S31188 3.7\n23   S31188 3.0\n24   S31188 3.0\n25   S31188 3.0\n26   S31188  NA\n27   S31191 3.0\n28   S31191 3.0\n29   S31191  NA\n30   S31191 3.7\n31   S31191 2.7\n32   S31191 3.7\n33   S31191 3.0\n34   S31191 3.0\n35   S31191 3.0\n36   S31191 3.3\n37   S31191 3.0\n38   S31191 3.3\n39   S31191 4.0\n40   S31191 3.3\n41   S31191 3.0\n42   S31194 3.0\n43   S31194 2.0\n44   S31194 3.0\n45   S31194 3.3\n46   S31194 3.7\n47   S31194 3.3\n48   S31194 3.0\n49   S31194 4.0\n50   S31194 3.3\n51   S31194 4.0\n52   S31194 3.7\n53   S31194 4.0\n54   S31197 3.0\n55   S31197 3.7\n56   S31197 3.3\n57   S31197 3.7\n58   S31197 3.0\n59   S31197 4.0\n60   S31197 3.3\n61   S31197 3.0\n62   S31197 3.3\n63   S31197 3.3\n64   S31197 3.0\n65   S31197 3.3\n66   S31197 3.7\n67   S31200 1.7\n68   S31200 2.7\n69   S31200 2.7\n70   S31200 1.7\n71   S31200 4.0\n72   S31200 1.7\n73   S31200 0.0\n74   S31200 2.0\n75   S31200 1.3\n76   S31200 2.0\n77   S31200 3.7\n78   S31200 2.3\n79   S31200 2.3\n80   S31200 2.7\n81   S31203 3.3\n82   S31203 3.0\n83   S31203 4.0\n84   S31203 4.0\n85   S31203 4.0\n86   S31203 3.7\n87   S31203 4.0\n88   S31203 4.0\n89   S31203 4.0\n90   S31203 4.0\n91   S31203 4.0\n92   S31203 3.7\n93   S31203 4.0\n94   S31206 1.3\n95   S31206 3.3\n96   S31206 2.7\n97   S31206 2.7\n98   S31206 3.0\n99   S31206 2.7\n100  S31206 0.0\n101  S31206 2.3\n102  S31206 2.7\n103  S31206 2.0\n104  S31206 2.3\n105  S31206 3.0\n106  S31206 4.0\n107  S31209 3.3\n108  S31209 2.0\n109  S31209 3.0\n110  S31209 3.3\n111  S31209 3.0\n112  S31209 3.3\n113  S31209 3.3\n114  S31209  NA\n115  S31209 3.7\n116  S31209  NA\n117  S31209 3.7\n118  S31209 2.7\n119  S31209  NA\n120  S31212 3.0\n121  S31212 3.0\n122  S31212 3.7\n123  S31212 4.0\n124  S31212 4.0\n125  S31212 3.3\n126  S31212 4.0\n127  S31212 3.7\n128  S31212 3.7\n129  S31212 4.0\n130  S31212 3.0\n131  S31212 4.0\n132  S31212 3.7\n133  S31212 4.0\n134  S31212 4.0\n135  S31215 3.3\n136  S31215 3.0\n137  S31215 3.7\n138  S31215 4.0\n139  S31215 4.0\n140  S31215 3.7\n141  S31215 3.7\n142  S31215 3.3\n143  S31215 4.0\n144  S31215 4.0\n145  S31215 3.7\n146  S31215  NA\n147  S31215 3.7\n148  S31215 3.7\n149  S31218 4.0\n150  S31218 4.0\n151  S31218 4.0\n152  S31218 3.7\n153  S31218 3.3\n154  S31218 4.0\n155  S31218 4.0\n156  S31218 3.3\n157  S31218 4.0\n158  S31218 3.7\n159  S31218 3.3\n160  S31218 3.7\n161  S31218 4.0\n162  S31218 4.0\n163  S31221 3.3\n164  S31221 3.7\n165  S31221  NA\n166  S31221 3.7\n167  S31221 3.0\n168  S31221 4.0\n169  S31221 3.7\n170  S31221 3.7\n171  S31221 3.3\n172  S31221 3.0\n173  S31221 3.3\n174  S31224 2.0\n175  S31224 3.0\n176  S31224  NA\n177  S31224 2.7\n178  S31224 3.0\n179  S31224 3.3\n180  S31224 3.0\n181  S31224 3.0\n182  S31224 3.0\n183  S31224 2.7\n184  S31224 2.0\n185  S31224 2.7\n186  S31224 3.0\n187  S31224 2.0\n188  S31227 2.3\n189  S31227 3.0\n190  S31227 4.0\n191  S31227 3.0\n192  S31227 3.7\n193  S31227 2.0\n194  S31227 3.0\n195  S31227 3.0\n196  S31227 4.0\n197  S31227 2.0\n198  S31227  NA\n199  S31227 0.0\n200  S31227 3.0\n201  S31230 2.0\n202  S31230 3.7\n203  S31230 4.0\n204  S31230 3.0\n205  S31230 3.3\n206  S31230 4.0\n207  S31230 3.7\n208  S31230 3.7\n209  S31230 3.7\n210  S31230 2.7\n211  S31230 2.7\n212  S31230 3.0\n213  S31230 3.0\n214  S31230 3.0\n215  S31230 3.7\n216  S31233 3.7\n217  S31233 3.3\n218  S31233 3.3\n219  S31233 3.7\n220  S31233 3.3\n221  S31233 4.0\n222  S31233 3.7\n223  S31233 4.0\n224  S31233 3.7\n225  S31233 3.7\n226  S31233 2.3\n227  S31233 4.0\n228  S31233  NA\n229  S31236 3.0\n230  S31236 2.7\n231  S31236 3.0\n232  S31236 3.0\n233  S31236 1.7\n234  S31236 0.0\n235  S31236  NA\n236  S31236 3.0\n237  S31236 3.3\n238  S31236 4.0\n239  S31236 1.0\n240  S31236 4.0\n241  S31239 3.3\n242  S31239 3.7\n243  S31239 3.0\n244  S31239 3.3\n245  S31239 3.7\n246  S31239 3.3\n247  S31239 3.0\n248  S31239 3.7\n249  S31239 4.0\n250  S31239 3.3\n251  S31242 2.7\n252  S31242 2.7\n253  S31242 3.0\n254  S31242 3.0\n255  S31242 2.7\n256  S31242 3.0\n257  S31242 2.3\n258  S31242 2.0\n259  S31242 2.7\n260  S31242 2.7\n261  S31242 3.3\n262  S31245 4.0\n263  S31245 4.0\n264  S31245 4.0\n265  S31245 4.0\n266  S31245 4.0\n267  S31245 4.0\n268  S31245  NA\n269  S31245 4.0\n270  S31245 4.0\n271  S31245  NA\n272  S31245 4.0\n273  S31245 4.0\n274  S31245 4.0\n275  S31245 4.0\n276  S31248 3.7\n277  S31248 4.0\n278  S31248 3.7\n279  S31248 3.3\n280  S31248 4.0\n281  S31248 4.0\n282  S31248 4.0\n283  S31248 4.0\n284  S31251 2.7\n285  S31251 3.0\n286  S31251 2.7\n287  S31251 3.3\n288  S31251 2.0\n289  S31251 3.0\n290  S31251 2.3\n291  S31251 3.0\n292  S31251 2.3\n293  S31251 2.0\n294  S31251 3.3\n295  S31251 3.7\n296  S31251 3.0\n297  S31251 3.0\n298  S31251 4.0\n299  S31251  NA\n300  S31254 4.0\n301  S31254 4.0\n302  S31254 4.0\n303  S31254 4.0\n304  S31254 4.0\n305  S31254 4.0\n306  S31254 4.0\n307  S31254 4.0\n308  S31254 4.0\n309  S31254 4.0\n310  S31254 4.0\n311  S31254 4.0\n312  S31254 4.0\n313  S31257 3.0\n314  S31257 3.7\n315  S31257  NA\n316  S31257 3.7\n317  S31257 3.3\n318  S31257 3.7\n319  S31257 3.3\n320  S31257 4.0\n321  S31257 4.0\n322  S31257 2.7\n323  S31257 4.0\n324  S31260 4.0\n325  S31260 4.0\n326  S31260 4.0\n327  S31260 4.0\n328  S31260 3.7\n329  S31260 4.0\n330  S31260 4.0\n331  S31260 4.0\n332  S31260 4.0\n333  S31260 4.0\n334  S31260 3.3\n335  S31260 4.0\n336  S31260 4.0\n337  S31260 4.0\n338  S31260 4.0\n339  S31263 2.7\n340  S31263 4.0\n341  S31263 3.7\n342  S31263 4.0\n343  S31263 4.0\n344  S31263 3.7\n345  S31263 3.3\n346  S31263 3.3\n347  S31263 3.0\n348  S31263 3.7\n349  S31263 3.3\n350  S31263 3.0\n351  S31266 3.3\n352  S31266 4.0\n353  S31266 3.3\n354  S31266 3.0\n355  S31266 4.0\n356  S31266 4.0\n357  S31266 2.0\n358  S31266  NA\n359  S31266 4.0\n360  S31266  NA\n361  S31266 3.0\n362  S31266 3.7\n363  S31266 4.0\n364  S31266  NA\n365  S31269 3.3\n366  S31269 3.3\n367  S31269 3.3\n368  S31269 3.7\n369  S31269 3.0\n370  S31269 4.0\n371  S31269 3.3\n372  S31269 4.0\n373  S31269 3.3\n374  S31269 4.0\n375  S31269 4.0\n376  S31272 3.7\n377  S31272 4.0\n378  S31272 3.7\n379  S31272 4.0\n380  S31272 4.0\n381  S31272 3.3\n382  S31272 4.0\n383  S31272 4.0\n384  S31272 4.0\n385  S31272 4.0\n386  S31272 4.0\n387  S31272  NA\n388  S31272 4.0\n389  S31275 4.0\n390  S31275 4.0\n391  S31275 4.0\n392  S31275 4.0\n393  S31275 4.0\n394  S31275 3.7\n395  S31275 4.0\n396  S31275 4.0\n397  S31275 3.7\n398  S31275 4.0\n399  S31275 4.0\n400  S31278 2.0\n401  S31278 3.0\n402  S31278 3.3\n403  S31278 3.3\n404  S31278 3.0\n405  S31278 3.3\n406  S31278 3.7\n407  S31278 2.7\n408  S31278 3.0\n409  S31278 3.3\n410  S31278 4.0\n411  S31278 3.3\n412  S31278 3.7\n413  S31278 4.0\n414  S31278 4.0\n415  S31278 4.0\n416  S31281 4.0\n417  S31281 4.0\n418  S31281 4.0\n419  S31281  NA\n420  S31281 3.7\n421  S31281 3.0\n422  S31281 3.3\n423  S31281 3.7\n424  S31281 3.7\n425  S31281 3.0\n426  S31281  NA\n427  S31281 3.0\n428  S31281 3.0\n429  S31284 1.0\n430  S31284 1.7\n431  S31284 3.0\n432  S31284 3.0\n433  S31284 3.3\n434  S31284 3.3\n435  S31284 3.7\n436  S31284 3.7\n437  S31284 2.7\n438  S31284 3.7\n439  S31284 3.7\n440  S31284 2.3\n441  S31284 2.7\n442  S31287 4.0\n443  S31287 4.0\n444  S31287 3.0\n445  S31287 4.0\n446  S31287 3.7\n447  S31287 3.7\n448  S31287 4.0\n449  S31287 3.0\n450  S31287 3.3\n451  S31287 2.7\n452  S31287 2.3\n453  S31287 0.7\n454  S31287 2.7\n455  S31287 2.7\n456  S31287 3.3\n457  S31290 2.7\n458  S31290 3.7\n459  S31290 2.7\n460  S31290 3.7\n461  S31290 3.0\n462  S31290 2.7\n463  S31290 4.0\n464  S31290 4.0\n465  S31290 2.7\n466  S31290 2.7\n467  S31290 3.7\n468  S31290 2.3\n469  S31290  NA\n470  S31290 3.3\n471  S31290  NA\n472  S31290  NA\n473  S31293 2.7\n474  S31293 3.3\n475  S31293 4.0\n476  S31293 4.0\n477  S31293 3.0\n478  S31293 3.0\n479  S31293 3.7\n480  S31293 3.7\n481  S31293 3.3\n482  S31293 3.3\n483  S31293 2.7\n484  S31293 4.0\n485  S31296 4.0\n486  S31296 3.7\n487  S31296 3.7\n488  S31296 4.0\n489  S31296 4.0\n490  S31296  NA\n491  S31296 3.3\n492  S31296 4.0\n493  S31296 4.0\n494  S31296 4.0\n495  S31296 4.0\n496  S31296 4.0\n497  S31296 4.0\n498  S31296 4.0\n499  S31299 2.7\n500  S31299 4.0\n501  S31299 3.7\n502  S31299 1.7\n503  S31299 4.0\n504  S31299 3.3\n505  S31299 3.0\n506  S31299 4.0\n507  S31299 3.7\n508  S31299 3.3\n509  S31299 4.0\n510  S31299 4.0\n511  S31299 4.0\n512  S31302 3.3\n513  S31302 4.0\n514  S31302 3.0\n515  S31302 2.0\n516  S31302 3.3\n517  S31302 3.0\n518  S31302 4.0\n519  S31302 3.7\n520  S31302 3.7\n521  S31302 4.0\n522  S31302 4.0\n523  S31305 3.3\n524  S31305 3.7\n525  S31305 3.3\n526  S31305 3.7\n527  S31305 2.3\n528  S31305 3.3\n529  S31305 3.7\n530  S31305 3.3\n531  S31305 3.3\n532  S31305 3.0\n533  S31305 3.0\n534  S31305 3.3\n535  S31308 2.0\n536  S31308 2.3\n537  S31308 3.7\n538  S31308 3.3\n539  S31308 3.7\n540  S31308 3.3\n541  S31308 3.0\n542  S31308 2.7\n543  S31308 2.7\n544  S31308 3.3\n545  S31308 3.7\n546  S31308 3.7\n547  S31308 2.3\n548  S31311 3.7\n549  S31311 3.3\n550  S31311 3.0\n551  S31311 3.3\n552  S31311 3.0\n553  S31311 4.0\n554  S31311 4.0\n555  S31311 3.3\n556  S31311 3.7\n557  S31311 3.7\n558  S31311 3.7\n559  S31314 3.0\n560  S31314 2.0\n561  S31314 3.7\n562  S31314 3.0\n563  S31314 3.7\n564  S31314 3.3\n565  S31314 3.3\n566  S31314 3.3\n567  S31314 3.0\n568  S31314 3.3\n569  S31317 3.7\n570  S31317 3.0\n571  S31317 3.0\n572  S31317 2.3\n573  S31317 3.7\n574  S31317 1.0\n575  S31317 3.0\n576  S31317  NA\n577  S31317  NA\n578  S31317 1.7\n579  S31317 2.3\n580  S31317  NA\n581  S31317 2.0\n582  S31317 3.0\n583  S31320 3.7\n584  S31320 2.0\n585  S31320 3.7\n586  S31320 2.7\n587  S31320 4.0\n588  S31320 4.0\n589  S31320 3.0\n590  S31320 3.3\n591  S31320 3.3\n592  S31320 4.0\n593  S31320 4.0\n594  S31320 2.0\n595  S31320  NA\n596  S31320 3.7\n597  S31323 3.0\n598  S31323 3.7\n599  S31323 3.7\n600  S31323 4.0\n601  S31323 3.3\n602  S31323 3.3\n603  S31323 3.3\n604  S31323 3.7\n605  S31323 3.7\n606  S31323 3.7\n607  S31323 3.7\n608  S31323 3.0\n609  S31323 3.7\n610  S31323 3.7\n611  S31326 3.7\n612  S31326 3.0\n613  S31326 3.3\n614  S31326 3.3\n615  S31326 3.0\n616  S31326 3.0\n617  S31326 3.7\n618  S31326 3.0\n619  S31326 3.7\n620  S31326 3.0\n621  S31326 3.0\n622  S31326 3.3\n623  S31326 3.7\n624  S31326 3.0\n625  S31326 3.0\n626  S31326  NA\n627  S31326 3.7\n628  S31329 3.0\n629  S31329 3.3\n630  S31329 3.7\n631  S31329 3.0\n632  S31329 2.3\n633  S31329 2.7\n634  S31329 3.0\n635  S31329  NA\n636  S31329 2.3\n637  S31329 3.0\n638  S31329 3.3\n639  S31329 3.0\n640  S31332 3.7\n641  S31332 3.0\n642  S31332 3.3\n643  S31332 4.0\n644  S31332 3.7\n645  S31332 4.0\n646  S31332 2.0\n647  S31332 3.7\n648  S31332 2.7\n649  S31332 4.0\n650  S31332 3.7\n651  S31335 3.3\n652  S31335 2.0\n653  S31335 4.0\n654  S31335 1.0\n655  S31335 3.3\n656  S31335 3.3\n657  S31335 3.0\n658  S31335 3.7\n659  S31335 3.7\n660  S31335 3.3\n661  S31335 3.7\n662  S31335 3.7\n663  S31335 3.7\n664  S31335 4.0\n665  S31335 3.7\n666  S31338 3.3\n667  S31338 1.7\n668  S31338 3.3\n669  S31338 3.3\n670  S31338 2.7\n671  S31338 3.0\n672  S31338 3.3\n673  S31338 3.3\n674  S31338 2.3\n675  S31338 3.3\n676  S31338 3.7\n677  S31341 4.0\n678  S31341 3.0\n679  S31341 2.7\n680  S31341 3.7\n681  S31341 3.0\n682  S31341 3.3\n683  S31341 3.0\n684  S31341 3.7\n685  S31341 2.7\n686  S31341 4.0\n687  S31341 3.0\n688  S31341 4.0\n689  S31341 3.3\n690  S31341 3.0\n691  S31344 3.0\n692  S31344 3.3\n693  S31344 3.3\n694  S31344 3.0\n695  S31344 3.7\n696  S31344  NA\n697  S31344 3.7\n698  S31344 2.7\n699  S31344 3.7\n700  S31344 3.7\n701  S31344 2.7\n702  S31344 3.0\n703  S31344 3.3\n704  S31344 3.0\n705  S31347 3.3\n706  S31347 4.0\n707  S31347 2.3\n708  S31347 2.7\n709  S31347 3.3\n710  S31347 3.0\n711  S31347 3.0\n712  S31347 3.3\n713  S31347 3.0\n714  S31347 2.7\n715  S31347 1.0\n716  S31347 3.0\n717  S31347 3.0\n718  S31347 3.7\n719  S31347 3.7\n720  S31350 2.3\n721  S31350 2.7\n722  S31350 2.7\n723  S31350 2.3\n724  S31350 3.0\n725  S31350  NA\n726  S31350 2.3\n727  S31350 2.0\n728  S31350 2.3\n729  S31350 3.0\n730  S31350 3.7\n731  S31350 3.0\n732  S31350  NA\n733  S31350 3.0\n734  S31350  NA\n735  S31350 3.7\n736  S31353 3.3\n737  S31353 3.0\n738  S31353 2.7\n739  S31353 4.0\n740  S31353 4.0\n741  S31353 3.3\n742  S31353 4.0\n743  S31353 4.0\n744  S31353  NA\n745  S31353  NA\n746  S31353 3.7\n747  S31356 4.0\n748  S31356 4.0\n749  S31356 3.7\n750  S31356 4.0\n751  S31356 4.0\n752  S31356 3.7\n753  S31356 3.3\n754  S31356 3.0\n755  S31356 4.0\n756  S31356 4.0\n757  S31356 3.7\n758  S31356 4.0\n759  S31356  NA\n760  S31356 4.0\n761  S31356  NA\n762  S31356 3.7\n763  S31356  NA\n764  S31359 4.0\n765  S31359 3.7\n766  S31359 4.0\n767  S31359 4.0\n768  S31359 3.3\n769  S31359 3.0\n770  S31359 3.3\n771  S31359 3.3\n772  S31359  NA\n773  S31359 3.7\n774  S31359 4.0\n775  S31359 4.0\n776  S31359 4.0\n777  S31359 4.0\n778  S31359 4.0\n779  S31362 3.7\n780  S31362 4.0\n781  S31362 4.0\n782  S31362 3.3\n783  S31362 4.0\n784  S31362  NA\n785  S31362 3.7\n786  S31362 3.7\n787  S31362 3.7\n788  S31362 3.7\n789  S31362 4.0\n790  S31365 3.3\n791  S31365 3.7\n792  S31365 3.0\n793  S31365 3.0\n794  S31365 3.0\n795  S31365 2.3\n796  S31365 3.7\n797  S31365  NA\n798  S31365 3.0\n799  S31365  NA\n800  S31365 3.7\n801  S31365 3.3\n802  S31365 3.0\n803  S31365 4.0\n804  S31368 4.0\n805  S31368 3.7\n806  S31368 4.0\n807  S31368 4.0\n808  S31368 3.3\n809  S31368 3.3\n810  S31368 3.7\n811  S31368 3.3\n812  S31368 3.3\n813  S31368 4.0\n814  S31368 3.3\n815  S31368 3.7\n816  S31368 4.0\n817  S31371 3.0\n818  S31371 3.7\n819  S31371 3.3\n820  S31371 4.0\n821  S31371 4.0\n822  S31371 3.3\n823  S31371 2.3\n824  S31371 2.7\n825  S31371 4.0\n826  S31371 3.7\n827  S31371 3.3\n828  S31371  NA\n829  S31371 4.0\n830  S31371  NA\n831  S31374 2.7\n832  S31374 2.3\n833  S31374 1.7\n834  S31374 2.0\n835  S31374 3.0\n836  S31374 3.0\n837  S31374 3.7\n838  S31374 3.0\n839  S31374 4.0\n840  S31374  NA\n841  S31374 3.3\n842  S31374 2.3\n843  S31374 3.0\n844  S31374  NA\n845  S31374 4.0\n846  S31377 4.0\n847  S31377 4.0\n848  S31377 4.0\n849  S31377 4.0\n850  S31377 4.0\n851  S31377 4.0\n852  S31377 4.0\n853  S31377 4.0\n854  S31377 3.3\n855  S31377 4.0\n856  S31377 3.7\n857  S31377 4.0\n858  S31377 4.0\n859  S31377 4.0\n860  S31377 3.3\n861  S31380 3.3\n862  S31380 3.3\n863  S31380 4.0\n864  S31380 4.0\n865  S31380 2.7\n866  S31380 3.3\n867  S31380 3.3\n868  S31380  NA\n869  S31380 4.0\n870  S31380 3.7\n871  S31380 4.0\n872  S31383 4.0\n873  S31383 3.7\n874  S31383 3.7\n875  S31383 3.7\n876  S31383 3.3\n877  S31383 4.0\n878  S31383 3.7\n879  S31383 3.7\n880  S31383 3.7\n881  S31383 4.0\n882  S31383  NA\n883  S31383 3.7\n884  S31386 3.7\n885  S31386 3.0\n886  S31386 3.0\n887  S31386 3.7\n888  S31386 3.3\n889  S31386 3.0\n890  S31386 4.0\n891  S31386 3.0\n892  S31386 2.7\n893  S31386  NA\n894  S31386 3.0\n895  S31389 2.3\n896  S31389 3.7\n897  S31389 3.3\n898  S31389 3.3\n899  S31389  NA\n900  S31389 3.3\n901  S31389 4.0\n902  S31389 2.7\n903  S31389 3.3\n904  S31389  NA\n905  S31389 3.3\n906  S31389  NA\n907  S31389 3.3\n908  S31389 3.3\n909  S31392 3.3\n910  S31392 3.3\n911  S31392 2.7\n912  S31392 4.0\n913  S31392 3.0\n914  S31392 3.3\n915  S31392  NA\n916  S31392 3.7\n917  S31392 3.7\n918  S31392 4.0\n919  S31392 3.7\n920  S31392 3.7\n921  S31392 3.3\n922  S31392  NA\n923  S31395 4.0\n924  S31395 2.7\n925  S31395 4.0\n926  S31395 4.0\n927  S31395 4.0\n928  S31395 3.7\n929  S31395 3.7\n930  S31395 4.0\n931  S31395  NA\n932  S31395 4.0\n933  S31395 4.0\n934  S31395  NA\n935  S31395 4.0\n936  S31398 3.0\n937  S31398 3.0\n938  S31398 3.7\n939  S31398 3.0\n940  S31398 3.0\n941  S31398 3.3\n942  S31398 3.7\n943  S31398 3.0\n944  S31398 3.3\n945  S31398 3.7\n946  S31398 2.7\n947  S31398 3.3\n948  S31401 3.7\n949  S31401 3.3\n950  S31401 2.3\n951  S31401 4.0\n952  S31401 2.0\n953  S31401 3.0\n954  S31401 3.0\n955  S31401 3.7\n956  S31401 1.7\n957  S31401 3.0\n958  S31401 2.3\n959  S31401 3.7\n960  S31401 3.0\n961  S31401 3.3\n962  S31404 2.3\n963  S31404 3.3\n964  S31404 3.3\n965  S31404 3.0\n966  S31404 2.3\n967  S31404 3.0\n968  S31404 3.7\n969  S31404 3.0\n970  S31404 4.0\n971  S31404 3.7\n972  S31404 3.7\n973  S31404  NA\n974  S31404 3.0\n975  S31404 2.7\n976  S31407 3.0\n977  S31407 3.3\n978  S31407 3.0\n979  S31407 3.3\n980  S31407 3.7\n981  S31407 4.0\n982  S31407 2.3\n983  S31407 3.7\n984  S31407 4.0\n985  S31407 3.3\n986  S31407 3.7\n987  S31407 4.0\n988  S31407  NA\n989  S31407 3.7\n990  S31407 3.7\n991  S31407  NA\n992  S31410 2.7\n993  S31410 2.7\n994  S31410 3.0\n995  S31410 3.3\n996  S31410 3.7\n997  S31410 3.3\n998  S31410 3.7\n999  S31410 3.3\n1000 S31410 4.0\n1001 S31410 3.7\n1002 S31410 3.3\n1003 S31413 3.7\n1004 S31413 4.0\n1005 S31413 3.7\n1006 S31413  NA\n1007 S31413 3.7\n1008 S31413 4.0\n1009 S31413 3.3\n1010 S31413  NA\n1011 S31413 4.0\n1012 S31413 3.7\n1013 S31413 4.0\n1014 S31416 2.3\n1015 S31416  NA\n1016 S31416 3.7\n1017 S31416 2.3\n1018 S31416 3.3\n1019 S31416 3.0\n1020 S31416 3.0\n1021 S31416 3.7\n1022 S31416 3.7\n1023 S31416 2.3\n1024 S31416 2.0\n1025 S31416 4.0\n1026 S31416 3.3\n1027 S31419 3.3\n1028 S31419 3.3\n1029 S31419 3.0\n1030 S31419 3.7\n1031 S31419 3.0\n1032 S31419  NA\n1033 S31419 2.3\n1034 S31419 3.3\n1035 S31419 3.7\n1036 S31419 2.3\n1037 S31419 3.0\n1038 S31419 0.0\n1039 S31419  NA\n1040 S31422  NA\n1041 S31422 4.0\n1042 S31422 4.0\n1043 S31422 3.7\n1044 S31422 4.0\n1045 S31422 4.0\n1046 S31422 3.7\n1047 S31422 4.0\n1048 S31422 3.7\n1049 S31422 4.0\n1050 S31422 4.0\n1051 S31422 3.7\n1052 S31422 3.7\n1053 S31422  NA\n1054 S31422 4.0\n1055 S31425 3.7\n1056 S31425 3.7\n1057 S31425 4.0\n1058 S31425 3.7\n1059 S31425 4.0\n1060 S31425 3.7\n1061 S31425 4.0\n1062 S31425 3.3\n1063 S31425 3.3\n1064 S31425 3.7\n1065 S31425 4.0\n1066 S31425 3.7\n1067 S31425 3.0\n1068 S31425 3.0\n1069 S31428 3.7\n1070 S31428 3.0\n1071 S31428 4.0\n1072 S31428 3.7\n1073 S31428 4.0\n1074 S31428 3.3\n1075 S31428 4.0\n1076 S31428 4.0\n1077 S31428 4.0\n1078 S31428  NA\n1079 S31428  NA\n1080 S31428 4.0\n1081 S31428 3.7\n1082 S31428  NA\n1083 S31428 4.0\n1084 S31431 3.0\n1085 S31431 3.0\n1086 S31431 3.7\n1087 S31431 3.7\n1088 S31431 3.3\n1089 S31431 3.3\n1090 S31431 4.0\n1091 S31431  NA\n1092 S31431 3.3\n1093 S31431 3.7\n1094 S31431 3.7\n1095 S31431 3.7\n1096 S31431 3.7\n1097 S31434 3.7\n1098 S31434 4.0\n1099 S31434 3.7\n1100 S31434 3.7\n1101 S31434 3.7\n1102 S31434 4.0\n1103 S31434 3.7\n1104 S31434 3.7\n1105 S31434 3.3\n1106 S31434 4.0\n1107 S31434 3.7\n1108 S31434 3.3\n1109 S31434 3.3\n1110 S31434  NA\n1111 S31437  NA\n1112 S31437 3.3\n1113 S31437 3.7\n1114 S31437 4.0\n1115 S31437 4.0\n1116 S31437 4.0\n1117 S31437 4.0\n1118 S31437 4.0\n1119 S31437 3.3\n1120 S31437 4.0\n1121 S31437 4.0\n1122 S31437 3.7\n1123 S31437 4.0\n1124 S31437 3.0\n1125 S31437 4.0\n1126 S31440 4.0\n1127 S31440 3.3\n1128 S31440 3.3\n1129 S31440 4.0\n1130 S31440 3.7\n1131 S31440 4.0\n1132 S31440 4.0\n1133 S31440 4.0\n1134 S31440 4.0\n1135 S31440 4.0\n1136 S31440 3.7\n1137 S31440 4.0\n1138 S31440 3.7\n1139 S31440 4.0\n1140 S31440 4.0\n1141 S31440 3.7\n1142 S31440 4.0\n1143 S31443 3.0\n1144 S31443 2.0\n1145 S31443 2.0\n1146 S31443 3.0\n1147 S31443 2.7\n1148 S31443 2.7\n1149 S31446 3.0\n1150 S31446 3.7\n1151 S31446 4.0\n1152 S31446 3.3\n1153 S31446 3.0\n1154 S31446 3.7\n1155 S31446 4.0\n1156 S31446 4.0\n1157 S31446 3.7\n1158 S31446  NA\n1159 S31446 4.0\n1160 S31449  NA\n1161 S31449 3.7\n1162 S31449 3.3\n1163 S31449 3.3\n1164 S31449 3.0\n1165 S31449 3.3\n1166 S31449 3.7\n1167 S31449  NA\n1168 S31449 3.3\n1169 S31449 3.0\n1170 S31449 3.3\n1171 S31449 3.0\n1172 S31452 3.3\n1173 S31452 3.0\n1174 S31452 4.0\n1175 S31452 3.7\n1176 S31452 4.0\n1177 S31452 3.7\n1178 S31452  NA\n1179 S31452 3.7\n1180 S31452 2.7\n1181 S31452 3.0\n1182 S31452 2.3\n1183 S31452 4.0\n1184 S31452 3.0\n1185 S31452 3.7\n1186 S31452 3.0\n1187 S31452 3.0\n1188 S31455 3.3\n1189 S31455 2.7\n1190 S31455 3.3\n1191 S31455 3.7\n1192 S31455 4.0\n1193 S31455 4.0\n1194 S31455 4.0\n1195 S31455 4.0\n1196 S31455 4.0\n1197 S31455  NA\n1198 S31455 4.0\n1199 S31455 4.0\n1200 S31455 4.0\n1201 S31455  NA\n1202 S31455  NA\n1203 S31458 4.0\n1204 S31458 4.0\n1205 S31458 4.0\n1206 S31458 4.0\n1207 S31458 4.0\n1208 S31458 4.0\n1209 S31458 4.0\n1210 S31458  NA\n1211 S31458 4.0\n1212 S31458 4.0\n1213 S31458 4.0\n1214 S31458 3.7\n1215 S31458 4.0\n1216 S31458 4.0\n1217 S31461 4.0\n1218 S31461 4.0\n1219 S31461 4.0\n1220 S31461 4.0\n1221 S31461 4.0\n1222 S31461 4.0\n1223 S31461  NA\n1224 S31461 4.0\n1225 S31461 4.0\n1226 S31461 4.0\n1227 S31461 3.3\n1228 S31461 4.0\n1229 S31464 3.0\n1230 S31464 3.7\n1231 S31464 2.7\n1232 S31464 2.7\n1233 S31464  NA\n1234 S31464 3.3\n1235 S31464 2.0\n1236 S31464  NA\n1237 S31464 3.7\n1238 S31464 4.0\n1239 S31464 3.7\n1240 S31464 3.0\n1241 S31464 3.3\n1242 S31467 4.0\n1243 S31467 3.0\n1244 S31467 3.3\n1245 S31467 3.7\n1246 S31467 4.0\n1247 S31467 3.7\n1248 S31467 3.3\n1249 S31467  NA\n1250 S31467 3.7\n1251 S31467 3.7\n1252 S31467  NA\n1253 S31467 3.7\n1254 S31467 2.7\n1255 S31467 4.0\n1256 S31467 3.0\n1257 S31467 3.0\n1258 S31467 1.0\n1259 S31470 2.0\n1260 S31470 4.0\n1261 S31470 2.7\n1262 S31470 3.7\n1263 S31470 2.7\n1264 S31470 3.7\n1265 S31470 2.7\n1266 S31470 3.3\n1267 S31470 3.3\n1268 S31470 3.3\n1269 S31470 3.0\n1270 S31470 3.0\n1271 S31470 3.7\n1272 S31470 3.0\n1273 S31470 4.0\n1274 S31473 3.3\n1275 S31473 3.0\n1276 S31473 3.0\n1277 S31473 3.7\n1278 S31473 4.0\n1279 S31473 3.7\n1280 S31473 3.3\n1281 S31473  NA\n1282 S31473 4.0\n1283 S31473 3.3\n1284 S31476 4.0\n1285 S31476 4.0\n1286 S31476 3.7\n1287 S31476 4.0\n1288 S31476 4.0\n1289 S31476 4.0\n1290 S31476 4.0\n1291 S31476 4.0\n1292 S31476 3.7\n1293 S31476 3.7\n1294 S31476 4.0\n1295 S31476 4.0\n1296 S31476 4.0\n1297 S31476 4.0\n1298 S31476 4.0\n1299 S31476 4.0\n1300 S31476  NA\n1301 S31479 4.0\n1302 S31479 4.0\n1303 S31479 3.3\n1304 S31479 3.7\n1305 S31479  NA\n1306 S31479 2.7\n1307 S31479 4.0\n1308 S31479  NA\n1309 S31479 3.3\n1310 S31479 3.3\n1311 S31479 3.0\n1312 S31479 3.7\n1313 S31482 3.7\n1314 S31482 3.0\n1315 S31482 4.0\n1316 S31482 3.3\n1317 S31482 3.3\n1318 S31482 2.7\n1319 S31482 3.0\n1320 S31482 3.7\n1321 S31482 4.0\n1322 S31482 3.3\n1323 S31482 3.7\n1324 S31485 3.7\n1325 S31485 4.0\n1326 S31485 4.0\n1327 S31485 3.0\n1328 S31485 4.0\n1329 S31485  NA\n1330 S31485  NA\n1331 S31485 4.0\n1332 S31485 3.7\n1333 S31485 4.0\n1334 S31485 3.0\n1335 S31485 3.7\n1336 S31488 3.3\n1337 S31488 3.3\n1338 S31488 4.0\n1339 S31488 2.7\n1340 S31488 3.0\n1341 S31488 4.0\n1342 S31488 4.0\n1343 S31488 4.0\n1344 S31488  NA\n1345 S31488 3.3\n1346 S31488 3.3\n1347 S31488 3.3\n1348 S31491 4.0\n1349 S31491 4.0\n1350 S31491 4.0\n1351 S31491 4.0\n1352 S31491 4.0\n1353 S31491 4.0\n1354 S31491 4.0\n1355 S31491 4.0\n1356 S31491 4.0\n1357 S31491 4.0\n1358 S31491 4.0\n1359 S31491 4.0\n1360 S31494 3.3\n1361 S31494 3.3\n1362 S31494 3.3\n1363 S31494 2.7\n1364 S31494 3.3\n1365 S31494 3.0\n1366 S31494 4.0\n1367 S31494 3.3\n1368 S31494 3.0\n1369 S31494 3.7\n1370 S31494 3.7\n1371 S31494 3.3\n1372 S31494  NA\n1373 S31494 3.7\n1374 S31494 3.3\n1375 S31497 4.0\n1376 S31497 3.7\n1377 S31497 3.0\n1378 S31497 3.7\n1379 S31497 2.7\n1380 S31497 3.0\n1381 S31497 3.3\n1382 S31497 4.0\n1383 S31497 2.7\n1384 S31497 3.0\n1385 S31497 3.3\n1386 S31497 3.3\n1387 S31500 3.0\n1388 S31500 3.3\n1389 S31500 4.0\n1390 S31500 2.3\n1391 S31500 3.3\n1392 S31500 3.3\n1393 S31500 3.7\n1394 S31500 3.7\n1395 S31500 3.7\n1396 S31500 4.0\n1397 S31500 4.0\n1398 S31500 3.7\n1399 S31503 3.3\n1400 S31503 4.0\n1401 S31503 2.7\n1402 S31503 3.3\n1403 S31503 3.3\n1404 S31503 3.7\n1405 S31503 3.3\n1406 S31503 3.7\n1407 S31503 3.3\n1408 S31503 3.7\n1409 S31503 3.3\n1410 S31503 3.7\n1411 S31503 3.7\n1412 S31506 3.7\n1413 S31506 4.0\n1414 S31506 3.7\n1415 S31506 3.7\n1416 S31506 4.0\n1417 S31506 4.0\n1418 S31506 3.7\n1419 S31506 3.7\n1420 S31506  NA\n1421 S31506 4.0\n1422 S31506 3.7\n1423 S31506 3.7\n1424 S31506 3.7\n1425 S31509 3.3\n1426 S31509 3.7\n1427 S31509 4.0\n1428 S31509 2.3\n1429 S31509  NA\n1430 S31509 3.7\n1431 S31509 4.0\n1432 S31509 4.0\n1433 S31509  NA\n1434 S31509 4.0\n1435 S31509 3.3\n1436 S31509 3.3\n1437 S31509 4.0\n1438 S31512 0.0\n1439 S31512 2.7\n1440 S31512 3.7\n1441 S31512 3.0\n1442 S31512 2.7\n1443 S31512  NA\n1444 S31512 1.7\n1445 S31512 2.7\n1446 S31512  NA\n1447 S31512 3.3\n1448 S31512  NA\n1449 S31512 1.0\n1450 S31512 2.7\n1451 S31512 3.7\n1452 S31512  NA\n1453 S31515 3.3\n1454 S31515 4.0\n1455 S31515 4.0\n1456 S31515 3.3\n1457 S31515 4.0\n1458 S31515  NA\n1459 S31515 3.7\n1460 S31515 4.0\n1461 S31515 3.7\n1462 S31515  NA\n1463 S31515 3.0\n1464 S31515 3.3\n1465 S31515 3.7\n1466 S31518 4.0\n1467 S31518 3.7\n1468 S31518 4.0\n1469 S31518 4.0\n1470 S31518 4.0\n1471 S31518 4.0\n1472 S31518  NA\n1473 S31518 4.0\n1474 S31518 4.0\n1475 S31518 4.0\n1476 S31518 4.0\n1477 S31518 4.0\n1478 S31518 4.0\n1479 S31518 4.0\n1480 S31518 4.0\n1481 S31521 3.7\n1482 S31521 3.7\n1483 S31521 3.7\n1484 S31521 3.7\n1485 S31521 4.0\n1486 S31521 3.7\n1487 S31521 3.3\n1488 S31521 3.7\n1489 S31521 3.7\n1490 S31521 4.0\n1491 S31521 3.0\n1492 S31521  NA\n1493 S31524 3.3\n1494 S31524 3.3\n1495 S31524 4.0\n1496 S31524 3.7\n1497 S31524 3.7\n1498 S31524 3.0\n1499 S31524 3.0\n1500 S31524 3.7\n1501 S31524 3.3\n1502 S31524 3.0\n1503 S31524 3.3\n1504 S31524  NA\n1505 S31524 4.0\n1506 S31527 3.0\n1507 S31527 3.0\n1508 S31527 3.7\n1509 S31527 4.0\n1510 S31527 3.7\n1511 S31527 3.7\n1512 S31527 3.7\n1513 S31527 3.7\n1514 S31527 3.7\n1515 S31527  NA\n1516 S31527 4.0\n1517 S31527  NA\n1518 S31527 4.0\n1519 S31530 3.3\n1520 S31530 4.0\n1521 S31530 4.0\n1522 S31530 4.0\n1523 S31530 3.7\n1524 S31530 4.0\n1525 S31530 3.0\n1526 S31530 4.0\n1527 S31530 4.0\n1528 S31530 4.0\n1529 S31530 3.7\n1530 S31530  NA\n1531 S31530 4.0\n1532 S31533 1.3\n1533 S31533 2.0\n1534 S31533 3.3\n1535 S31533 2.0\n1536 S31533 3.0\n1537 S31533 3.0\n1538 S31533 2.3\n1539 S31533 2.0\n1540 S31533 2.3\n1541 S31533 2.7\n1542 S31533 3.3\n1543 S31533 4.0\n1544 S31533 2.7\n1545 S31536 3.3\n1546 S31536 3.3\n1547 S31536 3.7\n1548 S31536 3.3\n1549 S31536 2.7\n1550 S31536 3.7\n1551 S31536 4.0\n1552 S31536 3.0\n1553 S31536 3.7\n1554 S31536 3.7\n1555 S31536 3.3\n1556 S31536 3.3\n1557 S31539 3.3\n1558 S31539 4.0\n1559 S31539 4.0\n1560 S31539 3.7\n1561 S31539 3.7\n1562 S31539 3.0\n1563 S31539 3.3\n1564 S31539 3.3\n1565 S31539 2.0\n1566 S31539 4.0\n1567 S31539  NA\n1568 S31539 3.0\n1569 S31542 3.3\n1570 S31542 4.0\n1571 S31542 3.3\n1572 S31542 4.0\n1573 S31542 3.7\n1574 S31542 3.0\n1575 S31542 3.0\n1576 S31542  NA\n1577 S31542  NA\n1578 S31542 1.7\n1579 S31542 3.0\n1580 S31542  NA\n1581 S31545 3.7\n1582 S31545 3.3\n1583 S31545 4.0\n1584 S31545 3.0\n1585 S31545 3.3\n1586 S31545 3.0\n1587 S31545 3.7\n1588 S31545 3.3\n1589 S31545 3.7\n1590 S31545 3.7\n1591 S31545 3.7\n1592 S31545 4.0\n1593 S31545 3.7\n1594 S31545 3.7\n1595 S31545  NA\n1596 S31548 3.0\n1597 S31548 3.0\n1598 S31548 3.7\n1599 S31548 4.0\n1600 S31548  NA\n1601 S31548 3.7\n1602 S31548 3.7\n1603 S31548 4.0\n1604 S31548 4.0\n1605 S31548 4.0\n1606 S31548 4.0\n1607 S31548 3.3\n1608 S31548 3.7\n1609 S31548 4.0\n1610 S31548 3.7\n1611 S31551 4.0\n1612 S31551 3.7\n1613 S31551 3.7\n1614 S31551 4.0\n1615 S31551 4.0\n1616 S31551 4.0\n1617 S31551 3.7\n1618 S31551 3.0\n1619 S31551 3.7\n1620 S31551 4.0\n1621 S31551  NA\n1622 S31551 4.0\n1623 S31551  NA\n1624 S31551 3.7\n1625 S31554 3.7\n1626 S31554 3.3\n1627 S31554 3.7\n1628 S31554 3.3\n1629 S31554 3.3\n1630 S31554 2.7\n1631 S31554  NA\n1632 S31554 4.0\n1633 S31554 3.7\n1634 S31554 4.0\n1635 S31554 3.7\n1636 S31554 4.0\n1637 S31557 3.3\n1638 S31557 3.0\n1639 S31557 3.3\n1640 S31557 4.0\n1641 S31557 3.0\n1642 S31557  NA\n1643 S31557 3.7\n1644 S31557 3.0\n1645 S31557 3.3\n1646 S31557 3.0\n1647 S31557 3.7\n1648 S31557 4.0\n1649 S31557 4.0\n1650 S31560 3.7\n1651 S31560 3.7\n1652 S31560 3.3\n1653 S31560 3.7\n1654 S31560 4.0\n1655 S31560 4.0\n1656 S31560 3.7\n1657 S31560 3.3\n1658 S31560 3.7\n1659 S31560 4.0\n1660 S31560 4.0\n1661 S31560 3.7\n1662 S31560 3.7\n1663 S31560 4.0\n1664 S31560  NA\n1665 S31563 3.0\n1666 S31563 2.3\n1667 S31563 2.3\n1668 S31563 3.3\n1669 S31563 3.7\n1670 S31563  NA\n1671 S31563 4.0\n1672 S31563 3.3\n1673 S31563  NA\n1674 S31563 3.3\n1675 S31563 3.7\n1676 S31566 4.0\n1677 S31566 3.0\n1678 S31566 4.0\n1679 S31566 4.0\n1680 S31566 3.7\n1681 S31566  NA\n1682 S31566 4.0\n1683 S31566 4.0\n1684 S31566 3.7\n1685 S31566 4.0\n1686 S31566 3.7\n1687 S31566 4.0\n1688 S31566 4.0\n1689 S31566 4.0\n1690 S31566 3.7\n1691 S31566 4.0\n1692 S31566 3.7\n1693 S31569 3.0\n1694 S31569 2.7\n1695 S31569 2.7\n1696 S31569 3.3\n1697 S31569 2.3\n1698 S31569 2.3\n1699 S31569 3.3\n1700 S31569 3.7\n1701 S31569 3.3\n1702 S31572 3.0\n1703 S31572 2.7\n1704 S31572 3.7\n1705 S31572 3.7\n1706 S31572 3.3\n1707 S31572 3.0\n1708 S31572 3.3\n1709 S31572 3.0\n1710 S31572 3.0\n1711 S31572 3.3\n1712 S31572 4.0\n1713 S31575 3.0\n1714 S31575 3.3\n1715 S31575 4.0\n1716 S31575 4.0\n1717 S31575 4.0\n1718 S31575  NA\n1719 S31575 3.3\n1720 S31575 4.0\n1721 S31575 3.3\n1722 S31578 3.7\n1723 S31578 2.7\n1724 S31578 3.7\n1725 S31578 3.0\n1726 S31578 2.7\n1727 S31578 3.0\n1728 S31578 3.7\n1729 S31578 3.3\n1730 S31578 3.7\n1731 S31578  NA\n1732 S31578  NA\n1733 S31578 3.3\n1734 S31581 4.0\n1735 S31581 3.3\n1736 S31581 3.3\n1737 S31581 2.7\n1738 S31581 3.0\n1739 S31581 2.7\n1740 S31581  NA\n1741 S31581 3.7\n1742 S31581 2.3\n1743 S31581 3.0\n1744 S31581 3.3\n1745 S31584 3.7\n1746 S31584 4.0\n1747 S31584 3.7\n1748 S31584 3.0\n1749 S31584 3.7\n1750 S31584 4.0\n1751 S31584 2.7\n1752 S31584 4.0\n1753 S31584 2.0\n1754 S31584 3.7\n1755 S31584 3.0\n1756 S31584 4.0\n1757 S31584  NA\n1758 S31584 3.0\n1759 S31587 2.7\n1760 S31587 2.3\n1761 S31587 3.7\n1762 S31587 2.3\n1763 S31587 2.7\n1764 S31587 3.3\n1765 S31587 2.7\n1766 S31587 3.3\n1767 S31587 2.7\n1768 S31587 2.0\n1769 S31587 0.0\n1770 S31587 1.7\n1771 S31587 2.3\n1772 S31590 3.7\n1773 S31590  NA\n1774 S31590 3.7\n1775 S31590 3.7\n1776 S31590 4.0\n1777 S31590 3.3\n1778 S31590 3.7\n1779 S31590 2.3\n1780 S31590  NA\n1781 S31590 3.7\n1782 S31590 3.7\n1783 S31590  NA\n1784 S31590 3.7\n1785 S31593  NA\n1786 S31593 3.0\n1787 S31593 3.3\n1788 S31593 3.7\n1789 S31593 3.0\n1790 S31593 3.7\n1791 S31593 3.3\n1792 S31593 3.3\n1793 S31593 3.3\n1794 S31596 3.0\n1795 S31596 3.3\n1796 S31596 3.0\n1797 S31596 3.3\n1798 S31596 3.7\n1799 S31596 3.3\n1800 S31596 3.7\n1801 S31596 2.7\n1802 S31596 3.3\n1803 S31596 4.0\n1804 S31596 2.7\n1805 S31596 3.3\n1806 S31599 3.7\n1807 S31599 3.7\n1808 S31599 3.7\n1809 S31599 3.7\n1810 S31599 3.3\n1811 S31599 4.0\n1812 S31599 3.7\n1813 S31599 4.0\n1814 S31599 4.0\n1815 S31599 4.0\n1816 S31599 4.0\n1817 S31599 4.0\n1818 S31599  NA\n1819 S31599 4.0\n1820 S31602 3.3\n1821 S31602 3.7\n1822 S31602 3.3\n1823 S31602 3.3\n1824 S31602 3.7\n1825 S31602 4.0\n1826 S31602 4.0\n1827 S31602 3.7\n1828 S31602 3.7\n1829 S31602 3.7\n1830 S31602 4.0\n1831 S31602  NA\n1832 S31602 3.7\n1833 S31602 4.0\n1834 S31602 4.0\n1835 S31605 3.0\n1836 S31605 4.0\n1837 S31605 4.0\n1838 S31605 4.0\n1839 S31605 4.0\n1840 S31605 4.0\n1841 S31605  NA\n1842 S31605 4.0\n1843 S31605 3.7\n1844 S31605  NA\n1845 S31605  NA\n1846 S31608 3.3\n1847 S31608 3.7\n1848 S31608 4.0\n1849 S31608 2.0\n1850 S31608  NA\n1851 S31608 3.3\n1852 S31608 3.0\n1853 S31608 3.7\n1854 S31608 4.0\n1855 S31608  NA\n1856 S31608 3.3\n1857 S31608  NA\n1858 S31608 3.7\n1859 S31608 3.7\n1860 S31608 3.3\n1861 S31608  NA\n1862 S31608 3.7\n1863 S31611 4.0\n1864 S31611 4.0\n1865 S31611 3.7\n1866 S31611 4.0\n1867 S31611 3.7\n1868 S31611 4.0\n1869 S31611 3.3\n1870 S31611 4.0\n1871 S31611 3.3\n1872 S31611 3.3\n1873 S31611 4.0\n1874 S31611 3.7\n1875 S31611 4.0\n1876 S31611 3.7\n1877 S31614 4.0\n1878 S31614 3.7\n1879 S31614 3.7\n1880 S31614 3.7\n1881 S31614 3.7\n1882 S31614 3.7\n1883 S31614 2.7\n1884 S31614 3.7\n1885 S31614 2.7\n1886 S31614 3.7\n1887 S31614  NA\n1888 S31614 4.0\n1889 S31614 3.3\n1890 S31614 4.0\n1891 S31617 3.0\n1892 S31617 3.7\n1893 S31617 2.3\n1894 S31617 4.0\n1895 S31617 4.0\n1896 S31617  NA\n1897 S31617 4.0\n1898 S31617 4.0\n1899 S31617 3.7\n1900 S31617 3.7\n1901 S31617  NA\n1902 S31617 4.0\n1903 S31617  NA\n1904 S31617 4.0\n1905 S31617 4.0\n1906 S31620 4.0\n1907 S31620 4.0\n1908 S31620 4.0\n1909 S31620 3.3\n1910 S31620  NA\n1911 S31620 3.3\n1912 S31620 4.0\n1913 S31620 3.7\n1914 S31620 3.7\n1915 S31620 4.0\n1916 S31620 3.7\n1917 S31620 3.7\n1918 S31620 3.7\n1919 S31623 3.3\n1920 S31623 3.3\n1921 S31623 3.3\n1922 S31623 3.7\n1923 S31623 3.7\n1924 S31623 3.7\n1925 S31623  NA\n1926 S31623 3.7\n1927 S31623 3.7\n1928 S31623 3.7\n1929 S31623 4.0\n1930 S31623 3.7\n1931 S31626 4.0\n1932 S31626 4.0\n1933 S31626 4.0\n1934 S31626 4.0\n1935 S31626 4.0\n1936 S31626 3.0\n1937 S31626 4.0\n1938 S31626 3.7\n1939 S31626 4.0\n1940 S31626 3.7\n1941 S31626  NA\n1942 S31626  NA\n1943 S31626 4.0\n1944 S31629 1.0\n1945 S31629 3.3\n1946 S31629 3.0\n1947 S31629 3.3\n1948 S31629  NA\n1949 S31629  NA\n1950 S31629 3.3\n1951 S31629 3.7\n1952 S31629 3.7\n1953 S31629 3.7\n1954 S31629 3.7\n1955 S31629 3.3\n1956 S31629 3.7\n1957 S31632 4.0\n1958 S31632 3.7\n1959 S31632 3.7\n1960 S31632 3.7\n1961 S31632 3.7\n1962 S31632 3.0\n1963 S31632 2.7\n1964 S31632 4.0\n1965 S31632 4.0\n1966 S31632 4.0\n1967 S31635 3.7\n1968 S31635 3.7\n1969 S31635 3.7\n1970 S31635  NA\n1971 S31635 3.3\n1972 S31635 3.0\n1973 S31635 3.7\n1974 S31635 3.3\n1975 S31635 3.3\n1976 S31635 3.7\n1977 S31635 3.0\n1978 S31638 3.0\n1979 S31638 4.0\n1980 S31638 3.3\n1981 S31638 4.0\n1982 S31638 4.0\n1983 S31638 4.0\n1984 S31638 3.7\n1985 S31638 3.7\n1986 S31638 3.7\n1987 S31638 3.7\n1988 S31638 3.7\n1989 S31638 3.7\n1990 S31638 4.0\n1991 S31638 4.0\n1992 S31638 4.0\n1993 S31641 4.0\n1994 S31641 3.3\n1995 S31641 3.7\n1996 S31641 3.7\n1997 S31641 3.0\n1998 S31641 3.7\n1999 S31641 3.7\n2000 S31641 3.7\n2001 S31641 3.7\n2002 S31641 3.7\n2003 S31641 3.7\n2004 S31644 2.0\n2005 S31644 3.0\n2006 S31644 2.7\n2007 S31644 3.7\n2008 S31644 3.7\n2009 S31644 3.3\n2010 S31644 4.0\n2011 S31644 3.3\n2012 S31644 2.7\n2013 S31644 2.7\n2014 S31644 3.0\n2015 S31644 3.3\n2016 S31644 3.0\n2017 S31644 3.0\n2018 S31644 3.7\n2019 S31644 3.3\n2020 S31644 3.3\n2021 S31647 3.0\n2022 S31647 2.7\n2023 S31647 3.7\n2024 S31647 4.0\n2025 S31647 4.0\n2026 S31647 3.7\n2027 S31647 3.3\n2028 S31647 4.0\n2029 S31647 3.3\n2030 S31647 3.7\n2031 S31647 4.0\n2032 S31647  NA\n2033 S31650 3.7\n2034 S31650 2.3\n2035 S31650 3.3\n2036 S31650 2.7\n2037 S31650 4.0\n2038 S31650 3.3\n2039 S31650 4.0\n2040 S31650 3.3\n2041 S31650 3.7\n2042 S31650 4.0\n2043 S31650 3.0\n2044 S31650 3.7\n2045 S31650 3.7\n2046 S31650  NA\n2047 S31653 3.0\n2048 S31653 4.0\n2049 S31653 3.7\n2050 S31653 3.7\n2051 S31653 3.7\n2052 S31653 4.0\n2053 S31653 3.7\n2054 S31653  NA\n2055 S31653 3.7\n2056 S31653  NA\n2057 S31653 3.7\n2058 S31653 3.3\n2059 S31653 3.7\n2060 S31653 3.7\n2061 S31656 3.7\n2062 S31656 3.3\n2063 S31656 4.0\n2064 S31656 3.0\n2065 S31656 4.0\n2066 S31656 4.0\n2067 S31656 3.3\n2068 S31656 3.7\n2069 S31656  NA\n2070 S31656 4.0\n2071 S31656  NA\n2072 S31656 3.3\n2073 S31656 4.0\n2074 S31656 4.0\n2075 S31656 4.0\n2076 S31659 2.0\n2077 S31659 2.7\n2078 S31659 3.3\n2079 S31659 2.7\n2080 S31659 3.3\n2081 S31659 2.7\n2082 S31659 3.0\n2083 S31659 2.7\n2084 S31659 3.7\n2085 S31659 3.3\n2086 S31659 3.3\n2087 S31659 3.3\n2088 S31659 3.7\n2089 S31662 2.0\n2090 S31662 1.7\n2091 S31662 3.3\n2092 S31662 2.7\n2093 S31662 3.0\n2094 S31662 2.3\n2095 S31662 2.0\n2096 S31662 3.0\n2097 S31662 0.7\n2098 S31662  NA\n2099 S31662 2.0\n2100 S31662 2.7\n2101 S31662 2.0\n2102 S31662 3.3\n2103 S31662 3.0\n2104 S31665 3.0\n2105 S31665 2.3\n2106 S31665 3.3\n2107 S31665 3.7\n2108 S31665 2.0\n2109 S31665 3.7\n2110 S31665  NA\n2111 S31665 3.0\n2112 S31665 4.0\n2113 S31665 4.0\n2114 S31665 2.0\n2115 S31665 3.0\n2116 S31665 3.0\n2117 S31665 3.3\n2118 S31665 3.7\n2119 S31668 3.7\n2120 S31668 3.7\n2121 S31668 4.0\n2122 S31668 3.7\n2123 S31668 3.3\n2124 S31668 3.7\n2125 S31668 3.3\n2126 S31668 4.0\n2127 S31668 4.0\n2128 S31668 4.0\n2129 S31668 3.7\n2130 S31668 3.7\n2131 S31671 4.0\n2132 S31671 3.7\n2133 S31671 3.3\n2134 S31671 2.3\n2135 S31671 2.3\n2136 S31671 2.7\n2137 S31671 3.3\n2138 S31671 3.3\n2139 S31671  NA\n2140 S31671  NA\n2141 S31671 3.7\n2142 S31671  NA\n2143 S31671 3.7\n2144 S31674 3.3\n2145 S31674 4.0\n2146 S31674 3.7\n2147 S31674 3.7\n2148 S31674 2.7\n2149 S31674 3.7\n2150 S31674 3.3\n2151 S31674 3.0\n2152 S31674 3.7\n2153 S31674 3.7\n2154 S31677 4.0\n2155 S31677 4.0\n2156 S31677 4.0\n2157 S31677 4.0\n2158 S31677  NA\n2159 S31677  NA\n2160 S31677 4.0\n2161 S31677 4.0\n2162 S31677 4.0\n2163 S31677 3.7\n2164 S31677 4.0\n2165 S31677 4.0\n2166 S31680 3.7\n2167 S31680 4.0\n2168 S31680 3.0\n2169 S31680 3.3\n2170 S31680  NA\n2171 S31680  NA\n2172 S31680 4.0\n2173 S31680 4.0\n2174 S31680 4.0\n2175 S31680 3.0\n2176 S31680 3.7\n2177 S31680 3.3\n2178 S31680 3.3\n2179 S31680 3.3\n2180 S31683 3.7\n2181 S31683 3.3\n2182 S31683 2.7\n2183 S31683 4.0\n2184 S31683 3.0\n2185 S31683 3.7\n2186 S31683 3.3\n2187 S31683 3.7\n2188 S31683 2.7\n2189 S31683 3.7\n2190 S31683 3.7\n2191 S31683 3.7\n2192 S31686 3.3\n2193 S31686 3.7\n2194 S31686 3.0\n2195 S31686 3.3\n2196 S31686  NA\n2197 S31686 2.7\n2198 S31686 4.0\n2199 S31686 3.3\n2200 S31686 3.7\n2201 S31686 3.0\n2202 S31686 3.7\n2203 S31689 3.3\n2204 S31689 4.0\n2205 S31689 4.0\n2206 S31689 2.7\n2207 S31689 3.0\n2208 S31689 3.0\n2209 S31689 4.0\n2210 S31689 3.0\n2211 S31689 2.3\n2212 S31689 4.0\n2213 S31689 4.0\n2214 S31689 2.7\n2215 S31689 3.3\n2216 S31689  NA\n2217 S31689 3.0\n2218 S31692 3.0\n2219 S31692 3.3\n2220 S31692 2.0\n2221 S31692 2.7\n2222 S31692 3.7\n2223 S31692 3.3\n2224 S31692 2.3\n2225 S31692 3.3\n2226 S31692 3.7\n2227 S31692 2.3\n2228 S31692 3.0\n2229 S31692 4.0\n2230 S31695 4.0\n2231 S31695 4.0\n2232 S31695 3.7\n2233 S31695 3.3\n2234 S31695 3.7\n2235 S31695 4.0\n2236 S31695 4.0\n2237 S31695 3.7\n2238 S31695 4.0\n2239 S31695 4.0\n2240 S31698 2.0\n2241 S31698 3.0\n2242 S31698 3.7\n2243 S31698 3.7\n2244 S31698 3.7\n2245 S31698 4.0\n2246 S31698 3.0\n2247 S31698 3.0\n2248 S31698 3.7\n2249 S31698 3.7\n2250 S31698 3.3\n2251 S31698 3.3\n2252 S31698 3.3\n2253 S31701 3.3\n2254 S31701 1.7\n2255 S31701 4.0\n2256 S31701 3.3\n2257 S31701 3.7\n2258 S31701 3.7\n2259 S31701 2.7\n2260 S31701 4.0\n2261 S31701 3.0\n2262 S31701 2.7\n2263 S31701 3.0\n2264 S31701 3.0\n2265 S31701 3.0\n2266 S31701 3.3\n2267 S31701 3.0\n2268 S31704 3.7\n2269 S31704 3.7\n2270 S31704 3.7\n2271 S31704 3.7\n2272 S31704 3.7\n2273 S31704 3.7\n2274 S31704 4.0\n2275 S31704  NA\n2276 S31704 3.7\n2277 S31707 3.0\n2278 S31707 3.3\n2279 S31707 3.0\n2280 S31707 3.3\n2281 S31707 3.7\n2282 S31707 3.3\n2283 S31707 3.3\n2284 S31707 3.7\n2285 S31707 3.0\n2286 S31707 3.3\n2287 S31707 3.3\n2288 S31707 3.7\n2289 S31707 3.7\n2290 S31710 2.3\n2291 S31710 3.7\n2292 S31710 3.7\n2293 S31710 3.3\n2294 S31710 3.0\n2295 S31710 3.7\n2296 S31710 3.7\n2297 S31710 3.7\n2298 S31710 3.7\n2299 S31710 2.7\n2300 S31710 3.7\n2301 S31710 3.7\n2302 S31710  NA\n2303 S31713  NA\n2304 S31713 3.7\n2305 S31713 3.0\n2306 S31713 4.0\n2307 S31713 4.0\n2308 S31713  NA\n2309 S31713 3.0\n2310 S31713 3.3\n2311 S31713  NA\n2312 S31713 3.0\n2313 S31713 3.7\n2314 S31713 3.7\n2315 S31713 4.0\n2316 S31713 3.3\n2317 S31716 3.7\n2318 S31716 4.0\n2319 S31716 3.7\n2320 S31716 4.0\n2321 S31716 4.0\n2322 S31716 3.7\n2323 S31716 4.0\n2324 S31716 4.0\n2325 S31716 3.7\n2326 S31716 3.7\n2327 S31719  NA\n2328 S31719 2.0\n2329 S31719 3.7\n2330 S31719 2.7\n2331 S31719 2.7\n2332 S31719 3.0\n2333 S31719 1.7\n2334 S31719 3.3\n2335 S31719 3.3\n2336 S31719 3.3\n2337 S31719 3.3\n2338 S31719 3.3\n2339 S31719 2.3\n2340 S31719 4.0\n2341 S31722 3.0\n2342 S31722 3.7\n2343 S31722 3.3\n2344 S31722 3.7\n2345 S31722 3.7\n2346 S31722 3.7\n2347 S31722 3.7\n2348 S31722 3.3\n2349 S31722 3.3\n2350 S31722 4.0\n2351 S31722 3.3\n2352 S31722 4.0\n2353 S31722 4.0\n2354 S31722 4.0\n2355 S31725 3.7\n2356 S31725 3.3\n2357 S31725 3.7\n2358 S31725 3.3\n2359 S31725 3.7\n2360 S31725 3.3\n2361 S31725 3.7\n2362 S31725 4.0\n2363 S31725 3.7\n2364 S31725 3.7\n2365 S31725  NA\n2366 S31725 3.3\n2367 S31728 3.3\n2368 S31728 4.0\n2369 S31728 3.3\n2370 S31728 3.7\n2371 S31728  NA\n2372 S31728 3.7\n2373 S31728 3.7\n2374 S31728 4.0\n2375 S31728 3.3\n2376 S31728  NA\n2377 S31728 3.7\n2378 S31728 3.7\n2379 S31728 3.7\n2380 S31731 3.0\n2381 S31731 3.7\n2382 S31731 3.0\n2383 S31731  NA\n2384 S31731 3.0\n2385 S31731 3.0\n2386 S31731 3.3\n2387 S31731 3.0\n2388 S31731 3.3\n2389 S31731 3.7\n2390 S31731 3.3\n2391 S31731 3.3\n2392 S31731  NA\n2393 S31731 3.0\n2394 S31734 3.0\n2395 S31734 3.0\n2396 S31734 4.0\n2397 S31734 4.0\n2398 S31734 4.0\n2399 S31734 4.0\n2400 S31734 2.7\n2401 S31734 4.0\n2402 S31734 2.7\n2403 S31734 4.0\n2404 S31734  NA\n2405 S31734 3.7\n2406 S31734 3.3\n2407 S31737 3.7\n2408 S31737 4.0\n2409 S31737 4.0\n2410 S31737 4.0\n2411 S31737 4.0\n2412 S31737 3.7\n2413 S31737 3.7\n2414 S31737 4.0\n2415 S31737 3.7\n2416 S31737 2.7\n2417 S31740 2.7\n2418 S31740 3.3\n2419 S31740 4.0\n2420 S31740 3.7\n2421 S31740 3.0\n2422 S31740 2.7\n2423 S31740 3.0\n2424 S31740 2.7\n2425 S31740 2.3\n2426 S31740 3.3\n2427 S31740 4.0\n2428 S31740 3.7\n2429 S31740 3.0\n2430 S31740 3.0\n2431 S31740  NA\n2432 S31740 3.7\n2433 S31743 3.3\n2434 S31743 3.3\n2435 S31743 3.7\n2436 S31743 4.0\n2437 S31743 3.7\n2438 S31743 3.7\n2439 S31743 3.7\n2440 S31743 3.7\n2441 S31743 3.7\n2442 S31743 3.3\n2443 S31743 3.3\n2444 S31746 3.7\n2445 S31746 3.3\n2446 S31746 4.0\n2447 S31746 3.7\n2448 S31746  NA\n2449 S31746 4.0\n2450 S31746 4.0\n2451 S31746 4.0\n2452 S31746 4.0\n2453 S31746 4.0\n2454 S31749 4.0\n2455 S31749 3.7\n2456 S31749 4.0\n2457 S31749 3.0\n2458 S31749 3.7\n2459 S31749  NA\n2460 S31749 3.3\n2461 S31749  NA\n2462 S31749 3.3\n2463 S31749 3.3\n2464 S31749 3.7\n2465 S31749 3.0\n2466 S31752 3.7\n2467 S31752 3.3\n2468 S31752 3.7\n2469 S31752 3.3\n2470 S31752 3.3\n2471 S31752  NA\n2472 S31752 3.7\n2473 S31752 3.0\n2474 S31752 3.7\n2475 S31752 3.3\n2476 S31752 3.0\n2477 S31752 3.3\n2478 S31752 3.3\n2479 S31752 3.7\n2480 S31755 3.0\n2481 S31755 3.7\n2482 S31755 2.0\n2483 S31755 3.7\n2484 S31755 3.3\n2485 S31755 3.0\n2486 S31755 3.3\n2487 S31755 3.7\n2488 S31755 3.7\n2489 S31755 3.7\n2490 S31758 3.3\n2491 S31758 4.0\n2492 S31758 3.7\n2493 S31758 4.0\n2494 S31758 3.7\n2495 S31758 3.3\n2496 S31758  NA\n2497 S31758 2.7\n2498 S31758 3.3\n2499 S31758 4.0\n2500 S31758 2.3\n2501 S31758 3.0\n2502 S31761 3.3\n2503 S31761 3.3\n2504 S31761 3.7\n2505 S31761 3.0\n2506 S31761  NA\n2507 S31761 3.0\n2508 S31761 3.3\n2509 S31761 3.3\n2510 S31761 3.7\n2511 S31761 3.3\n2512 S31761 3.3\n2513 S31761 3.7\n2514 S31761 3.3\n2515 S31764 3.3\n2516 S31764 4.0\n2517 S31764 4.0\n2518 S31764 4.0\n2519 S31764 3.7\n2520 S31764 2.3\n2521 S31764 3.3\n2522 S31764 2.0\n2523 S31764 4.0\n2524 S31764 3.3\n2525 S31764 3.7\n2526 S31764  NA\n2527 S31764 4.0\n2528 S31764 2.7\n2529 S31767 3.7\n2530 S31767 3.0\n2531 S31767 3.3\n2532 S31767 4.0\n2533 S31767 3.3\n2534 S31767 3.0\n2535 S31767 3.7\n2536 S31767 3.7\n2537 S31767 3.3\n2538 S31767 4.0\n2539 S31767 3.3\n2540 S31770 3.3\n2541 S31770 3.7\n2542 S31770 4.0\n2543 S31770  NA\n2544 S31770 3.7\n2545 S31770 4.0\n2546 S31770  NA\n2547 S31770 4.0\n2548 S31770 3.7\n2549 S31770 4.0\n2550 S31770 4.0\n2551 S31770 4.0\n2552 S31770 3.0\n2553 S31770 4.0\n2554 S31770 4.0\n2555 S31773 3.7\n2556 S31773 3.0\n2557 S31773 3.0\n2558 S31773 3.3\n2559 S31773 3.3\n2560 S31773 2.3\n2561 S31773 3.0\n2562 S31773 3.0\n2563 S31773 3.7\n2564 S31773 3.7\n2565 S31773 3.7\n2566 S31773 3.3\n2567 S31773 3.3\n2568 S31773 3.7\n2569 S31773 3.7\n2570 S31773 3.0\n2571 S31773 3.3\n2572 S31776 2.3\n2573 S31776 3.3\n2574 S31776 3.3\n2575 S31776 4.0\n2576 S31776 3.7\n2577 S31776 3.7\n2578 S31776 4.0\n2579 S31776 3.7\n2580 S31776 3.0\n2581 S31776 3.7\n2582 S31776 3.0\n2583 S31776 3.0\n2584 S31776 2.7\n2585 S31776 4.0\n2586 S31779 3.3\n2587 S31779 4.0\n2588 S31779 3.7\n2589 S31779 3.0\n2590 S31779 3.0\n2591 S31779 3.0\n2592 S31779 4.0\n2593 S31779 3.3\n2594 S31779 3.3\n2595 S31779 3.3\n2596 S31779 4.0\n2597 S31779 3.7\n2598 S31782 4.0\n2599 S31782 4.0\n2600 S31782 4.0\n2601 S31782 3.0\n2602 S31782 3.0\n2603 S31782 3.0\n2604 S31782 3.3\n2605 S31782 3.0\n2606 S31782 3.3\n2607 S31782 2.7\n2608 S31782 2.0\n2609 S31782 2.7\n2610 S31782  NA\n2611 S31785 3.0\n2612 S31785 4.0\n2613 S31785 4.0\n2614 S31785 4.0\n2615 S31785 4.0\n2616 S31785 3.3\n2617 S31785 3.3\n2618 S31785 4.0\n2619 S31785 4.0\n2620 S31785 3.7\n2621 S31785 3.7\n2622 S31785 4.0\n2623 S31785 4.0\n2624 S31785 4.0\n2625 S31788 3.3\n2626 S31788 3.7\n2627 S31788 3.3\n2628 S31788 3.7\n2629 S31788 4.0\n2630 S31788 4.0\n2631 S31788 3.7\n2632 S31788 4.0\n2633 S31788 3.7\n2634 S31788 3.0\n2635 S31788 3.3\n2636 S31788 3.7\n2637 S31791 1.0\n2638 S31791 2.7\n2639 S31791 3.7\n2640 S31791  NA\n2641 S31791 4.0\n2642 S31791 4.0\n2643 S31791 3.3\n2644 S31791 3.7\n2645 S31791 3.7\n2646 S31791 4.0\n2647 S31791 3.7\n2648 S31791 4.0\n2649 S31791 3.3\n2650 S31791 2.7\n2651 S31791 3.3\n2652 S31791  NA\n2653 S31794 3.0\n2654 S31794 3.0\n2655 S31794 3.0\n2656 S31794 3.0\n2657 S31794 3.0\n2658 S31794 3.0\n2659 S31794 3.7\n2660 S31794 3.0\n2661 S31794 2.7\n2662 S31794 3.7\n2663 S31794 4.0\n2664 S31794  NA\n2665 S31794 3.0\n2666 S31797 2.3\n2667 S31797 3.0\n2668 S31797 3.3\n2669 S31797 2.7\n2670 S31797  NA\n2671 S31797 2.7\n2672 S31797 3.3\n2673 S31797 2.0\n2674 S31797 3.3\n2675 S31797 2.7\n2676 S31797 2.0\n2677 S31797 3.0\n2678 S31797 3.0\n2679 S31797 3.7\n2680 S31797 3.0\n2681 S31797 3.3\n2682 S31800 3.3\n2683 S31800 3.7\n2684 S31800 3.3\n2685 S31800 3.3\n2686 S31800 3.7\n2687 S31800 4.0\n2688 S31800 3.7\n2689 S31800 3.7\n2690 S31800 3.7\n2691 S31800 4.0\n2692 S31800 3.7\n2693 S31800 3.7\n2694 S31803 3.7\n2695 S31803 2.3\n2696 S31803 2.7\n2697 S31803 3.7\n2698 S31803  NA\n2699 S31803 4.0\n2700 S31803 3.7\n2701 S31803 4.0\n2702 S31803 3.7\n2703 S31803  NA\n2704 S31803 3.3\n2705 S31803 3.7\n2706 S31806 3.0\n2707 S31806 4.0\n2708 S31806 3.3\n2709 S31806 3.7\n2710 S31806 3.0\n2711 S31806 3.7\n2712 S31806 0.0\n2713 S31806 3.0\n2714 S31806 3.7\n2715 S31806 4.0\n2716 S31806 3.3\n2717 S31806 2.7\n2718 S31809 3.3\n2719 S31809 3.7\n2720 S31809 3.7\n2721 S31809 3.0\n2722 S31809 3.7\n2723 S31809 2.7\n2724 S31809 3.3\n2725 S31809 4.0\n2726 S31809 4.0\n2727 S31809 3.7\n2728 S31809 2.7\n2729 S31809  NA\n2730 S31809 3.0\n2731 S31809 3.7\n2732 S31812 4.0\n2733 S31812 3.0\n2734 S31812 3.7\n2735 S31812 4.0\n2736 S31812 3.7\n2737 S31812 3.0\n2738 S31812 3.0\n2739 S31812 2.3\n2740 S31812 3.3\n2741 S31812 3.3\n2742 S31812 3.0\n2743 S31815 3.7\n2744 S31815 3.0\n2745 S31815 2.7\n2746 S31815 2.0\n2747 S31815 3.0\n2748 S31815 3.0\n2749 S31815 3.3\n2750 S31815 3.0\n2751 S31815 3.7\n2752 S31815 3.3\n2753 S31815 3.7\n2754 S31815 3.0\n2755 S31815 4.0\n2756 S31815 3.7\n2757 S31818 2.0\n2758 S31818 3.3\n2759 S31818 3.3\n2760 S31818 4.0\n2761 S31818 4.0\n2762 S31818  NA\n2763 S31818 3.3\n2764 S31818 3.3\n2765 S31818 3.7\n2766 S31818 2.7\n2767 S31818 3.3\n2768 S31821 3.7\n2769 S31821 3.0\n2770 S31821 4.0\n2771 S31821 3.7\n2772 S31821 3.3\n2773 S31821 3.0\n2774 S31821 3.7\n2775 S31821 3.3\n2776 S31821 3.7\n2777 S31821 3.3\n2778 S31821 3.0\n2779 S31821 4.0\n2780 S31821  NA\n2781 S31821 3.3\n2782 S31821 4.0\n2783 S31821 3.7\n2784 S31821 3.3\n2785 S31824 3.3\n2786 S31824 3.7\n2787 S31824 3.0\n2788 S31824 3.7\n2789 S31824 1.7\n2790 S31824 1.7\n2791 S31824 3.7\n2792 S31824 3.0\n2793 S31824 3.0\n2794 S31824 3.3\n2795 S31824  NA\n2796 S31824 3.0\n2797 S31824 4.0\n2798 S31827 3.3\n2799 S31827 3.7\n2800 S31827 3.0\n2801 S31827 3.3\n2802 S31827 3.3\n2803 S31827 3.0\n2804 S31827 3.7\n2805 S31827  NA\n2806 S31827 2.3\n2807 S31827 3.0\n2808 S31827  NA\n2809 S31827 3.7\n2810 S31827 4.0\n2811 S31827 4.0\n2812 S31830 3.7\n2813 S31830 3.3\n2814 S31830 3.7\n2815 S31830 3.7\n2816 S31830 3.7\n2817 S31830 2.7\n2818 S31830 4.0\n2819 S31830 2.7\n2820 S31830 4.0\n2821 S31830 4.0\n2822 S31830 3.3\n2823 S31830 3.7\n2824 S31830  NA\n2825 S31830 4.0\n2826 S31830 3.7\n2827 S31833 2.0\n2828 S31833 2.3\n2829 S31833 3.0\n2830 S31833 2.7\n2831 S31833 2.3\n2832 S31833  NA\n2833 S31833 2.0\n2834 S31833 3.3\n2835 S31833 4.0\n2836 S31833 3.3\n2837 S31833 3.3\n2838 S31833 3.3\n2839 S31833 3.3\n2840 S31836 2.7\n2841 S31836 3.0\n2842 S31836 4.0\n2843 S31836 4.0\n2844 S31836 3.0\n2845 S31836 3.3\n2846 S31836 3.3\n2847 S31836 0.0\n2848 S31836 3.3\n2849 S31836 3.3\n2850 S31836 3.7\n2851 S31836 3.7\n2852 S31836 4.0\n2853 S31836 3.7\n2854 S31836  NA\n2855 S31836 4.0\n2856 S31839  NA\n2857 S31839 2.7\n2858 S31839  NA\n2859 S31839 3.0\n2860 S31839 3.3\n2861 S31839 4.0\n2862 S31839  NA\n2863 S31839  NA\n2864 S31839  NA\n2865 S31839 3.7\n2866 S31839 3.7\n2867 S31839 3.7\n2868 S31839 3.7\n2869 S31839 3.3\n2870 S31842 2.0\n2871 S31842 2.7\n2872 S31842 3.3\n2873 S31842 3.3\n2874 S31842 3.3\n2875 S31842 2.7\n2876 S31842 2.0\n2877 S31842 3.0\n2878 S31842 3.0\n2879 S31842 2.7\n2880 S31842 3.3\n2881 S31842 2.7\n2882 S31842 4.0\n2883 S31842 4.0\n2884 S31842 4.0\n2885 S31845 3.0\n2886 S31845 3.0\n2887 S31845 3.3\n2888 S31845 3.3\n2889 S31845 3.3\n2890 S31845 2.3\n2891 S31845 3.7\n2892 S31845 3.0\n2893 S31845 3.3\n2894 S31845 3.3\n2895 S31845 3.0\n2896 S31845 4.0\n2897 S31848 3.0\n2898 S31848 2.7\n2899 S31848 3.0\n2900 S31848 3.3\n2901 S31848 3.7\n2902 S31848 3.0\n2903 S31848 3.0\n2904 S31848 3.3\n2905 S31848 3.7\n2906 S31848 4.0\n2907 S31848 4.0\n2908 S31848 2.7\n2909 S31848 4.0\n2910 S31848 3.7\n2911 S31848 2.3\n2912 S31848 2.7\n2913 S31848 4.0\n2914 S31851 2.7\n2915 S31851 3.0\n2916 S31851  NA\n2917 S31851 3.3\n2918 S31851 4.0\n2919 S31851 3.3\n2920 S31851 3.0\n2921 S31851 3.3\n2922 S31851  NA\n2923 S31851 3.0\n2924 S31851 3.3\n2925 S31851 3.7\n2926 S31851 3.0\n2927 S31851 3.0\n2928 S31854 3.7\n2929 S31854 3.3\n2930 S31854 3.7\n2931 S31854 4.0\n2932 S31854 3.0\n2933 S31854 3.7\n2934 S31854 4.0\n2935 S31854  NA\n2936 S31854  NA\n2937 S31854 3.0\n2938 S31854 3.0\n2939 S31854 3.7\n2940 S31854 2.7\n2941 S31854 4.0\n2942 S31857 2.3\n2943 S31857 3.0\n2944 S31857 2.7\n2945 S31857 2.0\n2946 S31857 2.7\n2947 S31857 2.7\n2948 S31857 1.7\n2949 S31857 3.3\n2950 S31857 3.3\n2951 S31857 4.0\n2952 S31857 3.7\n2953 S31860 4.0\n2954 S31860 3.7\n2955 S31860 3.0\n2956 S31860 3.7\n2957 S31860 3.7\n2958 S31860 4.0\n2959 S31860 3.3\n2960 S31860 4.0\n2961 S31860 4.0\n2962 S31860 4.0\n2963 S31860 3.7\n2964 S31860 4.0\n2965 S31860 3.7\n2966 S31860 3.7\n2967 S31863 2.7\n2968 S31863 4.0\n2969 S31863 3.7\n2970 S31863 2.7\n2971 S31863 3.7\n2972 S31863 3.3\n2973 S31863 3.7\n2974 S31863 3.7\n2975 S31863 3.7\n2976 S31863 3.3\n2977 S31863 4.0\n2978 S31866 4.0\n2979 S31866 3.3\n2980 S31866 3.7\n2981 S31866 4.0\n2982 S31866 3.7\n2983 S31866 3.3\n2984 S31866 4.0\n2985 S31866 3.3\n2986 S31866 4.0\n2987 S31866 3.3\n2988 S31866  NA\n2989 S31866 4.0\n2990 S31866 3.7\n2991 S31866 4.0\n2992 S31869 3.7\n2993 S31869 2.7\n2994 S31869 4.0\n2995 S31869 4.0\n2996 S31869 2.0\n2997 S31869 4.0\n2998 S31869 4.0\n2999 S31869  NA\n3000 S31869 4.0\n3001 S31869 4.0\n3002 S31869  NA\n3003 S31869 3.3\n3004 S31869 4.0\n3005 S31869 3.3\n3006 S31872 3.7\n3007 S31872 3.7\n3008 S31872 3.7\n3009 S31872 4.0\n3010 S31872 4.0\n3011 S31872 2.7\n3012 S31872 4.0\n3013 S31872 3.0\n3014 S31872 4.0\n3015 S31872 3.7\n3016 S31872 4.0\n3017 S31872 4.0\n3018 S31872 3.7\n3019 S31872 3.0\n3020 S31872 4.0\n3021 S31872 2.0\n3022 S31872 4.0\n3023 S31875 4.0\n3024 S31875 3.7\n3025 S31875 4.0\n3026 S31875 4.0\n3027 S31875 4.0\n3028 S31875 4.0\n3029 S31875 4.0\n3030 S31875 4.0\n3031 S31875 4.0\n3032 S31875 4.0\n3033 S31875 3.7\n3034 S31875 3.7\n3035 S31875 3.7\n3036 S31878 2.0\n3037 S31878 3.3\n3038 S31878 3.7\n3039 S31878 4.0\n3040 S31878 3.3\n3041 S31878 3.0\n3042 S31878 3.7\n3043 S31878 2.3\n3044 S31878 2.0\n3045 S31878  NA\n3046 S31878  NA\n3047 S31878  NA\n3048 S31878 2.3\n3049 S31881 3.7\n3050 S31881 3.7\n3051 S31881 3.7\n3052 S31881 3.3\n3053 S31881 3.3\n3054 S31881 4.0\n3055 S31881 3.7\n3056 S31881 4.0\n3057 S31881 3.3\n3058 S31881 3.7\n3059 S31881 3.7\n3060 S31881 4.0\n3061 S31884 4.0\n3062 S31884 3.7\n3063 S31884 4.0\n3064 S31884 4.0\n3065 S31884 3.7\n3066 S31884 4.0\n3067 S31884 4.0\n3068 S31884 3.3\n3069 S31884 4.0\n3070 S31884 4.0\n3071 S31884 4.0\n3072 S31884 3.7\n3073 S31887 3.7\n3074 S31887 4.0\n3075 S31887 4.0\n3076 S31887 3.3\n3077 S31887  NA\n3078 S31887 4.0\n3079 S31887 3.0\n3080 S31887 3.0\n3081 S31887 3.0\n3082 S31887 3.0\n3083 S31887 4.0\n3084 S31887 3.0\n3085 S31887 4.0\n3086 S31887 4.0\n3087 S31887 3.7\n3088 S31887  NA\n3089 S31887 3.7\n3090 S31890 3.0\n3091 S31890 3.7\n3092 S31890 3.7\n3093 S31890 4.0\n3094 S31890 3.7\n3095 S31890 4.0\n3096 S31890 3.3\n3097 S31890 4.0\n3098 S31890 4.0\n3099 S31890 4.0\n3100 S31890 4.0\n3101 S31890 4.0\n3102 S31890 4.0\n3103 S31890 4.0\n3104 S31890 4.0\n3105 S31893 3.0\n3106 S31893 3.3\n3107 S31893 3.3\n3108 S31893 3.7\n3109 S31893 4.0\n3110 S31893 2.7\n3111 S31893  NA\n3112 S31893 3.7\n3113 S31893 3.7\n3114 S31893 2.3\n3115 S31893 3.3\n3116 S31893 3.7\n3117 S31896 3.7\n3118 S31896 3.3\n3119 S31896 3.3\n3120 S31896 3.3\n3121 S31896 4.0\n3122 S31896 3.3\n3123 S31896 3.0\n3124 S31896  NA\n3125 S31896 3.0\n3126 S31896 3.3\n3127 S31896 4.0\n3128 S31896 3.7\n3129 S31896 3.3\n3130 S31896 3.7\n3131 S31896 4.0\n3132 S31899 3.3\n3133 S31899 3.3\n3134 S31899 3.7\n3135 S31899 3.3\n3136 S31899 3.0\n3137 S31899 2.3\n3138 S31899 3.3\n3139 S31899 3.7\n3140 S31899 4.0\n3141 S31899 3.3\n3142 S31899 3.3\n3143 S31902 3.3\n3144 S31902 3.0\n3145 S31902 3.7\n3146 S31902 2.0\n3147 S31902  NA\n3148 S31902  NA\n3149 S31902 3.3\n3150 S31902 2.0\n3151 S31902 1.7\n3152 S31902 3.0\n3153 S31902 2.0\n3154 S31902 3.0\n3155 S31902 3.7\n3156 S31902 3.3\n3157 S31902 3.0\n3158 S31902 3.3\n3159 S31905 3.3\n3160 S31905 4.0\n3161 S31905 3.3\n3162 S31905 4.0\n3163 S31905 4.0\n3164 S31905 4.0\n3165 S31905 4.0\n3166 S31905 4.0\n3167 S31905  NA\n3168 S31905 4.0\n3169 S31905 4.0\n3170 S31905 4.0\n3171 S31908 3.7\n3172 S31908 3.7\n3173 S31908 4.0\n3174 S31908 4.0\n3175 S31908 2.3\n3176 S31908 2.0\n3177 S31908 1.7\n3178 S31908 3.7\n3179 S31908 3.7\n3180 S31908 2.0\n3181 S31908 3.7\n3182 S31908 3.0\n3183 S31908 2.7\n3184 S31908 3.7\n3185 S31908 3.7\n3186 S31908  NA\n3187 S31911 4.0\n3188 S31911 4.0\n3189 S31911 4.0\n3190 S31911 4.0\n3191 S31911 4.0\n3192 S31911 4.0\n3193 S31911 3.7\n3194 S31911 4.0\n3195 S31911 4.0\n3196 S31911 3.7\n3197 S31911 4.0\n3198 S31914 3.7\n3199 S31914 4.0\n3200 S31914 3.7\n3201 S31914 3.7\n3202 S31914 3.3\n3203 S31914 3.3\n3204 S31914 3.0\n3205 S31914 3.0\n3206 S31914 3.7\n3207 S31914 3.0\n3208 S31914 2.7\n3209 S31914 4.0\n3210 S31914 3.7\n3211 S31914  NA\n3212 S31917 3.7\n3213 S31917 2.0\n3214 S31917 2.3\n3215 S31917 3.0\n3216 S31917 3.0\n3217 S31917  NA\n3218 S31917 3.3\n3219 S31917 3.0\n3220 S31917 2.3\n3221 S31917 3.7\n3222 S31917 3.0\n3223 S31917 2.7\n3224 S31917 3.0\n3225 S31917 3.3\n3226 S31917 3.0\n3227 S31917 4.0\n3228 S31920 1.3\n3229 S31920 4.0\n3230 S31920 2.7\n3231 S31920 1.7\n3232 S31920 3.0\n3233 S31920 2.7\n3234 S31920 3.3\n3235 S31920 0.7\n3236 S31920 4.0\n3237 S31920 3.7\n3238 S31920 3.3\n3239 S31920 0.0\n3240 S31920 2.7\n3241 S31920 1.7\n3242 S31920 3.0\n3243 S31920 2.0\n3244 S31923 3.3\n3245 S31923 3.3\n3246 S31923 3.7\n3247 S31923 4.0\n3248 S31923 3.0\n3249 S31923 3.0\n3250 S31923 4.0\n3251 S31923 3.3\n3252 S31923 4.0\n3253 S31923 4.0\n3254 S31923 4.0\n3255 S31923 3.3\n3256 S31923 4.0\n3257 S31923 3.7\n3258 S31926 3.3\n3259 S31926 3.0\n3260 S31926 3.0\n3261 S31926 2.7\n3262 S31926 3.3\n3263 S31926 2.0\n3264 S31926 3.7\n3265 S31926 1.7\n3266 S31926 3.3\n3267 S31926 3.0\n3268 S31926 3.7\n3269 S31926 4.0\n3270 S31929 3.7\n3271 S31929 3.7\n3272 S31929 3.7\n3273 S31929 3.7\n3274 S31929  NA\n3275 S31929 2.7\n3276 S31929 3.7\n3277 S31929 2.7\n3278 S31929 3.3\n3279 S31929  NA\n3280 S31929 3.3\n3281 S31929 0.0\n3282 S31929 4.0\n3283 S31929 4.0\n3284 S31929 3.3\n3285 S31932 2.7\n3286 S31932 2.3\n3287 S31932 3.3\n3288 S31932 3.3\n3289 S31932 1.7\n3290 S31932 3.0\n3291 S31932 4.0\n3292 S31932 2.7\n3293 S31932 2.7\n3294 S31932  NA\n3295 S31932 3.3\n3296 S31932 3.3\n3297 S31932 3.0\n3298 S31932 2.0\n3299 S31935 2.7\n3300 S31935 3.0\n3301 S31935 3.3\n3302 S31935 4.0\n3303 S31935 4.0\n3304 S31935 2.7\n3305 S31935 3.3\n3306 S31935 3.3\n3307 S31935  NA\n3308 S31935 3.3\n3309 S31935  NA\n3310 S31938 3.7\n3311 S31938 4.0\n3312 S31938 4.0\n3313 S31938 4.0\n3314 S31938 3.7\n3315 S31938 4.0\n3316 S31938 4.0\n3317 S31938 4.0\n3318 S31938  NA\n3319 S31941 3.7\n3320 S31941 3.3\n3321 S31941 3.7\n3322 S31941 3.7\n3323 S31941 3.3\n3324 S31941 3.7\n3325 S31941 3.3\n3326 S31941 4.0\n3327 S31941 4.0\n3328 S31941 3.3\n3329 S31941 4.0\n3330 S31944 3.7\n3331 S31944 4.0\n3332 S31944 3.0\n3333 S31944 4.0\n3334 S31944 4.0\n3335 S31944 3.7\n3336 S31944 2.0\n3337 S31944  NA\n3338 S31944 1.7\n3339 S31944 2.0\n3340 S31944 2.0\n3341 S31944 2.3\n3342 S31944 4.0\n3343 S31944 3.3\n3344 S31944  NA\n3345 S31944 3.7\n3346 S31944 4.0\n3347 S31947 3.0\n3348 S31947 4.0\n3349 S31947 4.0\n3350 S31947 4.0\n3351 S31947 4.0\n3352 S31947 3.7\n3353 S31947 3.3\n3354 S31947 3.7\n3355 S31947 3.3\n3356 S31947 4.0\n3357 S31947 3.3\n3358 S31947 3.7\n3359 S31947 4.0\n3360 S31947 3.7\n3361 S31947 4.0\n3362 S31947 4.0\n3363 S31947 4.0\n3364 S31950 3.7\n3365 S31950 3.3\n3366 S31950 3.3\n3367 S31950 4.0\n3368 S31950 3.7\n3369 S31950 4.0\n3370 S31950 3.7\n3371 S31950 3.3\n3372 S31950 4.0\n3373 S31950 3.3\n3374 S31950 4.0\n3375 S31950  NA\n3376 S31953 3.7\n3377 S31953 2.3\n3378 S31953 3.0\n3379 S31953 3.3\n3380 S31953 3.7\n3381 S31953 4.0\n3382 S31953 3.7\n3383 S31953  NA\n3384 S31953 3.7\n3385 S31953 4.0\n3386 S31953 3.3\n3387 S31956 4.0\n3388 S31956 3.7\n3389 S31956 4.0\n3390 S31956 4.0\n3391 S31956 3.7\n3392 S31956 3.7\n3393 S31956 3.7\n3394 S31956 4.0\n3395 S31956 3.7\n3396 S31956 3.3\n3397 S31956 4.0\n3398 S31956 4.0\n3399 S31956 4.0\n3400 S31956 3.7\n3401 S31959 3.7\n3402 S31959 4.0\n3403 S31959 3.3\n3404 S31959 3.7\n3405 S31959 3.7\n3406 S31959  NA\n3407 S31959 4.0\n3408 S31959 4.0\n3409 S31959  NA\n3410 S31959 2.7\n3411 S31959 3.3\n3412 S31959 4.0\n3413 S31959 4.0\n3414 S31959 3.3\n3415 S31959 3.0\n3416 S31962 3.7\n3417 S31962 4.0\n3418 S31962 4.0\n3419 S31962 3.0\n3420 S31962  NA\n3421 S31962 3.0\n3422 S31962 3.0\n3423 S31962 2.7\n3424 S31962 3.3\n3425 S31962  NA\n3426 S31962 4.0\n3427 S31962 1.7\n3428 S31962  NA\n3429 S31962 3.0\n3430 S31965 4.0\n3431 S31965 3.3\n3432 S31965 4.0\n3433 S31965 3.7\n3434 S31965 3.3\n3435 S31965 3.7\n3436 S31965 3.3\n3437 S31965 3.7\n3438 S31965 3.3\n3439 S31965 3.7\n3440 S31965 3.7\n3441 S31968 3.0\n3442 S31968 3.3\n3443 S31968 3.3\n3444 S31968 3.7\n3445 S31968 3.3\n3446 S31968 2.0\n3447 S31968 3.7\n3448 S31968 3.7\n3449 S31968 3.3\n3450 S31968 3.7\n3451 S31968 4.0\n3452 S31968 3.7\n3453 S31968 3.3\n3454 S31968 3.3\n3455 S31968 0.0\n3456 S31971 2.7\n3457 S31971 4.0\n3458 S31971 4.0\n3459 S31971 4.0\n3460 S31971 4.0\n3461 S31971 3.7\n3462 S31971 4.0\n3463 S31971  NA\n3464 S31971 3.0\n3465 S31971  NA\n3466 S31971 4.0\n3467 S31971 3.7\n3468 S31971  NA\n3469 S31974 3.7\n3470 S31974 3.0\n3471 S31974 2.7\n3472 S31974 3.7\n3473 S31974 3.3\n3474 S31974 3.7\n3475 S31974 3.0\n3476 S31974 3.0\n3477 S31974 2.7\n3478 S31974 4.0\n3479 S31974 3.0\n3480 S31974 4.0\n3481 S31974 3.0\n3482 S31974 4.0\n3483 S31974 2.7\n3484 S31974 4.0\n3485 S31977 3.7\n3486 S31977 1.7\n3487 S31977 0.7\n3488 S31977 3.3\n3489 S31977 3.0\n3490 S31977 2.7\n3491 S31977 2.3\n3492 S31977 1.7\n3493 S31977 3.7\n3494 S31977 2.3\n3495 S31977 3.0\n3496 S31977 2.7\n3497 S31977 3.3\n3498 S31980 3.7\n3499 S31980  NA\n3500 S31980 4.0\n3501 S31980 3.7\n3502 S31980 3.7\n3503 S31980 3.7\n3504 S31980 4.0\n3505 S31980 4.0\n3506 S31980 4.0\n3507 S31980  NA\n3508 S31980 4.0\n3509 S31980 3.7\n3510 S31980 4.0\n3511 S31983 4.0\n3512 S31983 4.0\n3513 S31983 4.0\n3514 S31983 4.0\n3515 S31983  NA\n3516 S31983 4.0\n3517 S31983 4.0\n3518 S31983 4.0\n3519 S31983 4.0\n3520 S31983 3.7\n3521 S31983 3.7\n3522 S31986 2.7\n3523 S31986 3.3\n3524 S31986 3.0\n3525 S31986 3.0\n3526 S31986  NA\n3527 S31986 4.0\n3528 S31986 3.7\n3529 S31986 4.0\n3530 S31986 4.0\n3531 S31986 4.0\n3532 S31986 3.3\n3533 S31986 3.0\n3534 S31986 3.7\n3535 S31989 3.3\n3536 S31989 2.0\n3537 S31989 3.3\n3538 S31989 3.7\n3539 S31989 4.0\n3540 S31989 3.3\n3541 S31989  NA\n3542 S31989 3.7\n3543 S31989 4.0\n3544 S31989 4.0\n3545 S31989 3.7\n3546 S31992 2.3\n3547 S31992 3.0\n3548 S31992 3.3\n3549 S31992 3.0\n3550 S31992 4.0\n3551 S31992 3.3\n3552 S31992 4.0\n3553 S31992 2.7\n3554 S31992 3.7\n3555 S31992 3.7\n3556 S31992 2.7\n3557 S31992  NA\n3558 S31992 3.0\n3559 S31995 4.0\n3560 S31995 4.0\n3561 S31995 4.0\n3562 S31995 4.0\n3563 S31995 4.0\n3564 S31995 3.7\n3565 S31995 4.0\n3566 S31995  NA\n3567 S31995 3.7\n3568 S31998 4.0\n3569 S31998 3.3\n3570 S31998 3.3\n3571 S31998 3.0\n3572 S31998 4.0\n3573 S31998 3.7\n3574 S31998 3.7\n3575 S31998 3.3\n3576 S31998 4.0\n3577 S31998 3.7\n3578 S31998 3.0\n3579 S31998 4.0\n3580 S31998 4.0\n3581 S32001 3.3\n3582 S32001 4.0\n3583 S32001 3.0\n3584 S32001 3.3\n3585 S32001 4.0\n3586 S32001 3.0\n3587 S32001 4.0\n3588 S32001 3.7\n3589 S32001 4.0\n3590 S32001 3.7\n3591 S32001 3.7\n3592 S32001 4.0\n3593 S32001  NA\n3594 S32004 3.0\n3595 S32004 3.7\n3596 S32004 3.7\n3597 S32004 2.7\n3598 S32004  NA\n3599 S32004 3.0\n3600 S32004 3.7\n3601 S32004 3.7\n3602 S32004 3.3\n3603 S32004 3.7\n3604 S32004 3.7\n3605 S32004 3.0\n3606 S32004 3.3\n3607 S32004 3.3\n3608 S32004 4.0\n3609 S32007 3.7\n3610 S32007 3.7\n3611 S32007 3.0\n3612 S32007 3.0\n3613 S32007 3.3\n3614 S32007 3.3\n3615 S32007 3.7\n3616 S32007 3.7\n3617 S32007 4.0\n3618 S32007 3.3\n3619 S32007 3.3\n3620 S32007 4.0\n3621 S32007 3.3\n3622 S32010 3.7\n3623 S32010 3.3\n3624 S32010 3.7\n3625 S32010 4.0\n3626 S32010 4.0\n3627 S32010 4.0\n3628 S32010 4.0\n3629 S32010 4.0\n3630 S32010 4.0\n3631 S32010  NA\n3632 S32010 4.0\n3633 S32010 3.7\n3634 S32010 4.0\n3635 S32013 3.0\n3636 S32013 4.0\n3637 S32013 4.0\n3638 S32013 3.0\n3639 S32013 3.7\n3640 S32013 3.0\n3641 S32013 4.0\n3642 S32013 3.7\n3643 S32013 3.3\n3644 S32013 3.3\n3645 S32016 3.7\n3646 S32016 4.0\n3647 S32016 3.7\n3648 S32016 4.0\n3649 S32016 3.7\n3650 S32016 3.7\n3651 S32016 4.0\n3652 S32016 3.0\n3653 S32016 4.0\n3654 S32016 3.7\n3655 S32016  NA\n3656 S32016  NA\n3657 S32016 3.3\n3658 S32016 3.3\n3659 S32019 1.7\n3660 S32019  NA\n3661 S32019 0.0\n3662 S32019 3.0\n3663 S32019 2.7\n3664 S32019 3.0\n3665 S32019  NA\n3666 S32019 3.3\n3667 S32019 2.3\n3668 S32019 3.7\n3669 S32019 3.0\n3670 S32019 4.0\n3671 S32019 3.0\n3672 S32019 2.7\n3673 S32019 2.7\n3674 S32019 3.7\n3675 S32022 1.7\n3676 S32022 2.3\n3677 S32022 1.0\n3678 S32022 0.7\n3679 S32022 3.3\n3680 S32022 2.3\n3681 S32022 3.0\n3682 S32022 3.3\n3683 S32022 2.7\n3684 S32022 3.3\n3685 S32022 2.7\n3686 S32022 2.7\n3687 S32022 3.7\n3688 S32022 4.0\n3689 S32025 3.3\n3690 S32025 3.3\n3691 S32025 3.7\n3692 S32025 3.7\n3693 S32025 3.7\n3694 S32025 3.7\n3695 S32025 3.3\n3696 S32025 4.0\n3697 S32025  NA\n3698 S32025 3.3\n3699 S32025 3.3\n3700 S32025 4.0\n3701 S32028 3.0\n3702 S32028 3.3\n3703 S32028 3.3\n3704 S32028 3.3\n3705 S32028  NA\n3706 S32028 3.0\n3707 S32028 4.0\n3708 S32028 4.0\n3709 S32028 4.0\n3710 S32028 4.0\n3711 S32028  NA\n3712 S32031 4.0\n3713 S32031 4.0\n3714 S32031 4.0\n3715 S32031 3.7\n3716 S32031 4.0\n3717 S32031 4.0\n3718 S32031 3.7\n3719 S32031 3.0\n3720 S32031 4.0\n3721 S32031 3.7\n3722 S32031  NA\n3723 S32031 3.7\n3724 S32031 3.3\n3725 S32034 2.3\n3726 S32034 3.0\n3727 S32034 2.7\n3728 S32034 3.0\n3729 S32034 2.7\n3730 S32034 3.7\n3731 S32034 4.0\n3732 S32034 2.7\n3733 S32034 3.7\n3734 S32034 3.0\n3735 S32034 2.3\n3736 S32034 3.3\n3737 S32034 2.3\n3738 S32034 3.7\n3739 S32034 3.3\n3740 S32034 3.3\n3741 S32034 3.7\n3742 S32037 3.3\n3743 S32037 4.0\n3744 S32037 3.0\n3745 S32037 3.0\n3746 S32037 3.0\n3747 S32037 2.3\n3748 S32037 2.7\n3749 S32037 4.0\n3750 S32037 3.0\n3751 S32037 3.3\n3752 S32037 4.0\n3753 S32037 3.3\n3754 S32037 2.7\n3755 S32037  NA\n3756 S32037 4.0\n3757 S32037 3.3\n3758 S32040 3.7\n3759 S32040 3.3\n3760 S32040 3.0\n3761 S32040 4.0\n3762 S32040 3.3\n3763 S32040 3.0\n3764 S32040  NA\n3765 S32040 3.7\n3766 S32040 3.3\n3767 S32040 3.7\n3768 S32040 4.0\n3769 S32040 4.0\n3770 S32043 4.0\n3771 S32043 3.7\n3772 S32043 4.0\n3773 S32043 3.3\n3774 S32043 4.0\n3775 S32043 4.0\n3776 S32043 4.0\n3777 S32043 4.0\n3778 S32043 3.7\n3779 S32043 3.3\n3780 S32043 3.0\n3781 S32043  NA\n3782 S32046 2.3\n3783 S32046 2.0\n3784 S32046 3.0\n3785 S32046 2.3\n3786 S32046 2.7\n3787 S32046 3.7\n3788 S32046 2.0\n3789 S32046 1.7\n3790 S32046 3.7\n3791 S32046 3.3\n3792 S32046 3.0\n3793 S32046 4.0\n3794 S32046 3.7\n3795 S32046 3.3\n3796 S32046 3.3\n3797 S32046 4.0\n3798 S32049 3.3\n3799 S32049 3.0\n3800 S32049 3.0\n3801 S32049 3.0\n3802 S32049 3.7\n3803 S32049 3.7\n3804 S32049 3.3\n3805 S32049 3.3\n3806 S32049 3.7\n3807 S32049 3.3\n3808 S32049 4.0\n3809 S32049 3.3\n3810 S32049 3.0\n3811 S32052 4.0\n3812 S32052 4.0\n3813 S32052 3.7\n3814 S32052 4.0\n3815 S32052 4.0\n3816 S32052 4.0\n3817 S32052 4.0\n3818 S32052 4.0\n3819 S32052 4.0\n3820 S32052 3.7\n3821 S32052 3.7\n3822 S32055 4.0\n3823 S32055 4.0\n3824 S32055 4.0\n3825 S32055 4.0\n3826 S32055 4.0\n3827 S32055 4.0\n3828 S32055 4.0\n3829 S32055 4.0\n3830 S32055 4.0\n3831 S32055 3.3\n3832 S32055 3.7\n3833 S32055  NA\n3834 S32055 3.3\n3835 S32055 4.0\n3836 S32055 4.0\n3837 S32055 4.0\n3838 S32058 4.0\n3839 S32058 4.0\n3840 S32058 3.7\n3841 S32058 3.7\n3842 S32058 3.3\n3843 S32058 4.0\n3844 S32058 4.0\n3845 S32058 3.7\n3846 S32058 4.0\n3847 S32058 3.3\n3848 S32058 3.7\n3849 S32058 3.7\n3850 S32061 3.0\n3851 S32061 3.0\n3852 S32061 3.3\n3853 S32061 3.7\n3854 S32061 2.3\n3855 S32061 4.0\n3856 S32061 4.0\n3857 S32061 4.0\n3858 S32061 3.7\n3859 S32061 3.7\n3860 S32061 4.0\n3861 S32061  NA\n3862 S32061 3.3\n3863 S32061 3.7\n3864 S32061 4.0\n3865 S32061 3.3\n3866 S32061 4.0\n3867 S32064 3.7\n3868 S32064 4.0\n3869 S32064 3.7\n3870 S32064 3.7\n3871 S32064 3.7\n3872 S32064 3.3\n3873 S32064 3.3\n3874 S32064 3.7\n3875 S32064 4.0\n3876 S32064  NA\n3877 S32064 3.7\n3878 S32064  NA\n3879 S32064 4.0\n3880 S32064 3.7\n3881 S32067 3.0\n3882 S32067 3.7\n3883 S32067 3.0\n3884 S32067 3.0\n3885 S32067 3.7\n3886 S32067 2.0\n3887 S32067 2.3\n3888 S32067 2.7\n3889 S32067 3.0\n3890 S32067 3.0\n3891 S32067 3.7\n3892 S32067 3.3\n3893 S32067 3.3\n3894 S32067 3.7\n3895 S32070 3.3\n3896 S32070 4.0\n3897 S32070 3.7\n3898 S32070 4.0\n3899 S32070 3.3\n3900 S32070 4.0\n3901 S32070 3.3\n3902 S32070 3.7\n3903 S32070 4.0\n3904 S32070 3.7\n3905 S32070 3.7\n3906 S32073 3.0\n3907 S32073 3.7\n3908 S32073 3.7\n3909 S32073 4.0\n3910 S32073 3.7\n3911 S32073 3.7\n3912 S32073 4.0\n3913 S32073 3.7\n3914 S32073 3.7\n3915 S32073 3.3\n3916 S32073 3.7\n3917 S32073  NA\n3918 S32073 3.7\n3919 S32076  NA\n3920 S32076 3.3\n3921 S32076 3.0\n3922 S32076 2.0\n3923 S32076 3.7\n3924 S32076 2.3\n3925 S32076 0.0\n3926 S32076 3.0\n3927 S32076 1.7\n3928 S32076 1.0\n3929 S32079 4.0\n3930 S32079 3.7\n3931 S32079 3.7\n3932 S32079 3.7\n3933 S32079 4.0\n3934 S32079  NA\n3935 S32079 4.0\n3936 S32079 3.7\n3937 S32079 3.3\n3938 S32079 2.7\n3939 S32079  NA\n3940 S32082 3.7\n3941 S32082 3.7\n3942 S32082 3.3\n3943 S32082 2.7\n3944 S32082 3.0\n3945 S32082 2.7\n3946 S32082 3.3\n3947 S32082 2.3\n3948 S32082 3.7\n3949 S32082 3.0\n3950 S32082 2.0\n3951 S32082 2.7\n3952 S32082 3.3\n3953 S32082 4.0\n3954 S32085 3.0\n3955 S32085 2.3\n3956 S32085 2.7\n3957 S32085 3.7\n3958 S32085 3.3\n3959 S32085 3.3\n3960 S32085  NA\n3961 S32085 4.0\n3962 S32085 2.7\n3963 S32085 3.7\n3964 S32085 2.7\n3965 S32085 2.7\n3966 S32085 3.3\n3967 S32088 2.7\n3968 S32088 2.7\n3969 S32088 3.0\n3970 S32088 2.7\n3971 S32088 4.0\n3972 S32088 3.7\n3973 S32088 4.0\n3974 S32088 3.7\n3975 S32088 4.0\n3976 S32088 4.0\n3977 S32088 4.0\n3978 S32088 3.7\n3979 S32088 3.7\n3980 S32088  NA\n3981 S32088 3.0\n3982 S32088  NA\n3983 S32091 3.7\n3984 S32091 3.3\n3985 S32091 2.0\n3986 S32091 4.0\n3987 S32091 2.3\n3988 S32091 2.0\n3989 S32091 3.3\n3990 S32091 3.7\n3991 S32091 2.3\n3992 S32091 2.3\n3993 S32091 1.0\n3994 S32091 2.7\n3995 S32091 2.7\n3996 S32094 4.0\n3997 S32094 3.7\n3998 S32094 2.7\n3999 S32094 3.3\n4000 S32094 4.0\n4001 S32094 4.0\n4002 S32094 3.7\n4003 S32094 3.7\n4004 S32094 4.0\n4005 S32094  NA\n4006 S32094  NA\n4007 S32097 2.0\n4008 S32097 2.7\n4009 S32097 3.0\n4010 S32097 4.0\n4011 S32097 2.0\n4012 S32097 3.3\n4013 S32097 3.0\n4014 S32097 3.0\n4015 S32097 3.0\n4016 S32097 3.0\n4017 S32097  NA\n4018 S32097 3.7\n4019 S32097  NA\n4020 S32097 3.0\n4021 S32097 3.3\n4022 S32100 2.7\n4023 S32100 3.3\n4024 S32100 2.7\n4025 S32100 3.3\n4026 S32100 3.0\n4027 S32100 4.0\n4028 S32100 4.0\n4029 S32100  NA\n4030 S32100 3.0\n4031 S32100 4.0\n4032 S32100 3.3\n4033 S32100 3.0\n4034 S32100 3.7\n4035 S32100 4.0\n4036 S32100  NA\n4037 S32100 3.0\n4038 S32103 2.3\n4039 S32103 2.0\n4040 S32103 3.0\n4041 S32103 4.0\n4042 S32103 3.7\n4043 S32103 3.3\n4044 S32103 3.3\n4045 S32103 3.3\n4046 S32103 3.3\n4047 S32103 4.0\n4048 S32103 3.0\n4049 S32103 3.7\n4050 S32103 4.0\n4051 S32103 4.0\n4052 S32103  NA\n4053 S32106 3.7\n4054 S32106 4.0\n4055 S32106 2.3\n4056 S32106 2.7\n4057 S32106 3.0\n4058 S32106 2.7\n4059 S32106 2.3\n4060 S32106 2.7\n4061 S32106 3.3\n4062 S32106 3.3\n4063 S32106 3.3\n4064 S32106 3.7\n4065 S32106 3.0\n4066 S32106 4.0\n4067 S32106 3.3\n4068 S32109 2.7\n4069 S32109 2.7\n4070 S32109 3.0\n4071 S32109 3.3\n4072 S32109 2.3\n4073 S32109 2.0\n4074 S32109 4.0\n4075 S32109 3.3\n4076 S32109 3.0\n4077 S32109  NA\n4078 S32109 4.0\n4079 S32109 2.7\n4080 S32112 2.3\n4081 S32112  NA\n4082 S32112 3.0\n4083 S32112 3.0\n4084 S32112 3.0\n4085 S32112 3.0\n4086 S32112 3.0\n4087 S32112 3.3\n4088 S32112 3.0\n4089 S32112 3.3\n4090 S32112 3.7\n4091 S32112 2.3\n4092 S32112  NA\n4093 S32115 2.0\n4094 S32115  NA\n4095 S32115 4.0\n4096 S32115  NA\n4097 S32115 3.7\n4098 S32115 3.7\n4099 S32115 3.0\n4100 S32115  NA\n4101 S32115 3.0\n4102 S32115 3.7\n4103 S32115 3.7\n4104 S32115 4.0\n4105 S32115 3.7\n4106 S32115 3.0\n4107 S32118 3.3\n4108 S32118 4.0\n4109 S32118 4.0\n4110 S32118 3.0\n4111 S32118 3.7\n4112 S32118 3.7\n4113 S32118 4.0\n4114 S32118 3.7\n4115 S32118 3.7\n4116 S32118 3.7\n4117 S32118 4.0\n4118 S32121 3.3\n4119 S32121 3.7\n4120 S32121 3.7\n4121 S32121 4.0\n4122 S32121  NA\n4123 S32121 3.7\n4124 S32121 3.0\n4125 S32121 4.0\n4126 S32121 4.0\n4127 S32121 3.3\n4128 S32121 3.7\n4129 S32121 3.7\n4130 S32124 2.3\n4131 S32124 3.0\n4132 S32124 1.3\n4133 S32124 3.0\n4134 S32124 2.0\n4135 S32124 3.0\n4136 S32124 3.3\n4137 S32124 3.0\n4138 S32124 2.3\n4139 S32124 2.7\n4140 S32124 3.7\n4141 S32124 4.0\n4142 S32124 4.0\n4143 S32124 2.3\n4144 S32127 4.0\n4145 S32127 4.0\n4146 S32127 3.0\n4147 S32127  NA\n4148 S32127 4.0\n4149 S32127 3.7\n4150 S32127 4.0\n4151 S32127 3.7\n4152 S32127 3.0\n4153 S32127 3.7\n4154 S32127 3.0\n4155 S32127 2.0\n4156 S32127 2.7\n4157 S32127 4.0\n4158 S32130 3.0\n4159 S32130 3.0\n4160 S32130 2.3\n4161 S32130 3.0\n4162 S32130 3.3\n4163 S32130 3.7\n4164 S32130 2.7\n4165 S32130 3.7\n4166 S32130 3.7\n4167 S32130 3.3\n4168 S32130 3.3\n4169 S32130 4.0\n4170 S32133 4.0\n4171 S32133 4.0\n4172 S32133 3.7\n4173 S32133 3.0\n4174 S32133 3.7\n4175 S32133 4.0\n4176 S32133 3.3\n4177 S32133 4.0\n4178 S32133 4.0\n4179 S32133 3.3\n4180 S32133 4.0\n4181 S32133 4.0\n4182 S32133 3.7\n4183 S32133 3.7\n4184 S32133 3.7\n4185 S32136 3.7\n4186 S32136 1.7\n4187 S32136 3.7\n4188 S32136 2.7\n4189 S32136 3.0\n4190 S32136 4.0\n4191 S32136 4.0\n4192 S32136 3.3\n4193 S32136 4.0\n4194 S32136 3.7\n4195 S32136 3.7\n4196 S32136 4.0\n4197 S32136 3.0\n4198 S32139 3.3\n4199 S32139 3.0\n4200 S32139 3.7\n4201 S32139 3.7\n4202 S32139 4.0\n4203 S32139 2.7\n4204 S32139 2.3\n4205 S32139  NA\n4206 S32139 3.7\n4207 S32139 4.0\n4208 S32139 4.0\n4209 S32139  NA\n4210 S32139  NA\n4211 S32139 4.0\n4212 S32139 4.0\n4213 S32142 4.0\n4214 S32142 2.3\n4215 S32142 3.7\n4216 S32142 2.7\n4217 S32142 3.7\n4218 S32142 4.0\n4219 S32142 3.0\n4220 S32142 3.0\n4221 S32142  NA\n4222 S32142 3.0\n4223 S32142 4.0\n4224 S32142 3.7\n4225 S32142 3.7\n4226 S32145 3.3\n4227 S32145 3.0\n4228 S32145 3.3\n4229 S32145 3.7\n4230 S32145 3.3\n4231 S32145 2.7\n4232 S32145 3.7\n4233 S32145 3.3\n4234 S32145 4.0\n4235 S32145 3.3\n4236 S32145  NA\n4237 S32145 3.3\n4238 S32145 3.3\n4239 S32145 3.7\n4240 S32148 3.0\n4241 S32148 3.3\n4242 S32148 4.0\n4243 S32148 3.3\n4244 S32148 3.0\n4245 S32148 3.0\n4246 S32148  NA\n4247 S32148 4.0\n4248 S32148 3.7\n4249 S32148 3.3\n4250 S32148 3.3\n4251 S32148 3.7\n4252 S32148  NA\n4253 S32151 3.0\n4254 S32151 3.0\n4255 S32151 3.3\n4256 S32151 3.7\n4257 S32151 3.3\n4258 S32151 3.0\n4259 S32151 3.3\n4260 S32151 3.3\n4261 S32151 4.0\n4262 S32151 3.0\n4263 S32151 4.0\n4264 S32151 3.7\n4265 S32151 2.7\n4266 S32151 4.0\n4267 S32154 3.3\n4268 S32154 2.0\n4269 S32154 3.0\n4270 S32154 2.7\n4271 S32154 2.7\n4272 S32154 4.0\n4273 S32154 2.0\n4274 S32154 3.3\n4275 S32154 3.0\n4276 S32154 3.3\n4277 S32154 3.3\n4278 S32154 2.3\n4279 S32154 2.0\n4280 S32154 3.7\n4281 S32154 3.3\n4282 S32157 3.3\n4283 S32157 3.7\n4284 S32157 3.7\n4285 S32157 4.0\n4286 S32157 4.0\n4287 S32157 3.0\n4288 S32157 3.0\n4289 S32157 3.7\n4290 S32157 4.0\n4291 S32157 4.0\n4292 S32157 4.0\n4293 S32157 4.0\n4294 S32157 4.0\n4295 S32160 3.7\n4296 S32160 3.0\n4297 S32160 3.0\n4298 S32160 4.0\n4299 S32160 3.0\n4300 S32160 3.3\n4301 S32160 3.3\n4302 S32160 3.7\n4303 S32160 3.3\n4304 S32160 3.0\n4305 S32160 3.7\n4306 S32160 3.7\n4307 S32160 4.0\n4308 S32163 3.3\n4309 S32163 3.7\n4310 S32163 3.0\n4311 S32163 3.3\n4312 S32163 4.0\n4313 S32163 4.0\n4314 S32163 4.0\n4315 S32163 4.0\n4316 S32163 4.0\n4317 S32163 4.0\n4318 S32163 3.7\n4319 S32163 4.0\n4320 S32163 3.3\n4321 S32163 2.7\n4322 S32166 4.0\n4323 S32166 3.7\n4324 S32166 4.0\n4325 S32166 4.0\n4326 S32166 3.7\n4327 S32166 3.7\n4328 S32166 4.0\n4329 S32166 3.0\n4330 S32166 3.3\n4331 S32166 3.7\n4332 S32166 3.7\n4333 S32166 4.0\n4334 S32169 3.7\n4335 S32169 3.0\n4336 S32169 4.0\n4337 S32169 3.3\n4338 S32169 3.3\n4339 S32169 4.0\n4340 S32169 3.0\n4341 S32169  NA\n4342 S32169  NA\n4343 S32169  NA\n4344 S32169  NA\n4345 S32169  NA\n4346 S32172 3.7\n4347 S32172 3.7\n4348 S32172 3.3\n4349 S32172 4.0\n4350 S32172 3.0\n4351 S32172 4.0\n4352 S32172 4.0\n4353 S32172 4.0\n4354 S32172 3.7\n4355 S32172 3.3\n4356 S32172 4.0\n4357 S32172 3.7\n4358 S32172 3.3\n4359 S32172 3.7\n4360 S32172 4.0\n4361 S32175 4.0\n4362 S32175 3.3\n4363 S32175 4.0\n4364 S32175 3.7\n4365 S32175 3.7\n4366 S32175 3.7\n4367 S32175 4.0\n4368 S32175 4.0\n4369 S32175 3.0\n4370 S32175 4.0\n4371 S32175 3.7\n4372 S32175 4.0\n4373 S32175 3.3\n4374 S32175 3.7\n4375 S32178 2.7\n4376 S32178 1.7\n4377 S32178 2.3\n4378 S32178 1.7\n4379 S32178 1.7\n4380 S32178 2.0\n4381 S32178 1.0\n4382 S32178 3.0\n4383 S32178 2.0\n4384 S32178 2.7\n4385 S32178 2.0\n4386 S32178 3.0\n4387 S32178 3.0\n4388 S32178 2.7\n4389 S32181 2.7\n4390 S32181 3.7\n4391 S32181 4.0\n4392 S32181 3.7\n4393 S32181 3.3\n4394 S32181 4.0\n4395 S32181 3.7\n4396 S32181  NA\n4397 S32181 4.0\n4398 S32181 4.0\n4399 S32181 3.7\n4400 S32181 4.0\n4401 S32181 4.0\n4402 S32181  NA\n4403 S32181 3.7\n4404 S32181  NA\n4405 S32184 4.0\n4406 S32184 3.7\n4407 S32184 4.0\n4408 S32184 3.7\n4409 S32184 4.0\n4410 S32184 3.3\n4411 S32184 3.7\n4412 S32184 3.3\n4413 S32184 3.7\n4414 S32184 3.7\n4415 S32184 3.7\n4416 S32184 3.7\n4417 S32187 3.3\n4418 S32187 3.3\n4419 S32187 4.0\n4420 S32187 3.3\n4421 S32187 3.3\n4422 S32187 3.7\n4423 S32187 3.3\n4424 S32187 3.0\n4425 S32187 3.3\n4426 S32187 3.0\n4427 S32187 3.7\n4428 S32187  NA\n4429 S32190 3.3\n4430 S32190 3.7\n4431 S32190 2.3\n4432 S32190 4.0\n4433 S32190 2.7\n4434 S32190 3.7\n4435 S32190 3.3\n4436 S32190 4.0\n4437 S32190 3.0\n4438 S32190 2.7\n4439 S32190 3.7\n4440 S32190 4.0\n4441 S32190 3.0\n4442 S32190 4.0\n4443 S32190 3.0\n4444 S32190 4.0\n4445 S32190 3.3\n4446 S32193 3.3\n4447 S32193 2.7\n4448 S32193 2.0\n4449 S32193 3.7\n4450 S32193 2.0\n4451 S32193 3.0\n4452 S32193 3.0\n4453 S32193 2.0\n4454 S32193 1.0\n4455 S32193 3.0\n4456 S32193 2.7\n4457 S32193 3.0\n4458 S32193 4.0\n4459 S32193 3.3\n4460 S32196 2.7\n4461 S32196 2.7\n4462 S32196 3.7\n4463 S32196 3.7\n4464 S32196 3.0\n4465 S32196 3.0\n4466 S32196 3.7\n4467 S32196 3.3\n4468 S32196 3.7\n4469 S32196  NA\n4470 S32196 2.7\n4471 S32196 3.7\n4472 S32196  NA\n4473 S32199 3.3\n4474 S32199 3.0\n4475 S32199 3.3\n4476 S32199 3.0\n4477 S32199 3.3\n4478 S32199 4.0\n4479 S32199 4.0\n4480 S32199 3.7\n4481 S32199 4.0\n4482 S32199  NA\n4483 S32199 3.7\n4484 S32199 3.7\n4485 S32199 3.7\n4486 S32199 3.7\n4487 S32202 3.7\n4488 S32202 3.3\n4489 S32202 4.0\n4490 S32202 3.7\n4491 S32202 3.7\n4492 S32202 3.3\n4493 S32202 3.7\n4494 S32202 3.0\n4495 S32202 4.0\n4496 S32202 4.0\n4497 S32202 4.0\n4498 S32205 4.0\n4499 S32205 3.7\n4500 S32205 4.0\n4501 S32205 3.7\n4502 S32205 4.0\n4503 S32205 3.7\n4504 S32205 3.7\n4505 S32205 3.3\n4506 S32205  NA\n4507 S32205 3.3\n4508 S32205 3.7\n4509 S32205 2.7\n4510 S32205 3.3\n4511 S32205 3.7\n4512 S32208 3.7\n4513 S32208  NA\n4514 S32208 3.7\n4515 S32208 4.0\n4516 S32208 4.0\n4517 S32208 2.0\n4518 S32208 3.0\n4519 S32208 3.7\n4520 S32208 4.0\n4521 S32208 4.0\n4522 S32208 3.0\n4523 S32211 3.0\n4524 S32211 3.3\n4525 S32211 3.7\n4526 S32211 2.3\n4527 S32211 4.0\n4528 S32211 3.0\n4529 S32211 3.0\n4530 S32211 3.3\n4531 S32211 3.7\n4532 S32211 3.7\n4533 S32211 3.0\n4534 S32211 2.7\n4535 S32211 3.0\n4536 S32211 3.3\n4537 S32211 3.7\n4538 S32214 3.0\n4539 S32214  NA\n4540 S32214 1.7\n4541 S32214 3.3\n4542 S32214 2.7\n4543 S32214 3.0\n4544 S32214 1.7\n4545 S32214 3.0\n4546 S32214 3.0\n4547 S32214 3.3\n4548 S32214 3.7\n4549 S32214 1.7\n4550 S32214 2.0\n4551 S32214 3.0\n4552 S32217 4.0\n4553 S32217 4.0\n4554 S32217 3.7\n4555 S32217 4.0\n4556 S32217 3.0\n4557 S32217 3.0\n4558 S32217 3.7\n4559 S32217  NA\n4560 S32217 3.7\n4561 S32217 3.3\n4562 S32217 3.7\n4563 S32217 4.0\n4564 S32217 3.7\n4565 S32217  NA\n4566 S32220 3.3\n4567 S32220 3.0\n4568 S32220 3.3\n4569 S32220 4.0\n4570 S32220 3.7\n4571 S32220 3.7\n4572 S32220 3.7\n4573 S32220 4.0\n4574 S32220 2.7\n4575 S32220 4.0\n4576 S32220 4.0\n4577 S32223 3.0\n4578 S32223 4.0\n4579 S32223 3.7\n4580 S32223 3.7\n4581 S32223 3.3\n4582 S32223  NA\n4583 S32223 3.3\n4584 S32223 3.7\n4585 S32223 3.7\n4586 S32223 3.7\n4587 S32223 3.0\n4588 S32226 3.7\n4589 S32226 4.0\n4590 S32226 4.0\n4591 S32226 4.0\n4592 S32226 4.0\n4593 S32226 4.0\n4594 S32226 4.0\n4595 S32226 3.7\n4596 S32226 4.0\n4597 S32226 3.7\n4598 S32226  NA\n4599 S32229 3.3\n4600 S32229 3.0\n4601 S32229 3.0\n4602 S32229 3.0\n4603 S32229 3.7\n4604 S32229 3.3\n4605 S32229 4.0\n4606 S32229 3.7\n4607 S32229 3.3\n4608 S32229 4.0\n4609 S32229 3.7\n4610 S32229 3.7\n4611 S32229 4.0\n4612 S32232 3.0\n4613 S32232 3.7\n4614 S32232 3.0\n4615 S32232 3.0\n4616 S32232 4.0\n4617 S32232 4.0\n4618 S32232 4.0\n4619 S32232 4.0\n4620 S32232 4.0\n4621 S32232 4.0\n4622 S32232 3.0\n4623 S32232 4.0\n4624 S32232 3.7\n4625 S32235 4.0\n4626 S32235 3.0\n4627 S32235 3.7\n4628 S32235 4.0\n4629 S32235 3.0\n4630 S32235 4.0\n4631 S32235 3.7\n4632 S32235 3.7\n4633 S32235 3.0\n4634 S32235 3.7\n4635 S32235  NA\n4636 S32235 3.0\n4637 S32238 3.7\n4638 S32238 3.0\n4639 S32238 3.3\n4640 S32238 4.0\n4641 S32238 4.0\n4642 S32238 3.3\n4643 S32238 4.0\n4644 S32238  NA\n4645 S32238 4.0\n4646 S32238 3.7\n4647 S32238 4.0\n4648 S32238 3.0\n4649 S32238 3.3\n4650 S32238 3.7\n4651 S32241 3.0\n4652 S32241 4.0\n4653 S32241 4.0\n4654 S32241 4.0\n4655 S32241 4.0\n4656 S32241  NA\n4657 S32241 3.3\n4658 S32241 3.7\n4659 S32241 4.0\n4660 S32241 3.0\n4661 S32241 3.7\n4662 S32241 3.7\n4663 S32241 4.0\n4664 S32244 2.3\n4665 S32244 3.7\n4666 S32244 2.7\n4667 S32244 3.0\n4668 S32244 3.0\n4669 S32244 3.0\n4670 S32244 3.7\n4671 S32244 2.0\n4672 S32244 3.0\n4673 S32244 3.7\n4674 S32244  NA\n4675 S32244 4.0\n4676 S32244 3.7\n4677 S32244 3.3\n4678 S32244 3.7\n4679 S32247 3.0\n4680 S32247 3.7\n4681 S32247 3.3\n4682 S32247 3.0\n4683 S32247 3.7\n4684 S32247 2.3\n4685 S32247 4.0\n4686 S32247 3.0\n4687 S32247 4.0\n4688 S32250 4.0\n4689 S32250 3.3\n4690 S32250 2.7\n4691 S32250 4.0\n4692 S32250 3.3\n4693 S32250 4.0\n4694 S32250 3.7\n4695 S32250 3.0\n4696 S32250 2.0\n4697 S32250 2.0\n4698 S32250 2.0\n4699 S32250 2.7\n4700 S32250 3.7\n4701 S32250 3.3\n4702 S32250 3.0\n4703 S32253 3.0\n4704 S32253 4.0\n4705 S32253 4.0\n4706 S32253 4.0\n4707 S32253 4.0\n4708 S32253 3.7\n4709 S32253 4.0\n4710 S32253 4.0\n4711 S32253 4.0\n4712 S32253 3.7\n4713 S32253 4.0\n4714 S32253 4.0\n4715 S32253  NA\n4716 S32256 4.0\n4717 S32256 3.3\n4718 S32256 3.7\n4719 S32256 3.7\n4720 S32256 3.0\n4721 S32256 3.0\n4722 S32256  NA\n4723 S32256 3.7\n4724 S32256 3.3\n4725 S32256 3.0\n4726 S32256 3.3\n4727 S32256 4.0\n4728 S32256 4.0\n4729 S32256 3.7\n4730 S32256 3.3\n4731 S32256 4.0\n4732 S32259 2.7\n4733 S32259 2.3\n4734 S32259 3.0\n4735 S32259 1.7\n4736 S32259 4.0\n4737 S32259 3.0\n4738 S32259 3.3\n4739 S32259 3.7\n4740 S32259 3.7\n4741 S32259 3.7\n4742 S32259 3.3\n4743 S32259 3.7\n4744 S32259 3.3\n4745 S32259 4.0\n4746 S32259 3.7\n4747 S32262 3.3\n4748 S32262 4.0\n4749 S32262 2.0\n4750 S32262 2.7\n4751 S32262 3.7\n4752 S32262 3.0\n4753 S32262 3.0\n4754 S32262 4.0\n4755 S32262  NA\n4756 S32262 4.0\n4757 S32262 3.7\n4758 S32262 4.0\n4759 S32262 4.0\n4760 S32265 2.7\n4761 S32265 3.0\n4762 S32265 3.7\n4763 S32265 3.0\n4764 S32265 3.3\n4765 S32265 2.7\n4766 S32265 3.0\n4767 S32265 2.7\n4768 S32265 2.3\n4769 S32265 3.7\n4770 S32265 4.0\n4771 S32265 3.0\n4772 S32265  NA\n4773 S32265 4.0\n4774 S32268 3.7\n4775 S32268 3.7\n4776 S32268 3.3\n4777 S32268 3.3\n4778 S32268 3.7\n4779 S32268 3.3\n4780 S32268 4.0\n4781 S32268 4.0\n4782 S32268 2.7\n4783 S32268 3.7\n4784 S32268 3.0\n4785 S32268 3.3\n4786 S32268 1.7\n4787 S32268 3.7\n4788 S32271 2.0\n4789 S32271 1.3\n4790 S32271 3.0\n4791 S32271 3.0\n4792 S32271 3.3\n4793 S32271 3.3\n4794 S32271 3.0\n4795 S32271 4.0\n4796 S32271 3.0\n4797 S32271 3.3\n4798 S32274 3.7\n4799 S32274 2.0\n4800 S32274 4.0\n4801 S32274 4.0\n4802 S32274 3.7\n4803 S32274 4.0\n4804 S32274 3.7\n4805 S32274 2.7\n4806 S32274 3.7\n4807 S32274 3.7\n4808 S32274 4.0\n4809 S32274 3.0\n4810 S32274 3.7\n4811 S32274 4.0\n4812 S32277 3.3\n4813 S32277 3.3\n4814 S32277 3.0\n4815 S32277  NA\n4816 S32277 2.3\n4817 S32277 3.3\n4818 S32277 3.3\n4819 S32277 3.0\n4820 S32277 3.7\n4821 S32277 2.7\n4822 S32277 2.7\n4823 S32277 2.7\n4824 S32277 2.7\n4825 S32277 3.7\n4826 S32280 3.7\n4827 S32280 3.7\n4828 S32280 3.0\n4829 S32280 4.0\n4830 S32280 3.7\n4831 S32280 4.0\n4832 S32280  NA\n4833 S32280 3.3\n4834 S32280 3.3\n4835 S32280 3.7\n4836 S32280 4.0\n4837 S32280 3.3\n4838 S32283 3.3\n4839 S32283 3.3\n4840 S32283 3.3\n4841 S32283  NA\n4842 S32283  NA\n4843 S32283 3.3\n4844 S32283 3.3\n4845 S32283  NA\n4846 S32283 3.3\n4847 S32283 2.3\n4848 S32283 3.3\n4849 S32283 4.0\n4850 S32283  NA\n4851 S32283  NA\n4852 S32283 3.0\n4853 S32283 4.0\n4854 S32286 1.7\n4855 S32286 2.7\n4856 S32286 1.0\n4857 S32286 1.7\n4858 S32286 3.7\n4859 S32286 4.0\n4860 S32286 2.7\n4861 S32286 2.0\n4862 S32286 4.0\n4863 S32286 2.3\n4864 S32286 3.7\n4865 S32286 4.0\n4866 S32286 3.3\n4867 S32286 3.0\n4868 S32289 3.7\n4869 S32289 4.0\n4870 S32289 4.0\n4871 S32289 4.0\n4872 S32289 3.7\n4873 S32289 3.7\n4874 S32289  NA\n4875 S32289 4.0\n4876 S32289 3.7\n4877 S32289 3.7\n4878 S32289 4.0\n4879 S32289 4.0\n4880 S32289 4.0\n4881 S32289 3.3\n4882 S32292 2.7\n4883 S32292 3.0\n4884 S32292 3.0\n4885 S32292 2.3\n4886 S32292 3.3\n4887 S32292 3.3\n4888 S32292 4.0\n4889 S32292 3.3\n4890 S32292 3.3\n4891 S32292 2.3\n4892 S32292 3.7\n4893 S32292 3.0\n4894 S32295 3.3\n4895 S32295 2.7\n4896 S32295 2.3\n4897 S32295 3.0\n4898 S32295 2.0\n4899 S32295 2.3\n4900 S32295 3.0\n4901 S32295  NA\n4902 S32295  NA\n4903 S32295 3.3\n4904 S32295  NA\n4905 S32295 1.7\n4906 S32295 2.0\n4907 S32295 3.0\n4908 S32295  NA\n4909 S32298 3.0\n4910 S32298 2.7\n4911 S32298 3.3\n4912 S32298 3.3\n4913 S32298 4.0\n4914 S32298  NA\n4915 S32298  NA\n4916 S32298 3.3\n4917 S32298 2.3\n4918 S32298 3.0\n4919 S32298 2.3\n4920 S32298 3.3\n4921 S32298 2.7\n4922 S32298 3.3\n4923 S32298 4.0\n4924 S32298  NA\n4925 S32301 3.7\n4926 S32301 3.3\n4927 S32301 3.7\n4928 S32301 3.7\n4929 S32301  NA\n4930 S32301 3.7\n4931 S32301 3.3\n4932 S32301 3.3\n4933 S32301 3.7\n4934 S32301 3.7\n4935 S32301 4.0\n4936 S32301 3.0\n4937 S32301  NA\n4938 S32301 3.3\n4939 S32304 3.7\n4940 S32304 4.0\n4941 S32304 3.3\n4942 S32304 4.0\n4943 S32304 4.0\n4944 S32304 4.0\n4945 S32304  NA\n4946 S32304 4.0\n4947 S32304 3.7\n4948 S32304 3.7\n4949 S32304 3.7\n4950 S32304 3.7\n4951 S32304 3.3\n4952 S32304 3.7\n4953 S32304  NA\n4954 S32307 3.3\n4955 S32307 4.0\n4956 S32307 4.0\n4957 S32307 3.0\n4958 S32307 3.0\n4959 S32307 3.0\n4960 S32307 4.0\n4961 S32307 3.0\n4962 S32307 3.3\n4963 S32307 4.0\n4964 S32307 2.7\n4965 S32307 2.0\n4966 S32307 2.7\n4967 S32307 3.7\n4968 S32310 3.7\n4969 S32310 3.3\n4970 S32310 4.0\n4971 S32310 3.3\n4972 S32310 3.3\n4973 S32310 3.7\n4974 S32310 3.3\n4975 S32310 3.0\n4976 S32310 4.0\n4977 S32310 4.0\n4978 S32310 4.0\n4979 S32310 4.0\n4980 S32313 3.7\n4981 S32313 3.3\n4982 S32313 4.0\n4983 S32313 4.0\n4984 S32313 4.0\n4985 S32313 3.0\n4986 S32313  NA\n4987 S32313 3.3\n4988 S32313 3.3\n4989 S32313 3.7\n4990 S32313 3.3\n4991 S32313 3.7\n4992 S32316 3.0\n4993 S32316 3.3\n4994 S32316 3.0\n4995 S32316 3.0\n4996 S32316 3.3\n4997 S32316 3.3\n4998 S32316 3.3\n4999 S32316 3.7\n5000 S32316 3.0\n5001 S32316 3.3\n5002 S32316 2.7\n5003 S32316 3.7\n5004 S32319 0.0\n5005 S32319 4.0\n5006 S32319 4.0\n5007 S32319 3.0\n5008 S32319  NA\n5009 S32319 1.7\n5010 S32319 2.3\n5011 S32319 3.7\n5012 S32319 3.3\n5013 S32319 3.7\n5014 S32319 1.7\n5015 S32319 3.0\n5016 S32319 2.0\n5017 S32319 0.7\n5018 S32319 2.0\n5019 S32322 3.7\n5020 S32322 3.7\n5021 S32322 3.3\n5022 S32322 3.3\n5023 S32322 3.0\n5024 S32322 3.7\n5025 S32322 3.7\n5026 S32322 4.0\n5027 S32322 3.7\n5028 S32322 4.0\n5029 S32322 3.7\n5030 S32325 3.0\n5031 S32325 3.3\n5032 S32325 3.7\n5033 S32325 4.0\n5034 S32325  NA\n5035 S32325 3.7\n5036 S32325 3.3\n5037 S32325 3.7\n5038 S32328 3.7\n5039 S32328 4.0\n5040 S32328 3.3\n5041 S32328 3.7\n5042 S32328 3.7\n5043 S32328 3.7\n5044 S32328 3.3\n5045 S32328 3.0\n5046 S32328 3.7\n5047 S32328 3.3\n5048 S32328 3.0\n5049 S32328 3.7\n5050 S32328 3.7\n5051 S32331 2.0\n5052 S32331 2.7\n5053 S32331 2.0\n5054 S32331 3.0\n5055 S32331 2.7\n5056 S32331 3.0\n5057 S32331 3.7\n5058 S32331  NA\n5059 S32331  NA\n5060 S32331 3.0\n5061 S32331 3.0\n5062 S32331 3.3\n5063 S32331 3.7\n5064 S32334 3.3\n5065 S32334 3.3\n5066 S32334 3.0\n5067 S32334 3.7\n5068 S32334 3.0\n5069 S32334 3.3\n5070 S32334 3.3\n5071 S32334 3.7\n5072 S32334 3.3\n5073 S32334 3.0\n5074 S32334 3.7\n5075 S32334 3.3\n5076 S32334  NA\n5077 S32337 4.0\n5078 S32337 3.7\n5079 S32337 2.0\n5080 S32337 2.0\n5081 S32337 3.0\n5082 S32337 2.7\n5083 S32337 4.0\n5084 S32337 4.0\n5085 S32337 3.3\n5086 S32337 3.7\n5087 S32337 4.0\n5088 S32340 3.3\n5089 S32340 3.7\n5090 S32340 3.3\n5091 S32340 4.0\n5092 S32340 3.0\n5093 S32340 3.7\n5094 S32340 4.0\n5095 S32340 4.0\n5096 S32340  NA\n5097 S32340 4.0\n5098 S32340 4.0\n5099 S32340 4.0\n5100 S32340  NA\n5101 S32340 4.0\n5102 S32343 3.7\n5103 S32343 4.0\n5104 S32343 3.0\n5105 S32343 3.7\n5106 S32343 3.3\n5107 S32343 3.3\n5108 S32343 4.0\n5109 S32343  NA\n5110 S32343 4.0\n5111 S32343  NA\n5112 S32343 3.3\n5113 S32343 3.7\n5114 S32343  NA\n5115 S32343 3.7\n5116 S32346 3.7\n5117 S32346 3.0\n5118 S32346 3.7\n5119 S32346  NA\n5120 S32346 4.0\n5121 S32346 4.0\n5122 S32346 3.3\n5123 S32346 4.0\n5124 S32346 4.0\n5125 S32346  NA\n5126 S32346 3.3\n5127 S32346 4.0\n5128 S32346 3.7\n5129 S32346 4.0\n5130 S32346 4.0\n5131 S32346 3.7\n5132 S32349 3.7\n5133 S32349 4.0\n5134 S32349 4.0\n5135 S32349 4.0\n5136 S32349 3.7\n5137 S32349 3.7\n5138 S32349 2.7\n5139 S32349 3.3\n5140 S32349 3.0\n5141 S32349 2.7\n5142 S32349 1.7\n5143 S32349 3.3\n5144 S32349 3.7\n5145 S32349 3.7\n5146 S32349  NA\n5147 S32352 3.7\n5148 S32352 1.7\n5149 S32352 3.0\n5150 S32352 3.0\n5151 S32352 3.0\n5152 S32352 3.7\n5153 S32352 4.0\n5154 S32352 3.0\n5155 S32352 3.7\n5156 S32352 3.3\n5157 S32352 3.3\n5158 S32352 3.7\n5159 S32352 4.0\n5160 S32352 4.0\n5161 S32355 3.3\n5162 S32355 3.0\n5163 S32355 3.3\n5164 S32355 4.0\n5165 S32355 2.3\n5166 S32355  NA\n5167 S32355 4.0\n5168 S32355 4.0\n5169 S32355 4.0\n5170 S32355 4.0\n5171 S32355 4.0\n5172 S32355 4.0\n5173 S32355 3.7\n5174 S32355 4.0\n5175 S32358 4.0\n5176 S32358 3.7\n5177 S32358 4.0\n5178 S32358 4.0\n5179 S32358 4.0\n5180 S32358 4.0\n5181 S32358 4.0\n5182 S32358 4.0\n5183 S32358 4.0\n5184 S32358 4.0\n5185 S32358 3.7\n5186 S32361 2.0\n5187 S32361 2.7\n5188 S32361 3.7\n5189 S32361 2.7\n5190 S32361 1.0\n5191 S32361 4.0\n5192 S32361 1.7\n5193 S32361 3.7\n5194 S32361 4.0\n5195 S32361 3.7\n5196 S32361 4.0\n5197 S32361 4.0\n5198 S32361 3.3\n5199 S32361 4.0\n5200 S32364 3.0\n5201 S32364 4.0\n5202 S32364 4.0\n5203 S32364 4.0\n5204 S32364 4.0\n5205 S32364 4.0\n5206 S32364 4.0\n5207 S32364 3.7\n5208 S32364 4.0\n5209 S32364 4.0\n5210 S32364 4.0\n5211 S32364 3.7\n5212 S32364 4.0\n5213 S32364 3.7\n5214 S32364 3.0\n5215 S32367 3.7\n5216 S32367 4.0\n5217 S32367 3.3\n5218 S32367 3.0\n5219 S32367 3.3\n5220 S32367 3.3\n5221 S32367 3.7\n5222 S32367 2.7\n5223 S32367 3.3\n5224 S32367 3.7\n5225 S32367 4.0\n5226 S32367 4.0\n5227 S32367 4.0\n5228 S32367 3.7\n5229 S32367 3.7\n5230 S32367 4.0\n5231 S32367  NA\n5232 S32370 1.7\n5233 S32370 3.3\n5234 S32370 3.7\n5235 S32370 4.0\n5236 S32370 3.7\n5237 S32370 4.0\n5238 S32370  NA\n5239 S32370 4.0\n5240 S32370 4.0\n5241 S32370 3.7\n5242 S32370  NA\n5243 S32370 4.0\n5244 S32373 3.3\n5245 S32373 2.0\n5246 S32373 4.0\n5247 S32373 4.0\n5248 S32373 4.0\n5249 S32373 3.7\n5250 S32373 4.0\n5251 S32373 4.0\n5252 S32373 3.7\n5253 S32373 4.0\n5254 S32373 4.0\n5255 S32373 4.0\n5256 S32373 3.7\n5257 S32373 3.0\n5258 S32376 3.3\n5259 S32376 2.0\n5260 S32376 4.0\n5261 S32376 4.0\n5262 S32376 4.0\n5263 S32376 3.0\n5264 S32376 4.0\n5265 S32376 4.0\n5266 S32376 3.0\n5267 S32376 3.0\n5268 S32376 2.3\n5269 S32376 1.3\n5270 S32376 3.7\n5271 S32376 3.7\n5272 S32376 4.0\n5273 S32379 4.0\n5274 S32379 3.7\n5275 S32379 4.0\n5276 S32379 4.0\n5277 S32379 3.7\n5278 S32379 4.0\n5279 S32379 4.0\n5280 S32379 4.0\n5281 S32379 4.0\n5282 S32379  NA\n5283 S32379 4.0\n5284 S32382 4.0\n5285 S32382 3.0\n5286 S32382 2.7\n5287 S32382 3.7\n5288 S32382 3.7\n5289 S32382 2.7\n5290 S32382 3.3\n5291 S32382 4.0\n5292 S32382 3.0\n5293 S32382 2.3\n5294 S32382 2.7\n5295 S32382 4.0\n5296 S32382 3.0\n5297 S32382 3.7\n5298 S32382 3.7\n5299 S32385 3.7\n5300 S32385 2.7\n5301 S32385 3.0\n5302 S32385 3.7\n5303 S32385 3.0\n5304 S32385 3.7\n5305 S32385 3.7\n5306 S32385 3.0\n5307 S32385 3.3\n5308 S32385 4.0\n5309 S32385 3.7\n5310 S32385 3.7\n5311 S32388 4.0\n5312 S32388 4.0\n5313 S32388 3.7\n5314 S32388 3.0\n5315 S32388 3.7\n5316 S32388 3.7\n5317 S32388 4.0\n5318 S32388 4.0\n5319 S32388 4.0\n5320 S32388 3.7\n5321 S32388 3.7\n5322 S32388 3.3\n5323 S32388 3.0\n5324 S32391 3.7\n5325 S32391 4.0\n5326 S32391 3.7\n5327 S32391 4.0\n5328 S32391 4.0\n5329 S32391 3.7\n5330 S32391 4.0\n5331 S32391  NA\n5332 S32391  NA\n5333 S32394 3.7\n5334 S32394 3.7\n5335 S32394 3.7\n5336 S32394 4.0\n5337 S32394 3.3\n5338 S32394 3.7\n5339 S32394 4.0\n5340 S32394 3.7\n5341 S32394 3.7\n5342 S32394 4.0\n5343 S32394 4.0\n5344 S32394  NA\n5345 S32394  NA\n5346 S32394 3.3\n5347 S32397 3.3\n5348 S32397 4.0\n5349 S32397 3.3\n5350 S32397 2.3\n5351 S32397 3.0\n5352 S32397 3.3\n5353 S32397 2.7\n5354 S32397  NA\n5355 S32397 3.7\n5356 S32397 3.3\n5357 S32397 3.3\n5358 S32397 2.0\n5359 S32397 3.7\n5360 S32400 4.0\n5361 S32400 2.7\n5362 S32400 2.7\n5363 S32400 3.0\n5364 S32400 3.3\n5365 S32400 3.0\n5366 S32400 2.0\n5367 S32400  NA\n5368 S32400 3.7\n5369 S32400 3.7\n5370 S32400 3.3\n5371 S32400 2.3\n5372 S32400 3.7\n5373 S32400 3.7\n5374 S32403 2.7\n5375 S32403 3.0\n5376 S32403 3.7\n5377 S32403 4.0\n5378 S32403 3.3\n5379 S32403 3.7\n5380 S32403 4.0\n5381 S32403 4.0\n5382 S32403 3.7\n5383 S32403 4.0\n5384 S32403 4.0\n5385 S32403 3.0\n5386 S32403 4.0\n5387 S32403 3.3\n5388 S32403 3.7\n5389 S32403 4.0\n5390 S32403 3.3\n5391 S32406 3.0\n5392 S32406 2.3\n5393 S32406 2.3\n5394 S32406 2.7\n5395 S32406 3.3\n5396 S32406 2.3\n5397 S32406 3.0\n5398 S32406 3.3\n5399 S32406 3.3\n5400 S32406 3.7\n5401 S32406 3.7\n5402 S32406 4.0\n5403 S32406 3.0\n5404 S32406 3.7\n5405 S32406 4.0\n5406 S32409 3.0\n5407 S32409 3.0\n5408 S32409 3.0\n5409 S32409 3.0\n5410 S32409 2.7\n5411 S32409 3.0\n5412 S32409 3.7\n5413 S32409 1.0\n5414 S32409 3.7\n5415 S32409 3.7\n5416 S32409 3.3\n5417 S32409 3.7\n5418 S32409 4.0\n5419 S32409 3.0\n5420 S32409 2.7\n5421 S32409 3.7\n5422 S32412 3.7\n5423 S32412 4.0\n5424 S32412 3.7\n5425 S32412 4.0\n5426 S32412 3.7\n5427 S32412 3.7\n5428 S32412 3.0\n5429 S32412 4.0\n5430 S32412 3.3\n5431 S32412  NA\n5432 S32412 3.0\n5433 S32412 3.7\n5434 S32412 4.0\n5435 S32415 2.0\n5436 S32415 3.7\n5437 S32415 3.3\n5438 S32415 2.3\n5439 S32415 2.3\n5440 S32415 3.0\n5441 S32415  NA\n5442 S32415 4.0\n5443 S32415 4.0\n5444 S32415 3.0\n5445 S32415 3.3\n5446 S32415 3.7\n5447 S32415 3.0\n5448 S32415 3.3\n5449 S32415 3.3\n5450 S32415 3.3\n5451 S32418 3.0\n5452 S32418 3.7\n5453 S32418 3.3\n5454 S32418 2.0\n5455 S32418 3.0\n5456 S32418 3.0\n5457 S32418 3.0\n5458 S32418 3.0\n5459 S32418 3.0\n5460 S32418 3.0\n5461 S32418 3.3\n5462 S32418 3.3\n5463 S32418 3.3\n5464 S32421 3.7\n5465 S32421 3.7\n5466 S32421 3.7\n5467 S32421 3.7\n5468 S32421 3.0\n5469 S32421 2.0\n5470 S32421  NA\n5471 S32421 2.0\n5472 S32421 2.7\n5473 S32421 2.7\n5474 S32421 3.7\n5475 S32421 3.3\n5476 S32424 3.3\n5477 S32424 2.7\n5478 S32424 3.0\n5479 S32424 2.7\n5480 S32424 3.3\n5481 S32424 3.7\n5482 S32424  NA\n5483 S32424 3.3\n5484 S32424 3.7\n5485 S32424 3.7\n5486 S32424  NA\n5487 S32424 3.3\n5488 S32424 3.7\n5489 S32424 2.0\n5490 S32424 4.0\n5491 S32427 3.0\n5492 S32427 2.3\n5493 S32427 3.0\n5494 S32427 3.7\n5495 S32427  NA\n5496 S32427 3.3\n5497 S32427 3.0\n5498 S32427 1.0\n5499 S32427 3.0\n5500 S32427 3.0\n5501 S32427 2.7\n5502 S32427  NA\n5503 S32427 2.7\n5504 S32427 4.0\n5505 S32427 2.7\n5506 S32427 2.7\n5507 S32427 4.0\n5508 S32427 4.0\n5509 S32430 2.3\n5510 S32430 3.0\n5511 S32430 3.0\n5512 S32430 3.3\n5513 S32430 4.0\n5514 S32430 4.0\n5515 S32430 3.3\n5516 S32430 3.3\n5517 S32430  NA\n5518 S32430 4.0\n5519 S32430 3.3\n5520 S32430 3.7\n5521 S32430 3.7\n5522 S32430 4.0\n5523 S32430 3.7\n5524 S32433 3.3\n5525 S32433 3.0\n5526 S32433 3.0\n5527 S32433 3.0\n5528 S32433  NA\n5529 S32433 3.0\n5530 S32433 2.0\n5531 S32433 3.0\n5532 S32433  NA\n5533 S32433 3.0\n5534 S32433 3.7\n5535 S32433 3.0\n5536 S32433 2.0\n5537 S32433 2.0\n5538 S32433 2.7\n5539 S32433 3.0\n5540 S32436 3.7\n5541 S32436 4.0\n5542 S32436 2.3\n5543 S32436 4.0\n5544 S32436 4.0\n5545 S32436 3.3\n5546 S32436 4.0\n5547 S32436 4.0\n5548 S32436 3.3\n5549 S32436 4.0\n5550 S32436  NA\n5551 S32436 3.0\n5552 S32436 3.3\n5553 S32436 4.0\n5554 S32436 3.7\n5555 S32439 2.7\n5556 S32439 3.7\n5557 S32439 3.3\n5558 S32439 3.7\n5559 S32439 4.0\n5560 S32439  NA\n5561 S32439 4.0\n5562 S32439 4.0\n5563 S32439 4.0\n5564 S32439 3.7\n5565 S32439 4.0\n5566 S32442  NA\n5567 S32442 2.3\n5568 S32442 3.7\n5569 S32442 4.0\n5570 S32442 2.3\n5571 S32442 3.3\n5572 S32442 4.0\n5573 S32442 3.0\n5574 S32442 2.7\n5575 S32442 3.0\n5576 S32442 3.7\n5577 S32442 3.7\n5578 S32442 4.0\n5579 S32442 3.3\n5580 S32442 3.7\n5581 S32445 3.7\n5582 S32445 4.0\n5583 S32445 3.7\n5584 S32445 2.3\n5585 S32445 4.0\n5586 S32445 4.0\n5587 S32445 2.7\n5588 S32445 4.0\n5589 S32445  NA\n5590 S32445 4.0\n5591 S32445  NA\n5592 S32445  NA\n5593 S32445  NA\n5594 S32445 4.0\n5595 S32448 2.7\n5596 S32448 2.3\n5597 S32448 3.7\n5598 S32448 3.0\n5599 S32448 3.7\n5600 S32448 4.0\n5601 S32448 4.0\n5602 S32448 3.0\n5603 S32448 4.0\n5604 S32448 4.0\n5605 S32448 3.0\n5606 S32448 3.3\n5607 S32448 3.7\n5608 S32451 1.7\n5609 S32451 3.3\n5610 S32451  NA\n5611 S32451 2.7\n5612 S32451 2.7\n5613 S32451 4.0\n5614 S32451 3.0\n5615 S32451  NA\n5616 S32451  NA\n5617 S32451 3.3\n5618 S32451 3.0\n5619 S32451 3.7\n5620 S32451 4.0\n5621 S32451 4.0\n5622 S32451 3.7\n5623 S32454 4.0\n5624 S32454 2.7\n5625 S32454 4.0\n5626 S32454 3.3\n5627 S32454 3.3\n5628 S32454 4.0\n5629 S32454  NA\n5630 S32454 4.0\n5631 S32454 4.0\n5632 S32454 4.0\n5633 S32454 4.0\n5634 S32454 2.7\n5635 S32454  NA\n5636 S32457 4.0\n5637 S32457 3.7\n5638 S32457 3.7\n5639 S32457 4.0\n5640 S32457 3.7\n5641 S32457 3.7\n5642 S32457 4.0\n5643 S32457 3.3\n5644 S32457 4.0\n5645 S32457 4.0\n5646 S32457 3.7\n5647 S32460 2.3\n5648 S32460 4.0\n5649 S32460 4.0\n5650 S32460 3.7\n5651 S32460 3.3\n5652 S32460 3.7\n5653 S32460 3.7\n5654 S32460 3.7\n5655 S32460 3.0\n5656 S32460 2.3\n5657 S32460 3.0\n5658 S32460 3.3\n5659 S32460  NA\n5660 S32463 3.0\n5661 S32463  NA\n5662 S32463 4.0\n5663 S32463 3.3\n5664 S32463 3.0\n5665 S32463 2.7\n5666 S32463 3.0\n5667 S32463 3.3\n5668 S32463 3.0\n5669 S32463 3.0\n5670 S32463 3.7\n5671 S32463 3.3\n5672 S32466 4.0\n5673 S32466 4.0\n5674 S32466 3.7\n5675 S32466 3.3\n5676 S32466 3.0\n5677 S32466 3.0\n5678 S32466 3.7\n5679 S32466 4.0\n5680 S32466 4.0\n5681 S32466 3.7\n5682 S32466 3.3\n5683 S32466 3.7\n5684 S32466 3.3\n5685 S32466 4.0\n5686 S32466 4.0\n5687 S32466  NA\n5688 S32469 3.7\n5689 S32469 3.3\n5690 S32469 3.7\n5691 S32469 3.7\n5692 S32469 3.0\n5693 S32469 3.3\n5694 S32469 2.7\n5695 S32469 3.7\n5696 S32469 3.0\n5697 S32469 3.3\n5698 S32469 3.3\n5699 S32469 3.3\n5700 S32469 4.0\n5701 S32469 3.7\n5702 S32469 4.0\n5703 S32472 4.0\n5704 S32472 3.7\n5705 S32472 4.0\n5706 S32472 4.0\n5707 S32472 4.0\n5708 S32472 2.7\n5709 S32472 4.0\n5710 S32472 3.3\n5711 S32472 4.0\n5712 S32472 3.7\n5713 S32472 4.0\n5714 S32472 4.0\n5715 S32472 4.0\n5716 S32475 3.0\n5717 S32475 3.7\n5718 S32475 2.0\n5719 S32475 3.3\n5720 S32475 4.0\n5721 S32475 2.3\n5722 S32475  NA\n5723 S32475 3.7\n5724 S32475 4.0\n5725 S32475 4.0\n5726 S32475 3.7\n5727 S32475  NA\n5728 S32475 3.0\n5729 S32475 4.0\n5730 S32475 4.0\n5731 S32475 3.7\n5732 S32475 3.0\n5733 S32478 2.3\n5734 S32478 3.3\n5735 S32478 3.0\n5736 S32478 2.3\n5737 S32478 3.7\n5738 S32478 3.0\n5739 S32478 3.0\n5740 S32478  NA\n5741 S32478  NA\n5742 S32478 3.7\n5743 S32478 3.3\n5744 S32478 3.7\n5745 S32478 3.7\n5746 S32478 3.7\n5747 S32478 3.7\n5748 S32481 3.7\n5749 S32481 3.3\n5750 S32481 3.0\n5751 S32481 4.0\n5752 S32481 4.0\n5753 S32481 3.7\n5754 S32481 3.7\n5755 S32481 4.0\n5756 S32481 4.0\n5757 S32481 3.7\n5758 S32481 3.7\n5759 S32481 4.0\n5760 S32481 3.7\n5761 S32481 3.7\n5762 S32484 4.0\n5763 S32484 3.3\n5764 S32484 2.7\n5765 S32484 4.0\n5766 S32484  NA\n5767 S32484 3.7\n5768 S32484 4.0\n5769 S32484 3.3\n5770 S32484 4.0\n5771 S32484 3.7\n5772 S32484 2.3\n5773 S32487 3.7\n5774 S32487 3.7\n5775 S32487 3.7\n5776 S32487 4.0\n5777 S32487 3.7\n5778 S32487 4.0\n5779 S32490 3.7\n5780 S32490 4.0\n5781 S32490 3.3\n5782 S32490 3.3\n5783 S32490 3.7\n5784 S32490 3.7\n5785 S32490  NA\n5786 S32490 4.0\n5787 S32493 3.3\n5788 S32493 3.3\n5789 S32493 3.0\n5790 S32493  NA\n5791 S32493 4.0\n5792 S32493 3.0\n5793 S32493 4.0\n5794 S32496 3.7\n5795 S32496 3.7\n5796 S32496 3.3\n5797 S32496 3.0\n5798 S32496 4.0\n5799 S32496  NA\n5800 S32496  NA\n5801 S32496 3.0\n5802 S32499 3.0\n5803 S32499 3.7\n5804 S32499 3.0\n5805 S32499 3.7\n5806 S32499 3.7\n5807 S32499  NA\n5808 S32499 2.7\n5809 S32499 3.7\n5810 S32502 2.3\n5811 S32502 2.3\n5812 S32502 3.3\n5813 S32502 4.0\n5814 S32502 4.0\n5815 S32502 3.3\n5816 S32502 3.3\n5817 S32502  NA\n5818 S32502 3.3\n5819 S32502  NA\n5820 S32505 4.0\n5821 S32505 4.0\n5822 S32505 4.0\n5823 S32505 4.0\n5824 S32505 4.0\n5825 S32505 3.7\n5826 S32505 3.7\n5827 S32505 4.0\n5828 S32505 4.0\n5829 S32505 4.0\n5830 S32508 4.0\n5831 S32508 4.0\n5832 S32508 4.0\n5833 S32508 3.7\n5834 S32508  NA\n5835 S32508 4.0\n5836 S32508 3.7\n5837 S32508 4.0\n5838 S32508 3.7\n5839 S32511 3.3\n5840 S32511 3.7\n5841 S32511 3.0\n5842 S32511 2.3\n5843 S32511 3.0\n5844 S32511 3.7\n5845   &lt;NA&gt; 4.3\n\n\n\n\nPart c\nWhat’s the median GPA across all students?\n\n\nPart d\nWhat fraction of grades are below B+?\n\n\nPart e\nWhat’s the grade-point average for each instructor? Order from low to high.\n\n\nPart f\nCHALLENGE: Estimate the grade-point average for each department, and sort from low to high. NOTE: Don’t include cross-listed courses. Students in cross-listed courses could be enrolled under either department, and we do not know which department to assign the grade to. HINT: You’ll need to do multiple joins.\n\n\n\n\n\n\n\n\nExercise 7: HOMEWORK PRACTICE\nThis exercise is on Homework 4, thus no solutions are provided. In Homework 4, you’ll be working with the Birthdays data:\n\nlibrary(mosaic)\ndata(\"Birthdays\")\nhead(Birthdays)\n\n  state year month day       date wday births\n1    AK 1969     1   1 1969-01-01  Wed     14\n2    AL 1969     1   1 1969-01-01  Wed    174\n3    AR 1969     1   1 1969-01-01  Wed     78\n4    AZ 1969     1   1 1969-01-01  Wed     84\n5    CA 1969     1   1 1969-01-01  Wed    824\n6    CO 1969     1   1 1969-01-01  Wed    100\n\n\nYou’ll also be exploring how the number of daily births is (or isn’t!) related to holidays. To this end, import data on U.S. federal holidays here. NOTE: lubridate::dmy() converts the character-string date stored in the CSV to a “POSIX” date-number.\n\nholidays &lt;- read.csv(\"https://mac-stat.github.io/data/US-Holidays.csv\") |&gt;\n  mutate(date = as.POSIXct(lubridate::dmy(date)))\n\n\nPart a\nCreate a new dataset, daily_births_1980, which:\n\nkeeps only daily_births related to 1980\nadds a variable called is_holiday which is TRUE when the day is a holiday, and FALSE otherwise. NOTE: !is.na(x) is TRUE if column x is not NA, and FALSE if it is NA.\n\nPrint out the first 6 rows and confirm that your dataset has 366 rows (1 per day in 1980) and 7 columns. HINT: You’ll need to combine 2 different datasets.\n\n# Define daily_births_1980\n\n\n# Check out the first 6 rows\n\n\n# Confirm that daily_births_1980 has 366 rows and 7 columns\n\n\n\nPart b\nPlot the total number of babies born (y-axis) per day (x-axis) in 1980. Color each date according to its day of the week, and shape each date according to whether or not it’s a holiday. (This is a modified version of 3c!)\n\n\nPart c\nDiscuss your observations. For example: To what degree does the theory that there tend to be fewer births on holidays hold up? What holidays stand out the most?\n\n\nPart d (OPTIONAL)\nSome holidays stand out more than others. It would be helpful to label them. Use geom_text to add labels to each of the holidays. NOTE: You can set the orientation of a label with the angle argument; e.g., geom_text(angle = 40, ...).\n\n\n\n\n\n\n\n\nNext steps\nIf you finish this all during class, you’re expected to work on Homework 4. If you’re done with Homework 4, you’re expected to play around with more TidyTuesday data. Mainly, and naturally, you’re expected to spend 112 class time on 112 :)",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "activities/activity-11.html#solutions",
    "href": "activities/activity-11.html#solutions",
    "title": "9  Joining Data",
    "section": "9.3 Solutions",
    "text": "9.3 Solutions\n\n\nClick for Solutions\n\nEXAMPLE 1\n\nclass\na student that took ANTH 101\ndata on ART 101\n\n\n\n\nEXAMPLE 2\n\nWhat did this do? Linked course info to all students in students_1\nWhich observations from students_1 (the left table) were retained? All of them.\nWhich observations from enrollments_1 (the right table) were retained? Only STAT and GEOL, those that matched the students.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. We retain the courses, not students.\n\n\nenrollments_1 |&gt; \n  left_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2  ART 101         17    &lt;NA&gt;\n3 GEOL 101         24       B\n\n\n\n\n\n\n\nEXAMPLE 3\n\nWhich observations from students_1 (the left table) were retained? A and B, only those with enrollment info.\nWhich observations from enrollments_1 (the right table) were retained? STAT and GEOL, only those with studen info.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. Same info, different column order.\n\n\nenrollments_1 |&gt; \n    inner_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2 GEOL 101         24       B\n\n\n\n\n\n\n\nEXAMPLE 4\n\nWhich observations from students_1 (the left table) were retained? All\nWhich observations from enrollments_1 (the right table) were retained? All\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. Same data, different order.\n\n\nenrollments_1 |&gt; \n    full_join(students_1)\n\n     class enrollment student\n1 STAT 101         18       A\n2  ART 101         17    &lt;NA&gt;\n3 GEOL 101         24       B\n4 ANTH 101         NA       C\n\n\n\n\n\n\n\nEXAMPLE 5\n\nWhich observations from students_1 (the left table) were retained? Only those with enrollment info.\nWhich observations from enrollments_1 (the right table) were retained? None.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. Same data, different order.\n\n\nenrollments_1 |&gt; \n  semi_join(students_1)\n\n     class enrollment\n1 STAT 101         18\n2 GEOL 101         24\n\n\n\n\n\n\n\nEXAMPLE 6\n\nWhich observations from students_1 (the left table) were retained? Only C, the one without enrollment info.\nWhich observations from enrollments_1 (the right table) were retained? None.\nWhat, if anything, would change if we reversed the order of the data tables? Think about it, then try. Retain only ART 101, the course with no student info.\n\n\nenrollments_1 |&gt; \n  anti_join(students_1)\n\n    class enrollment\n1 ART 101         17\n\n\n\n\n\n\n\n\nExercise 2: More small practice\n\n# 1. We want contact info for people who HAVEN'T voted\ncontact |&gt; \n  anti_join(voters, by = c(\"name\" = \"id\"))\n\n  name  address age\n1    B    grand  89\n2    C snelling  43\n\n# 2. We want contact info for people who HAVE voted\ncontact |&gt; \n  semi_join(voters, by = c(\"name\" = \"id\"))\n\n  name  address age\n1    A   summit  24\n2    D fairview  38\n\n# 3. We want any data available on each person\ncontact |&gt; \n  full_join(voters, by = c(\"name\" = \"id\"))\n\n  name  address age times_voted\n1    A   summit  24           2\n2    B    grand  89          NA\n3    C snelling  43          NA\n4    D fairview  38           4\n5    E     &lt;NA&gt;  NA          17\n6    F     &lt;NA&gt;  NA           6\n7    G     &lt;NA&gt;  NA          20\n\nvoters |&gt; \n  full_join(contact, by = c(\"id\" = \"name\"))\n\n  id times_voted  address age\n1  A           2   summit  24\n2  D           4 fairview  38\n3  E          17     &lt;NA&gt;  NA\n4  F           6     &lt;NA&gt;  NA\n5  G          20     &lt;NA&gt;  NA\n6  B          NA    grand  89\n7  C          NA snelling  43\n\n# 4. We want to add contact info, when possible, to the voting roster\nvoters |&gt; \n  left_join(contact, by = c(\"id\" = \"name\"))\n\n  id times_voted  address age\n1  A           2   summit  24\n2  D           4 fairview  38\n3  E          17     &lt;NA&gt;  NA\n4  F           6     &lt;NA&gt;  NA\n5  G          20     &lt;NA&gt;  NA\n\n\n\n\n\n\n\n\n\nExercise 3: Bigger datasets\n\n# How many observations (rows) and variables (columns) are there in the grades data?\ndim(grades)\n\n[1] 5844    3\n\n# How many observations (rows) and variables (columns) are there in the courses data?\ndim(courses)\n\n[1] 1718    6\n\n\n\n\n\n\n\n\n\nExercise 4: Class size\n\nPart a\n\ncourses_combined &lt;- courses |&gt;\n  group_by(sessionID) |&gt;\n  summarize(enroll = sum(enroll))\n\n# Check that this has 1695 rows and 2 columns\ndim(courses_combined)\n\n[1] 1695    2\n\n\n\n\nPart b\n\ncourses_combined |&gt; \n  summarize(median(enroll))\n\n\n\nPart c\n\nstudent_class_size &lt;- grades |&gt; \n  left_join(courses_combined) |&gt; \n  group_by(sid) |&gt; \n  summarize(med_class = median(enroll))\n\nhead(student_class_size)\n\n\n\nPart d\n\nggplot(student_class_size, aes(x = med_class)) +\n  geom_histogram(color = \"white\")\n\n\n\n\n\n\n\n\n\nExercise 5: Narrowing in on classes\n\nPart a\n\ngrades |&gt; \n  filter(sessionID == \"session1986\")\n\n\n\nPart b\n\ngrades |&gt; \n  semi_join(dept_E)\n\n\n\n\n\n\n\n\n\nExercise 6: All the wrangling\n\nPart a\n\ncourses |&gt; \n  group_by(dept) |&gt; \n  summarize(total = sum(enroll)) |&gt; \n  arrange(desc(total))\n\n\n\nPart b\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  group_by(sid) |&gt; \n  summarize(mean(gp, na.rm = TRUE))\n\n\n\nPart c\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  group_by(sid) |&gt; \n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; \n  summarize(median(gpa))\n\n\n\nPart d\n\n# There are lots of approaches here!\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  mutate(below_b_plus = (gp &lt; 3.3)) |&gt; \n  summarize(mean(below_b_plus, na.rm = TRUE))\n\n\n\nPart e\n\ngrades |&gt; \n  left_join(gpa_conversion) |&gt; \n  left_join(courses) |&gt; \n  group_by(iid) |&gt; \n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; \n  arrange(gpa)\n\n\n\nPart f\n\ncross_listed &lt;- courses |&gt; \n  count(sessionID) |&gt; \n  filter(n &gt; 1)\n\ngrades |&gt; \n  anti_join(cross_listed) |&gt; \n  inner_join(courses) |&gt; \n  left_join(gpa_conversion) |&gt; \n  group_by(dept) |&gt; \n  summarize(gpa = mean(gp, na.rm = TRUE)) |&gt; \n  arrange(gpa)",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "activities/activity-11.html#footnotes",
    "href": "activities/activity-11.html#footnotes",
    "title": "9  Joining Data",
    "section": "",
    "text": "There is also a right_join() that adds variables in the reverse direction from the left table to the right table, but we do not really need it as we can always switch the roles of the two tables.︎↩︎",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Joining Data</span>"
    ]
  },
  {
    "objectID": "activities/activity-12.html",
    "href": "activities/activity-12.html",
    "title": "10  Factors",
    "section": "",
    "text": "10.1 Warm-up\nWhere are we? Data preparation\nThus far, we’ve learned how to:\nWhat next?\nIn the remaining days of our data preparation unit, we’ll focus on working with special types of “categorical” variables: characters and factors. Variables with these structures often require special tools and considerations.\nWe’ll focus on two common considerations:\nsessionID dept level    sem enroll     iid\n1 session1784    M   100 FA1991     22 inst265\n2 session1785    k   100 FA1991     52 inst458\n3 session1791    J   100 FA1993     22 inst223\n4 session1792    J   300 FA1993     20 inst235\n5 session1794    J   200 FA1993     22 inst234\n6 session1795    J   200 SP1994     26 inst230\n\n\n'data.frame':   1718 obs. of  6 variables:\n $ sessionID: chr  \"session1784\" \"session1785\" \"session1791\" \"session1792\" ...\n $ dept     : chr  \"M\" \"k\" \"J\" \"J\" ...\n $ level    : int  100 100 100 300 200 200 200 100 300 100 ...\n $ sem      : chr  \"FA1991\" \"FA1991\" \"FA1993\" \"FA1993\" ...\n $ enroll   : int  22 52 22 20 22 26 25 38 16 43 ...\n $ iid      : chr  \"inst265\" \"inst458\" \"inst223\" \"inst235\" ...\nFocusing on just the sem character variable, we might want to…\nEXAMPLE 1\nRecall our data on presidential election outcomes in each U.S. county (except those in Alaska):\nlibrary(tidyverse)\nelections &lt;- read.csv(\"https://mac-stat.github.io/data/election_2020_county.csv\") |&gt; \n  select(state_abbr, historical, county_name, total_votes_20, repub_pct_20, dem_pct_20) |&gt; \n  mutate(dem_support_20 = case_when(\n    (repub_pct_20 - dem_pct_20 &gt;= 5) ~ \"low\",\n    (repub_pct_20 - dem_pct_20 &lt;= -5) ~ \"high\",\n    .default = \"medium\"\n  ))\n\n# Check it out\nhead(elections)  \n\n  state_abbr historical    county_name total_votes_20 repub_pct_20 dem_pct_20\n1         AL        red Autauga County          27770        71.44      27.02\n2         AL        red Baldwin County         109679        76.17      22.41\n3         AL        red Barbour County          10518        53.45      45.79\n4         AL        red    Bibb County           9595        78.43      20.70\n5         AL        red  Blount County          27588        89.57       9.57\n6         AL        red Bullock County           4613        24.84      74.70\n  dem_support_20\n1            low\n2            low\n3            low\n4            low\n5            low\n6           high\nCheck out the below visual and numerical summaries of dem_support_20:\nggplot(elections, aes(x = dem_support_20)) + \n  geom_bar()\n\n\n\n\n\n\n\nelections |&gt; \n  count(dem_support_20)\n\n  dem_support_20    n\n1           high  458\n2            low 2494\n3         medium  157\nFollow-up:\nWhat don’t you like about these results?\nEXAMPLE 2: Creating factor variables with meaningfully ordered levels (fct_relevel)\nThe above categories of dem_support_20 are listed alphabetically, which isn’t particularly meaningful here. This is because dem_support_20 is a character variable and R thinks of character strings as words, not category labels with any meaningful order (other than alphabetical):\nstr(elections)\n\n'data.frame':   3109 obs. of  7 variables:\n $ state_abbr    : chr  \"AL\" \"AL\" \"AL\" \"AL\" ...\n $ historical    : chr  \"red\" \"red\" \"red\" \"red\" ...\n $ county_name   : chr  \"Autauga County\" \"Baldwin County\" \"Barbour County\" \"Bibb County\" ...\n $ total_votes_20: int  27770 109679 10518 9595 27588 4613 9488 50983 15284 12301 ...\n $ repub_pct_20  : num  71.4 76.2 53.5 78.4 89.6 ...\n $ dem_pct_20    : num  27.02 22.41 45.79 20.7 9.57 ...\n $ dem_support_20: chr  \"low\" \"low\" \"low\" \"low\" ...\nWe can fix this by using fct_relevel() to both:\n# Notice that the order of the levels is not alphabetical!\nelections &lt;- elections |&gt; \n  mutate(dem_support_20 = fct_relevel(dem_support_20, c(\"low\", \"medium\", \"high\")))\n\n# Notice the new structure of the dem_support_20 variable\nstr(elections)\n\n'data.frame':   3109 obs. of  7 variables:\n $ state_abbr    : chr  \"AL\" \"AL\" \"AL\" \"AL\" ...\n $ historical    : chr  \"red\" \"red\" \"red\" \"red\" ...\n $ county_name   : chr  \"Autauga County\" \"Baldwin County\" \"Barbour County\" \"Bibb County\" ...\n $ total_votes_20: int  27770 109679 10518 9595 27588 4613 9488 50983 15284 12301 ...\n $ repub_pct_20  : num  71.4 76.2 53.5 78.4 89.6 ...\n $ dem_pct_20    : num  27.02 22.41 45.79 20.7 9.57 ...\n $ dem_support_20: Factor w/ 3 levels \"low\",\"medium\",..: 1 1 1 1 1 3 1 1 1 1 ...\n# And plot dem_support_20\nggplot(elections, aes(x = dem_support_20)) +\n  geom_bar()\nEXAMPLE 3: Changing the labels of the levels in factor variables\nWe now have a factor variable, dem_support_20, with categories that are ordered in a meaningful way:\nelections |&gt; \n  count(dem_support_20)\n\n  dem_support_20    n\n1            low 2494\n2         medium  157\n3           high  458\nBut maybe we want to change up the category labels. For demo purposes, let’s create a new factor variable, results_20, that’s the same as dem_support_20 but with different category labels:\n# We can redefine any number of the category labels.\n# Here we'll relabel all 3 categories:\nelections &lt;- elections |&gt; \n  mutate(results_20 = fct_recode(dem_support_20, \n                                 \"strong republican\" = \"low\",\n                                 \"close race\" = \"medium\",\n                                 \"strong democrat\" = \"high\"))\n\n# Check it out\n# Note that the new category labels are still in a meaningful,\n# not necessarily alphabetical, order!\nelections |&gt; \n  count(results_20)\n\n         results_20    n\n1 strong republican 2494\n2        close race  157\n3   strong democrat  458\nEXAMPLE 4: Re-ordering factor levels\nFinally, let’s explore how the Republican vote varied from county to county within each state:\n# Note that we're just piping the data into ggplot instead of writing\n# it as the first argument\nelections |&gt; \n  ggplot(aes(x = repub_pct_20, fill = state_abbr)) + \n    geom_density(alpha = 0.5)\nThis is too many density plots to put on top of one another. Let’s spread these out while keeping them in the same frame, hence easier to compare, using a joy plot or ridge plot:\nlibrary(ggridges)\nelections |&gt; \n  ggplot(aes(x = repub_pct_20, y = state_abbr, fill = historical)) + \n    geom_density_ridges() + \n    scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\nOK, but this is alphabetical. Suppose we want to reorder the states according to their typical Republican support. Recall that we did something similar in Example 2, using fct_relevel() to specify a meaningful order for the dem_support_20 categories:\nfct_relevel(dem_support_20, c(\"low\", \"medium\", \"high\"))\nWe could use fct_relevel() to reorder the states here, but what would be the drawbacks?\nEXAMPLE 5: Re-ordering factor levels according to another variable\nWhen a meaningful order for the categories of a factor variable can be defined by another variable in our dataset, we can use fct_reorder(). In our joy plot, let’s reorder the states according to their median Republican support:\n# Since we might want states to be alphabetical in other parts of our analysis,\n# we'll pipe the data into the ggplot without storing it:\nelections |&gt; \n  mutate(state_abbr = fct_reorder(state_abbr, repub_pct_20, .fun = \"median\")) |&gt; \n  ggplot(aes(x = repub_pct_20, y = state_abbr, fill = historical)) + \n    geom_density_ridges() + \n    scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\n# How did the code change?\n# And the corresponding output?\nelections |&gt; \n  mutate(state_abbr = fct_reorder(state_abbr, repub_pct_20, .fun = \"median\", .desc = TRUE)) |&gt; \n  ggplot(aes(x = repub_pct_20, y = state_abbr, fill = historical)) + \n    geom_density_ridges() + \n    scale_fill_manual(values = c(\"blue\", \"purple\", \"red\"))\nWORKING WITH FACTOR VARIABLES\nThe forcats package, part of the tidyverse, includes handy functions for working with categorical variables (for + cats):\nHere are just some, some of which we explored above:",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "activities/activity-12.html#warm-up",
    "href": "activities/activity-12.html#warm-up",
    "title": "10  Factors",
    "section": "",
    "text": "do some wrangling:\n\narrange() our data in a meaningful order\nsubset the data to only filter() the rows and select() the columns of interest\nmutate() existing variables and define new variables\nsummarize() various aspects of a variable, both overall and by group (group_by())\n\nreshape our data to fit the task at hand (pivot_longer(), pivot_wider())\njoin() different datasets into one\n\n\n\n\n\n\nRegular expressions\nWhen working with character strings, we might want to detect, replace, or extract certain patterns. For example, recall our data on courses:\n\n\n\n- change `FA` to `fall_` and `SP` to `spring_`\n- keep only courses taught in fall\n- split the variable into 2 new variables: `semester` (`FA` or `SP`) and `year`\n\n\nConverting characters to factors (and factors to meaningful factors) (today)\nWhen categorical information is stored as a character variable, the categories of interest might not be labeled or ordered in a meaningful way. We can fix that!\n\n\n\n\n\n\n\nlow = the Republican won the county by at least 5 percentage points\nmedium = the Republican and Democrat votes were within 5 percentage points\nhigh = the Democrat won the county by at least 5 percentage points\n\n\n\n\n\n\n\n\n\n\nStore dem_support_20 as a factor variable, the levels of which are recognized as specific levels or categories, not just words.\nSpecify a meaningful order for the levels of the factor variable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunctions for changing the order of factor levels\n\nfct_relevel() = manually reorder levels\nfct_reorder() = reorder levels according to values of another variable\nfct_infreq() = order levels from highest to lowest frequency\nfct_rev() = reverse the current order\n\nfunctions for changing the labels or values of factor levels\n\nfct_recode() = manually change levels\nfct_lump() = group together least common levels",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "activities/activity-12.html#exercises",
    "href": "activities/activity-12.html#exercises",
    "title": "10  Factors",
    "section": "10.2 Exercises",
    "text": "10.2 Exercises\nThe exercises revisit our grades data:\n\n\n     sid grade   sessionID\n1 S31185    D+ session1784\n2 S31185    B+ session1785\n3 S31185    A- session1791\n4 S31185    B+ session1792\n5 S31185    B- session1794\n6 S31185    C+ session1795\n\n\nWe’ll explore the number of times each grade was assigned:\n\ngrade_distribution &lt;- grades |&gt; \n  count(grade)\n\nhead(grade_distribution)\n\n  grade    n\n1     A 1506\n2    A- 1381\n3    AU   27\n4     B  804\n5    B+ 1003\n6    B-  330\n\n\n\nExercise 1: Changing the order (option 1)\nCheck out a column plot of the number of times each grade was assigned during the study period. This is similar to a bar plot, but where we define the height of a bar according to variable in our dataset.\n\ngrade_distribution |&gt; \n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\nThe order of the grades is goofy! Construct a new column plot, manually reordering the grades from high (A) to low (NC) with “S” and “AU” at the end:\n\ngrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\nConstruct a new column plot, reordering the grades in ascending frequency (i.e. how often the grades were assigned):\n\ngrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\nConstruct a new column plot, reordering the grades in descending frequency (i.e. how often the grades were assigned):\n\ngrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n, .desc = TRUE)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Changing factor level labels\nIt may not be clear what “AU” and “S” stand for. Construct a new column plot that renames these levels “Audit” and “Satisfactory”, while keeping the other grade labels the same and in a meaningful order:\n\ngrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  mutate(grade = fct_recode(grade, \"Satisfactory\" = \"S\", \"Audit\" = \"AU\")) |&gt;  # Multiple pieces go into the last 2 blanks\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "activities/activity-12.html#solutions",
    "href": "activities/activity-12.html#solutions",
    "title": "10  Factors",
    "section": "10.3 Solutions",
    "text": "10.3 Solutions\n\n\nClick for Solutions\n\nEXAMPLE 1\nThe categories are in alphabetical order, which isn’t meaningful here.\n\n\n\n\n\nEXAMPLE 4: Re-ordering factor levels\nwe would have to:\n\nCalculate the typical Republican support in each state, e.g. using group_by() and summarize().\nWe’d then have to manually type out a meaningful order for 50 states! That’s a lot of typing and manual bookkeeping.\n\n\n\n\n\n\n\nExercise 1: Changing the order\n\ngrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\n\ngrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\n\ngrade_distribution |&gt;\n  mutate(grade = fct_reorder(grade, n, .desc = TRUE)) |&gt;\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Changing factor level labels\n\ngrade_distribution |&gt;\n  mutate(grade = fct_relevel(grade, c(\"A\", \"A-\", \"B+\", \"B\", \"B-\", \"C+\", \"C\", \"C-\", \"D+\", \"D\", \"D-\", \"NC\", \"S\", \"AU\"))) |&gt;\n  mutate(grade = fct_recode(grade, \"Satisfactory\" = \"S\", \"Audit\" = \"AU\")) |&gt;  # Multiple pieces go into the last 2 blanks\n  ggplot(aes(x = grade, y = n)) +\n    geom_col()",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Factors</span>"
    ]
  },
  {
    "objectID": "activities/activity-13.html",
    "href": "activities/activity-13.html",
    "title": "11  Strings",
    "section": "",
    "text": "11.1 Warm-up\nWHERE ARE WE?\nWe’re in the last day of our “data preparation” unit:\nBefore spring break, we started discussing some considerations in working with special types of “categorical” variables: characters and factors.\nsessionID dept level    sem enroll     iid\n1 session1784    M   100 FA1991     22 inst265\n2 session1785    k   100 FA1991     52 inst458\n3 session1791    J   100 FA1993     22 inst223\n4 session1792    J   300 FA1993     20 inst235\n5 session1794    J   200 FA1993     22 inst234\n6 session1795    J   200 SP1994     26 inst230\n\n\n'data.frame':   1718 obs. of  6 variables:\n $ sessionID: chr  \"session1784\" \"session1785\" \"session1791\" \"session1792\" ...\n $ dept     : chr  \"M\" \"k\" \"J\" \"J\" ...\n $ level    : int  100 100 100 300 200 200 200 100 300 100 ...\n $ sem      : chr  \"FA1991\" \"FA1991\" \"FA1993\" \"FA1993\" ...\n $ enroll   : int  22 52 22 20 22 26 25 38 16 43 ...\n $ iid      : chr  \"inst265\" \"inst458\" \"inst223\" \"inst235\" ...\nFocusing on just the sem character variable, we might want to…\nESSENTIAL STRING FUNCTIONS\nThe stringr package within tidyverse contains lots of functions to help process strings. We’ll focus on the most common. Letting x be a string variable…\nEXAMPLE 1\nConsider the following data with string variables :\nlibrary(tidyverse)\n\nclasses &lt;- data.frame(\n  sem        = c(\"SP2023\", \"FA2023\", \"SP2024\"),\n  area       = c(\"History\", \"Math\", \"Anthro\"),\n  enroll     = c(\"30 - people\", \"20 - people\", \"25 - people\"),\n  instructor = c(\"Ernesto Capello\", \"Lori Ziegelmeier\", \"Arjun Guneratne\")\n)\n\nclasses\n\n     sem    area      enroll       instructor\n1 SP2023 History 30 - people  Ernesto Capello\n2 FA2023    Math 20 - people Lori Ziegelmeier\n3 SP2024  Anthro 25 - people  Arjun Guneratne\nUsing only your intuition, use our str_ functions to complete the following. NOTE: You might be able to use other wrangling verbs in some cases, but focus on the new functions here.\n# Define a new variable \"num\" that adds up the number of characters in the area label\n# Change the areas to \"history\", \"math\", \"anthro\" instead of \"History\", \"Math\", \"Anthro\"\n# Create a variable that id's which courses were taught in spring\n# Change the semester labels to \"fall2023\", \"spring2024\", \"spring2023\"\n# In the enroll variable, change all e's to 3's (just because?)\n# Use sem to create 2 new variables, one with only the semester (SP/FA) and 1 with the year\nSUMMARY\nHere’s what we learned about each function:\nEXAMPLE 2\nSuppose we only want the spring courses:\n# How can we do this after mutating?\nclasses |&gt; \n  mutate(spring = str_detect(sem, \"SP\"))\n\n     sem    area      enroll       instructor spring\n1 SP2023 History 30 - people  Ernesto Capello   TRUE\n2 FA2023    Math 20 - people Lori Ziegelmeier  FALSE\n3 SP2024  Anthro 25 - people  Arjun Guneratne   TRUE\n# We don't have to mutate first!\nclasses |&gt; \n  filter(str_detect(sem, \"SP\"))\n\n     sem    area      enroll      instructor\n1 SP2023 History 30 - people Ernesto Capello\n2 SP2024  Anthro 25 - people Arjun Guneratne\n# Yet another way\nclasses |&gt; \n  filter(!str_detect(sem, \"FA\"))\n\n     sem    area      enroll      instructor\n1 SP2023 History 30 - people Ernesto Capello\n2 SP2024  Anthro 25 - people Arjun Guneratne\nEXAMPLE 3\nSuppose we wanted to get separate columns for the first and last names of each course instructor in classes. Try doing this using str_sub(). But don’t try too long! Explain what trouble you ran into.\nEXAMPLE 4\nIn general, when we want to split a column into 2+ new columns, we can often use separate():\nclasses |&gt; \n  separate(instructor, c(\"first\", \"last\"), sep = \" \")\n\n     sem    area      enroll   first        last\n1 SP2023 History 30 - people Ernesto     Capello\n2 FA2023    Math 20 - people    Lori Ziegelmeier\n3 SP2024  Anthro 25 - people   Arjun   Guneratne\n# Sometimes the function can \"intuit\" how we want to separate the variable\nclasses |&gt; \n  separate(instructor, c(\"first\", \"last\"))\n\n     sem    area      enroll   first        last\n1 SP2023 History 30 - people Ernesto     Capello\n2 FA2023    Math 20 - people    Lori Ziegelmeier\n3 SP2024  Anthro 25 - people   Arjun   Guneratne\n# classes |&gt; \n#   separate(___, c(___, ___), sep = \"___\")\n# (?&lt;=[SP|FA]): any character *before* the split point is a \"SP\" or \"FA\"\n# (?=2): the first character *after* the split point is a 2\nclasses |&gt; \n  separate(sem, \n          c(\"semester\", \"year\"),\n          \"(?&lt;=[SP|FA])(?=2)\")\n\n  semester year    area      enroll       instructor\n1       SP 2023 History 30 - people  Ernesto Capello\n2       FA 2023    Math 20 - people Lori Ziegelmeier\n3       SP 2024  Anthro 25 - people  Arjun Guneratne\n# More general:\n# (?&lt;=[a-zA-Z]): any character *before* the split point is a lower or upper case letter\n# (?=[0-9]): the first character *after* the split point is number\nclasses |&gt; \n  separate(sem, \n          c(\"semester\", \"year\"),\n          \"(?&lt;=[A-Z])(?=[0-9])\")\n\n  semester year    area      enroll       instructor\n1       SP 2023 History 30 - people  Ernesto Capello\n2       FA 2023    Math 20 - people Lori Ziegelmeier\n3       SP 2024  Anthro 25 - people  Arjun Guneratne",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "activities/activity-13.html#warm-up",
    "href": "activities/activity-13.html#warm-up",
    "title": "11  Strings",
    "section": "",
    "text": "Converting characters to factors (and factors to meaningful factors) (last time)\nWhen categorical information is stored as a character variable, the categories of interest might not be labeled or ordered in a meaningful way. We can fix that!\nStrings (today!)\nWhen working with character strings, we might want to detect, replace, or extract certain patterns. For example, recall our data on courses:\n\n\n\n- change `FA` to `fall_` and `SP` to `spring_`\n- keep only courses taught in fall\n- split the variable into 2 new variables: `semester` (`FA` or `SP`) and `year`\n\nMuch more! (maybe in your projects or COMP/STAT 212)\nThere are a lot of ways to process character variables. For example, we might have a variable that records the text for a sample of news articles. We might want to analyze things like the articles’ sentiments, word counts, typical word lengths, most common words, etc.\n\n\n\n\n\n\n\nfunction\narguments\nreturns\n\n\n\n\nstr_replace()\nx, pattern, replacement\na modified string\n\n\nstr_replace_all()\nx, pattern, replacement\na modified string\n\n\nstr_to_lower()\nx\na modified string\n\n\nstr_sub()\nx, start, end\na modified string\n\n\nstr_length()\nx\na number\n\n\nstr_detect()\nx, pattern\nTRUE/FALSE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstr_replace(x, pattern, replacement) finds the first part of x that matches the pattern and replaces it with replacement\nstr_replace_all(x, pattern, replacement) finds all instances in x that matches the pattern and replaces it with replacement\nstr_to_lower(x) converts all upper case letters in x to lower case\nstr_sub(x, start, end) only keeps a subset of characters in x, from start (a number indexing the first letter to keep) to end (a number indexing the last letter to keep)\nstr_length(x) records the number of characters in x\nstr_detect(x, pattern) is TRUE if x contains the given pattern and FALSE otherwise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeparate enroll into 2 separate columns: students and people. (These columns don’t make sense this is just practice).\n\n\n\nWe separated sem into semester and year above using str_sub(). Why would this be hard using separate()?\nWhen we want to split a column into 2+ new columns (or do other types of string processing), but there’s no consistent pattern by which to do this, we can use regular expressions (an optional topic):",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "activities/activity-13.html#exercises",
    "href": "activities/activity-13.html#exercises",
    "title": "11  Strings",
    "section": "11.2 Exercises",
    "text": "11.2 Exercises\n\nExercise 1: Time slots\nThe courses data includes actual data scraped from Mac’s class schedule. (Thanks to Prof Leslie Myint for the scraping code!!)\nIf you want to learn how to scrape data, take COMP/STAT 212, Intermediate Data Science! NOTE: For simplicity, I removed classes that had “TBA” for the days.\n\ncourses &lt;- read.csv(\"https://mac-stat.github.io/data/registrar.csv\")\n\n# Check it out\nhead(courses)\n\n       number   crn                                                name  days\n1 AMST 112-01 10318         Introduction to African American Literature M W F\n2 AMST 194-01 10073              Introduction to Asian American Studies M W F\n3 AMST 194-F1 10072 What’s After White Empire - And Is It Already Here?  T R \n4 AMST 203-01 10646 Politics and Inequality: The American Welfare State M W F\n5 AMST 205-01 10842                         Trans Theories and Politics  T R \n6 AMST 209-01 10474                   Civil Rights in the United States   W  \n             time      room             instructor avail_max\n1 9:40 - 10:40 am  MAIN 009       Daylanne English    3 / 20\n2  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa   -4 / 16\n3  3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan    0 / 14\n4 9:40 - 10:40 am  CARN 305          Lesley Lavery    3 / 25\n5  3:00 - 4:30 pm  MAIN 009              Myrl Beam   -2 / 20\n6 7:00 - 10:00 pm  MAIN 010         Walter Greason   -1 / 15\n\n\nUse our more familiar wrangling tools to warm up.\n\n# Construct a table that indicates the number of classes offered in each day/time slot\n# Print only the 6 most popular time slots\n\ncourses %&gt;%  \n  count(days, time) %&gt;%  \n  arrange(desc(n)) %&gt;%  \n  head()\n\n   days             time  n\n1 M W F 10:50 - 11:50 am 76\n2  T R   9:40 - 11:10 am 71\n3 M W F  9:40 - 10:40 am 68\n4 M W F   1:10 - 2:10 pm 66\n5  T R    3:00 - 4:30 pm 62\n6  T R    1:20 - 2:50 pm 59\n\n\n\n\n\n\n\n\n\nExercise 2: Prep the data\nSo that we can analyze it later, we want to wrangle the courses data:\n\nLet’s get some enrollment info:\n\nSplit avail_max into 2 separate variables: avail and max.\nUse avail and max to define a new variable called enrollment. HINT: You’ll need as.numeric()\n\nSplit the course number into 3 separate variables: dept, number, and section. HINT: You can use separate() to split a variable into 3, not just 2 new variables.\n\nStore this as courses_clean so that you can use it later.\n\ncourses_clean &lt;- courses %&gt;% \n  separate(avail_max, c(\"avail\", \"max\"), sep = \"/\") %&gt;% \n  mutate(avail = as.numeric(avail), max = as.numeric(max)) %&gt;% \n  mutate(enrollment = max-avail) %&gt;% \n  separate(number, c(\"dept\", \"number\", \"section\"))\n\n\n\n\n\n\n\n\nExercise 3: Courses by department\nUsing courses_clean…\n\n# Identify the 6 departments that offered the most sections\ncourses_clean %&gt;% \n  count(dept) %&gt;% \n  arrange(desc(n)) %&gt;% \n  head()\n\n  dept  n\n1 SPAN 45\n2 BIOL 44\n3 ENVI 38\n4 PSYC 37\n5 CHEM 33\n6 COMP 31\n\n# Identify the 6 departments with the longest average course titles\ncourses_clean %&gt;% \n  mutate(length = str_length(name)) %&gt;% \n  group_by(dept) %&gt;% \n  summarize(avg_len = mean(length)) %&gt;% \n  arrange(desc(avg_len)) %&gt;% \n  head()\n\n# A tibble: 6 × 2\n  dept  avg_len\n  &lt;chr&gt;   &lt;dbl&gt;\n1 WGSS     46.3\n2 INTL     41.4\n3 EDUC     39.4\n4 MCST     39.4\n5 POLI     37.4\n6 AMST     37.3\n\n\n\n\n\n\n\n\n\nExercise 4: STAT courses\n\nPart a\nGet a subset of courses_clean that only includes courses taught by Alicia Johnson.\n\ncourses_clean %&gt;% \n  filter(instructor == \"Alicia Johnson\")\n\n  dept number section   crn                         name  days            time\n1 STAT    253      01 10806 Statistical Machine Learning  T R  9:40 - 11:10 am\n2 STAT    253      02 10807 Statistical Machine Learning  T R   1:20 - 2:50 pm\n3 STAT    253      03 10808 Statistical Machine Learning  T R   3:00 - 4:30 pm\n        room     instructor avail max enrollment\n1 THEATR 206 Alicia Johnson    -3  20         23\n2 THEATR 206 Alicia Johnson    -3  20         23\n3 THEATR 206 Alicia Johnson     2  20         18\n\n\n\n\nPart b\nCreate a new dataset from courses_clean, named stat, that only includes STAT sections. In this dataset:\n\nIn the course names:\n\nRemove “Introduction to” from any name.\nShorten “Statistical” to “Stat” where relevant.\n\nDefine a variable that records the start_time for the course.\nKeep only the number, name, start_time, enroll columns.\nThe result should have 19 rows and 4 columns.\n\n\nstat &lt;- courses_clean %&gt;% \n  filter(dept == \"STAT\") %&gt;% \n  mutate(name = str_replace(name, \"Introduction to \", \"\")) %&gt;% \n  mutate(name = str_replace(name, \"Statistical\", \"Stat\")) %&gt;% \n  mutate(start_time = str_sub(time, 1, 5)) %&gt;% \n  select(number, name, start_time, enrollment)\n\ndim(stat)\n\n[1] 19  4\n\n\n\n\n\n\n\n\n\n\nExercise 5: More cleaning\nIn the next exercises, we’ll dig into enrollments. Let’s get the data ready for that analysis here. Make the following changes to the courses_clean data. Because they have different enrollment structures, and we don’t want to compare apples and oranges, remove the following:\n\nall sections in PE and INTD (interdisciplinary studies courses)\nall music ensembles and dance practicums, i.e. all MUSI and THDA classes with numbers less than 100. HINT: !(dept == \"MUSI\" & as.numeric(number) &lt; 100)\nall lab sections. Be careful which variable you use here. For example, you don’t want to search by “Lab” and accidentally eliminate courses with words such as “Labor”.\n\nSave the results as enrollments (don’t overwrite courses_clean).\n\nenrollments &lt;- courses_clean %&gt;% \n  filter(dept != c(\"PE\", \"INTD\")) %&gt;% \n  filter(!(dept == \"MUSI\" & as.numeric(number) &lt; 100)) %&gt;% \n  filter(!(dept == \"THDA\" & as.numeric(number) &lt; 100)) %&gt;% \n  filter(!str_detect(section, \"L\"))\n  \nhead(enrollments)\n\n  dept number section   crn                                                name\n1 AMST    112      01 10318         Introduction to African American Literature\n2 AMST    194      01 10073              Introduction to Asian American Studies\n3 AMST    194      F1 10072 What’s After White Empire - And Is It Already Here?\n4 AMST    203      01 10646 Politics and Inequality: The American Welfare State\n5 AMST    205      01 10842                         Trans Theories and Politics\n6 AMST    209      01 10474                   Civil Rights in the United States\n   days            time      room             instructor avail max enrollment\n1 M W F 9:40 - 10:40 am  MAIN 009       Daylanne English     3  20         17\n2 M W F  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa    -4  16         20\n3  T R   3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan     0  14         14\n4 M W F 9:40 - 10:40 am  CARN 305          Lesley Lavery     3  25         22\n5  T R   3:00 - 4:30 pm  MAIN 009              Myrl Beam    -2  20         22\n6   W   7:00 - 10:00 pm  MAIN 010         Walter Greason    -1  15         16\n\n\n\n\n\n\n\n\n\nExercise 6: Enrollment & departments\nExplore enrollments by department. You decide what research questions to focus on. Use both visual and numerical summaries.\n\n\n\n\n\n\n\nExercise 7: Enrollment & faculty\nLet’s now explore enrollments by instructor. In doing so, we have to be cautious of cross-listed courses that are listed under multiple different departments. Uncomment the code lines in the chunk below for an example.\n\n\n\n\n\n\nCommenting/Uncommenting Code\n\n\n\nTo comment/uncomment several lines of code at once, highlight them then click ctrl/cmd+shift+c.\n\n\n\nenrollments |&gt;\n  filter(dept %in% c(\"STAT\", \"COMP\"), number == 112, section == \"01\")\n\n  dept number section   crn                         name  days           time\n1 COMP    112      01 10248 Introduction to Data Science  T R  3:00 - 4:30 pm\n2 STAT    112      01 10249 Introduction to Data Science  T R  3:00 - 4:30 pm\n      room        instructor avail max enrollment\n1 OLRI 254 Brianna Heggeseth     1  28         27\n2 OLRI 254 Brianna Heggeseth     1  28         27\n\n\nNotice that these are the exact same section! In order to not double count an instructor’s enrollments, we can keep only the courses that have distinct() combinations of days, time, instructor values. Uncomment the code lines in the chunk below.\n\nenrollments_2 &lt;- enrollments |&gt;\n  distinct(days, time, instructor, .keep_all = TRUE)\n\n# NOTE: By default this keeps the first department alphabetically\n#That's fine because we won't use this to analyze department enrollments!\nenrollments_2 |&gt;\n  filter(instructor == \"Brianna Heggeseth\", name == \"Introduction to Data Science\")\n\n  dept number section   crn                         name  days           time\n1 COMP    112      01 10248 Introduction to Data Science  T R  3:00 - 4:30 pm\n      room        instructor avail max enrollment\n1 OLRI 254 Brianna Heggeseth     1  28         27\n\n\nNow, explore enrollments by instructor. You decide what research questions to focus on. Use both visual and numerical summaries.\nCAVEAT: The above code doesn’t deal with co-taught courses that have more than one instructor. Thus instructors that co-taught are recorded as a pair, and their co-taught enrollments aren’t added to their total enrollments. This is tough to get around with how the data were scraped as the instructor names are smushed together, not separated by a comma!\n\n\n\n\n\n\n\nOptional extra practice\n\n# Make a bar plot showing the number of night courses by day of the week\n# Use courses_clean\n\n\n\n\n\n\n\n\nDig Deeper: regex\nExample 4 gave 1 small example of a regular expression.\nThese are handy when we want process a string variable, but there’s no consistent pattern by which to do this. You must think about the structure of the string and how you can use regular expressions to capture the patterns you want (and exclude the patterns you don’t want).\nFor example, how would you describe the pattern of a 10-digit phone number? Limit yourself to just a US phone number for now.\n\nThe first 3 digits are the area code.\nThe next 3 digits are the exchange code.\nThe last 4 digits are the subscriber number.\n\nThus, a regular expression for a US phone number could be:\n\n[:digit:]{3}-[:digit:]{3}-[:digit:]{4} which limits you to XXX-XXX-XXXX pattern or\n\\\\([:digit:]{3}\\\\) [:digit:]{3}-[:digit:]{4} which limits you to (XXX) XXX-XXXX pattern or\n[:digit:]{3}\\\\.[:digit:]{3}\\\\.[:digit:]{4} which limits you to XXX.XXX.XXXX pattern\n\nThe following would include the three patterns above in addition to the XXXXXXXXXX pattern (no dashes or periods): - [\\\\(]*[:digit:]{3}[-.\\\\)]*[:digit:]{3}[-.]*[:digit:]{4}\nIn order to write a regular expression, you first need to consider what patterns you want to include and exclude.\nWork through the following examples, and the tutorial after them to learn about the syntax.\nEXAMPLES\n\n# Define some strings to play around with\nexample &lt;- \"The quick brown fox jumps over the lazy dog.\"\n\n\nstr_replace(example, \"quick\", \"really quick\")\n\n[1] \"The really quick brown fox jumps over the lazy dog.\"\n\n\n\nstr_replace_all(example, \"(fox|dog)\", \"****\") # | reads as OR\n\n[1] \"The quick brown **** jumps over the lazy ****.\"\n\n\n\nstr_replace_all(example, \"(fox|dog).\", \"****\") # \".\" for any character\n\n[1] \"The quick brown ****jumps over the lazy ****\"\n\n\n\nstr_replace_all(example, \"(fox|dog)\\\\.$\", \"****\") # at end of sentence only, \"\\\\.\" only for a period\n\n[1] \"The quick brown fox jumps over the lazy ****\"\n\n\n\nstr_replace_all(example, \"the\", \"a\") # case-sensitive only matches one\n\n[1] \"The quick brown fox jumps over a lazy dog.\"\n\n\n\nstr_replace_all(example, \"[Tt]he\", \"a\") # # will match either t or T; could also make \"a\" conditional on capitalization of t\n\n[1] \"a quick brown fox jumps over a lazy dog.\"\n\n\n\nstr_replace_all(example, \"[Tt]he\", \"a\") # first match only\n\n[1] \"a quick brown fox jumps over a lazy dog.\"\n\n\n\n# More examples\nexample2 &lt;- \"Two roads diverged in a yellow wood, / And sorry I could not travel both / And be one traveler, long I stood / And looked down one as far as I could\"\nexample3 &lt;- \"This is a test\"\n\n# Store the examples in 1 place\nexamples &lt;- c(example, example2, example3)\n\n\npat &lt;- \"[^aeiouAEIOU ]{3}\" # Regular expression for three straight consonants. Note that I've excluded spaces as well\n\nstr_detect(examples, pat) # TRUE/FALSE if it detects pattern\n\n[1]  TRUE  TRUE FALSE\n\n\n\nstr_subset(examples, pat) # Pulls out those that detects pattern\n\n[1] \"The quick brown fox jumps over the lazy dog.\"                                                                                                        \n[2] \"Two roads diverged in a yellow wood, / And sorry I could not travel both / And be one traveler, long I stood / And looked down one as far as I could\"\n\n\n\npat2 &lt;- \"[^aeiouAEIOU ][aeiouAEIOU]{2}[^aeiouAEIOU ]{1}\" # consonant followed by two vowels followed by a consonant\n\nstr_extract(example2, pat2) # extract first match\n\n[1] \"road\"\n\n\n\nstr_extract_all(example2, pat2, simplify = TRUE) # extract all matches\n\n     [,1]   [,2]   [,3]   [,4]   [,5]   [,6]  \n[1,] \"road\" \"wood\" \"coul\" \"tood\" \"look\" \"coul\"\n\n\nTUTORIAL\nTry out this interactive tutorial. Note that neither the tutorial nor regular expressions more generally are specific to R, but it still illustrates the main ideas of regular expressions.",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "activities/activity-13.html#solutions",
    "href": "activities/activity-13.html#solutions",
    "title": "11  Strings",
    "section": "11.3 Solutions",
    "text": "11.3 Solutions\n\n\nClick for Solutions\n\nEXAMPLE 1\n\n# Define a new variable \"num\" that adds up the number of characters in the area label\nclasses |&gt; \n  mutate(num = str_length(area))\n\n     sem    area      enroll       instructor num\n1 SP2023 History 30 - people  Ernesto Capello   7\n2 FA2023    Math 20 - people Lori Ziegelmeier   4\n3 SP2024  Anthro 25 - people  Arjun Guneratne   6\n\n# Change the areas to \"history\", \"math\", \"anthro\"\nclasses |&gt; \n  mutate(area = str_to_lower(area))\n\n     sem    area      enroll       instructor\n1 SP2023 history 30 - people  Ernesto Capello\n2 FA2023    math 20 - people Lori Ziegelmeier\n3 SP2024  anthro 25 - people  Arjun Guneratne\n\n# Create a variable that id's which courses were taught in spring \nclasses |&gt; \n  mutate(spring = str_detect(sem, \"SP\"))\n\n     sem    area      enroll       instructor spring\n1 SP2023 History 30 - people  Ernesto Capello   TRUE\n2 FA2023    Math 20 - people Lori Ziegelmeier  FALSE\n3 SP2024  Anthro 25 - people  Arjun Guneratne   TRUE\n\n# Change the semester labels to \"fall2023\", \"spring2024\", \"spring2023\"\nclasses |&gt; \n  mutate(sem = str_replace(sem, \"SP\", \"spring\")) |&gt; \n  mutate(sem = str_replace(sem, \"FA\", \"fall\"))\n\n         sem    area      enroll       instructor\n1 spring2023 History 30 - people  Ernesto Capello\n2   fall2023    Math 20 - people Lori Ziegelmeier\n3 spring2024  Anthro 25 - people  Arjun Guneratne\n\n# In the enroll variable, change all e's to 3's (just because?)\nclasses |&gt; \n  mutate(enroll = str_replace_all(enroll, \"e\", \"3\"))\n\n     sem    area      enroll       instructor\n1 SP2023 History 30 - p3opl3  Ernesto Capello\n2 FA2023    Math 20 - p3opl3 Lori Ziegelmeier\n3 SP2024  Anthro 25 - p3opl3  Arjun Guneratne\n\n# Use sem to create 2 new variables, one with only the semester (SP/FA) and 1 with the year\nclasses |&gt; \n  mutate(semester = str_sub(sem, 1, 2),\n         year = str_sub(sem, 3, 6))\n\n     sem    area      enroll       instructor semester year\n1 SP2023 History 30 - people  Ernesto Capello       SP 2023\n2 FA2023    Math 20 - people Lori Ziegelmeier       FA 2023\n3 SP2024  Anthro 25 - people  Arjun Guneratne       SP 2024\n\n\n\n\n\n\n\nEXAMPLE 2\n\n# How can we do this after mutating?\nclasses |&gt; \n  mutate(spring = str_detect(sem, \"SP\")) |&gt; \n  filter(spring == TRUE)\n\n     sem    area      enroll      instructor spring\n1 SP2023 History 30 - people Ernesto Capello   TRUE\n2 SP2024  Anthro 25 - people Arjun Guneratne   TRUE\n\n\n\n\n\n\n\n\nExercise 1: Popular time slots\n\n# Construct a table that indicates the number of classes offered in each day/time slot\n# Print only the 6 most popular time slots\ncourses |&gt; \n  count(days, time) |&gt; \n  arrange(desc(n)) |&gt; \n  head()\n\n   days             time  n\n1 M W F 10:50 - 11:50 am 76\n2  T R   9:40 - 11:10 am 71\n3 M W F  9:40 - 10:40 am 68\n4 M W F   1:10 - 2:10 pm 66\n5  T R    3:00 - 4:30 pm 62\n6  T R    1:20 - 2:50 pm 59\n\n\n\n\n\n\n\n\n\nExercise 2: Prep the data\n\ncourses_clean &lt;- courses |&gt; \n  separate(avail_max, c(\"avail\", \"max\"), sep = \" / \") |&gt; \n  mutate(enroll = as.numeric(max) - as.numeric(avail)) |&gt; \n  separate(number, c(\"dept\", \"number\", \"section\"))\n  \nhead(courses_clean)\n\n  dept number section   crn                                                name\n1 AMST    112      01 10318         Introduction to African American Literature\n2 AMST    194      01 10073              Introduction to Asian American Studies\n3 AMST    194      F1 10072 What’s After White Empire - And Is It Already Here?\n4 AMST    203      01 10646 Politics and Inequality: The American Welfare State\n5 AMST    205      01 10842                         Trans Theories and Politics\n6 AMST    209      01 10474                   Civil Rights in the United States\n   days            time      room             instructor avail max enroll\n1 M W F 9:40 - 10:40 am  MAIN 009       Daylanne English     3  20     17\n2 M W F  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa    -4  16     20\n3  T R   3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan     0  14     14\n4 M W F 9:40 - 10:40 am  CARN 305          Lesley Lavery     3  25     22\n5  T R   3:00 - 4:30 pm  MAIN 009              Myrl Beam    -2  20     22\n6   W   7:00 - 10:00 pm  MAIN 010         Walter Greason    -1  15     16\n\n\n\n\n\n\n\n\n\nExercise 3: Courses offered by department\n\n# Identify the 6 departments that offered the most sections\ncourses_clean |&gt; \n  count(dept) |&gt; \n  arrange(desc(n)) |&gt; \n  head()\n\n  dept  n\n1 SPAN 45\n2 BIOL 44\n3 ENVI 38\n4 PSYC 37\n5 CHEM 33\n6 COMP 31\n\n# Identify the 6 departments with the longest average course titles\ncourses_clean |&gt; \n  mutate(length = str_length(name)) |&gt; \n  group_by(dept) |&gt; \n  summarize(avg_length = mean(length)) |&gt; \n  arrange(desc(avg_length)) |&gt; \n  head()\n\n# A tibble: 6 × 2\n  dept  avg_length\n  &lt;chr&gt;      &lt;dbl&gt;\n1 WGSS        46.3\n2 INTL        41.4\n3 EDUC        39.4\n4 MCST        39.4\n5 POLI        37.4\n6 AMST        37.3\n\n\n\n\nExercise 4: STAT courses\n\nPart a\n\ncourses_clean |&gt; \n  filter(str_detect(instructor, \"Alicia Johnson\")) \n\n  dept number section   crn                         name  days            time\n1 STAT    253      01 10806 Statistical Machine Learning  T R  9:40 - 11:10 am\n2 STAT    253      02 10807 Statistical Machine Learning  T R   1:20 - 2:50 pm\n3 STAT    253      03 10808 Statistical Machine Learning  T R   3:00 - 4:30 pm\n        room     instructor avail max enroll\n1 THEATR 206 Alicia Johnson    -3  20     23\n2 THEATR 206 Alicia Johnson    -3  20     23\n3 THEATR 206 Alicia Johnson     2  20     18\n\n\n\n\nPart b\n\nstat &lt;- courses_clean |&gt; \n  filter(dept == \"STAT\") |&gt; \n  mutate(name = str_replace(name, \"Introduction to \", \"\")) |&gt;\n  mutate(name = str_replace(name, \"Statistical\", \"Stat\")) |&gt; \n  mutate(start_time = str_sub(time, 1, 5)) |&gt; \n  select(number, name, start_time, enroll)\n\nstat\n\n   number                      name start_time enroll\n1     112              Data Science      3:00      27\n2     112              Data Science      9:40      21\n3     112              Data Science      1:20      25\n4     125              Epidemiology      12:00     26\n5     155             Stat Modeling      1:10      32\n6     155             Stat Modeling      9:40      24\n7     155             Stat Modeling      10:50     26\n8     155             Stat Modeling      3:30      25\n9     155             Stat Modeling      1:20      30\n10    155             Stat Modeling      3:00      27\n11    212 Intermediate Data Science      9:40      11\n12    212 Intermediate Data Science      1:20      11\n13    253     Stat Machine Learning      9:40      23\n14    253     Stat Machine Learning      1:20      23\n15    253     Stat Machine Learning      3:00      18\n16    354               Probability      3:00      22\n17    452           Correlated Data      9:40       7\n18    452           Correlated Data      1:20       8\n19    456  Projects in Data Science      9:40      11\n\ndim(stat)\n\n[1] 19  4\n\n\n\n\n\n\n\n\n\n\nExercise 5: More cleaning\n\nenrollments &lt;- courses_clean |&gt; \n  filter(dept != \"PE\", dept != \"INTD\") |&gt; \n  filter(!(dept == \"MUSI\" & as.numeric(number) &lt; 100)) |&gt; \n  filter(!(dept == \"THDA\" & as.numeric(number) &lt; 100)) |&gt; \n  filter(!str_detect(section, \"L\"))\n  \nhead(enrollments)\n\n  dept number section   crn                                                name\n1 AMST    112      01 10318         Introduction to African American Literature\n2 AMST    194      01 10073              Introduction to Asian American Studies\n3 AMST    194      F1 10072 What’s After White Empire - And Is It Already Here?\n4 AMST    203      01 10646 Politics and Inequality: The American Welfare State\n5 AMST    205      01 10842                         Trans Theories and Politics\n6 AMST    209      01 10474                   Civil Rights in the United States\n   days            time      room             instructor avail max enroll\n1 M W F 9:40 - 10:40 am  MAIN 009       Daylanne English     3  20     17\n2 M W F  1:10 - 2:10 pm MUSIC 219          Jake Nagasawa    -4  16     20\n3  T R   3:00 - 4:30 pm   HUM 214 Karin Aguilar-San Juan     0  14     14\n4 M W F 9:40 - 10:40 am  CARN 305          Lesley Lavery     3  25     22\n5  T R   3:00 - 4:30 pm  MAIN 009              Myrl Beam    -2  20     22\n6   W   7:00 - 10:00 pm  MAIN 010         Walter Greason    -1  15     16\n\n\n\n\n\n\n\n\n\nOptional extra practice\n\n# Make a bar plot showing the number of night courses by day of the week.\ncourses_clean |&gt; \n  filter(str_detect(time, \"7:00\")) |&gt; \n  ggplot(aes(x = days)) + \n    geom_bar()",
    "crumbs": [
      "Activities",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Strings</span>"
    ]
  },
  {
    "objectID": "notes/README.html",
    "href": "notes/README.html",
    "title": "Notes",
    "section": "",
    "text": "All notes live here.",
    "crumbs": [
      "Notes"
    ]
  },
  {
    "objectID": "notes/Journal.html",
    "href": "notes/Journal.html",
    "title": "12  Journal",
    "section": "",
    "text": "13 3. Data Viz\nData Viz: displaying data in such a way that we can glean more information about the data, such as patterns, typical outcomes, and distribution.\n\n13.0.1 Components of graphs:\n\nframe (sets up the grid and axes)\nlayer (adds the geometric elements that make up the graph, called geom_ in ggplot)\nscales (changes color/size/shape/etc)\nfaceting (splits up the data )\ntheme (finer scale adjustments like font size)\n\n\n\n13.0.2 For one variable viz:\n\ncategorical: bar plot\nnumerical: histogram/density plot\n\nggplot syntax:\n\nggplot(data = ___, aes(x = ___)) + \n  geom___(color = \"___\", fill = \"___\") + \n  labs(x = \"___\", y = \"___\")\n\n\n\n13.0.3 When discussing data visualizations, you typically look at:\n\ntypical outcome\nvariability + range\nshape\noutliers\n\n\n\n\n14 4. Bivariate Viz\nAnother way to look at the data is to look at relationships within the data. There are - Repsonse variables (dependent variables) - Predictors (independent variables)\n\n14.0.1 What plots to use:\n\n2 quantitative: scatterplot\n2 categorical: stacked bar/facet wrapped bar plot\n1 quantitative, 1 categorical: violin plot, boxplot, stacked density plots.\n\n\n\n\n15 5. Multivariate Viz\nWhich chart to use:\n\n3 cat: bar (x, color, facet)\n2 cat, 1 num: histogram/density (color, facet, x), box, violin (x, facet, y)\n1 cat, 2 num: scatter (facet/color/shape, x, y), heatmap (y, x, shade), star (y, size, )\n3 num: scatter (x, y, size/shade)\n\n\n\n16 6. Spatial Viz\n\nPoint map: a map that specific locations are plotted on (ex: favorite places in the twin cities).\nContour map: plotting density/distribution on a map rather than individual points\nChoropleth Maps: plotting outcomes in different regions (ex: percentage of democratic votes in each state)\n\nLeaflet: a package that can be used to make interactive maps, commonly point maps common functions for leaflet:\n\nleaflet() (makes the leaflet)\naddTiles() (makes the base map)\naddMarkers()\naddPolygons()\nprint() (need to print to see the leaflet)\n\nggplot is typically better for making chloropleth maps than leaflet.\n\n\n17 7. Effective Viz\nEffective visualizations are important in order to accurately portray your data in a way that people can understand.\nVisualizations can be good, ugly, bad, or wrong.\n\nGood: viz is clear, easy to understand, accessible, and helpful.\nUgly: nothing is incorrect about the viz, but the overall aesthetic is ugly\nBad: the viz is “unclear, confusing, overly complicated, or deceiving”\nWrong: viz is incorrectly displaying the data and/or the trends\n\nIn order to have effective viz you need to:\n\nhave clear, concise titles/axes\ninclude a figure caption\ninclude an alt caption, for screen readers\navoid using overly similar colors whenever possible\nadd context\nmake data anonymous when possible\n\n\n\n18 8. Wrangling Verbs\ntidyverse has many helpful functions that allow us to wrangle the data into a way that we can meaningfully create data viz.\n99.99% of data will not be entirely clean and ready to use. You will have to clean and reformat the data in order to use it.\nSome key functions to use are:\n\n\n\nverb\naction\n\n\n\n\narrange\narrange the rows according to some column\n\n\nfilter\nfilter out or obtain a subset of the rows\n\n\nselect\nselect a subset of columns\n\n\nmutate\nmutate or create a column\n\n\nsummarize\ncalculate a numerical summary of a column\n\n\ngroup_by\ngroup the rows by a specified column\n\n\n\n\n\n19 9. Wrangling + Dates\nOrder of the functions matter. If you want to arrange by one column but then only include 2 different columns in your final data frame, you need to do arrange first before selecting the columns or else you will get an error.\nKey syntax for filtering:\n\n\n\nsymbol\nmeaning\n\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n%in% c(***, ***)\na list of multiple values\n\n\n\nsummarize ex:\n\npenguins |&gt; filter(species == “Chinstrap”) |&gt; group_by(sex) |&gt; summarize(min = min(body_mass_g), max = max(body_mass_g)) |&gt; mutate(range = max - min)\n\n\n\n\n\nfunction\narguments\nreturns\n\n\n\n\nstr_replace()\nx, pattern, replacement\na modified string\n\n\nstr_replace_all()\nx, pattern, replacement\na modified string\n\n\nstr_to_lower()\nx\na modified string\n\n\nstr_sub()\nx, start, end\na modified string\n\n\nstr_length()\nx\na number\n\n\nstr_detect()\nx, pattern\nTRUE/FALSE",
    "crumbs": [
      "Notes",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Journal</span>"
    ]
  }
]